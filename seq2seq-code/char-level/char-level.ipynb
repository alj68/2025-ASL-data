{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56410f5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow_text\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtext\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras_hub\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow_text'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import tensorflow_text as text\n",
    "import keras_hub\n",
    "import tensorflow as tf\n",
    "import string\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ebbbc871",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50  # Batch size for training.\n",
    "epochs = 50  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "num_samples = 1500  # Number of samples to train on.\n",
    "\n",
    "train_pairs = (\"Z:\\\\official-code\\\\TRAINING_pairs.txt\") # Path to the data txt file on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29d5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gloss_cleaning(text):\n",
    "    text.replace(\"|-->\", \"\")\n",
    "    text.replace(\"!\", \"\")\n",
    "    text.replace(\"(1h)\", \"\")\n",
    "    text.replace(\"(2h)\", \"\")\n",
    "    text.replace(\"alt.\", \"\")\n",
    "    text.replace(\"+ \", \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0fece",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "BytePairTokenizer requires `tensorflow` and `tensorflow-text` for text processing. Run `pip install tensorflow-text` to install both packages or visit https://www.tensorflow.org/install\n\nIf `tensorflow-text` is already installed, try importing it in a clean python session. Your installation may have errors.\n\nKerasHub uses `tf.data` and `tensorflow-text` to preprocess text on all Keras backends. If you are running on Jax or Torch, this installation does not need GPU support.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m char \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m input_characters:\n\u001b[32m     17\u001b[39m             input_characters.add(char)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m tokenizer = \u001b[43mkeras_hub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mBytePairTokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvocabulary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmerges\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43madd_prefix_space\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43munsplittable_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43munsplittables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mint32\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m asl_vocabulary = tokenizer.get_vocabulary()\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m target_texts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Panoptic System\\anaconda3\\envs\\seq2seq\\Lib\\site-packages\\keras_hub\\src\\tokenizers\\byte_pair_tokenizer.py:286\u001b[39m, in \u001b[36mBytePairTokenizer.__init__\u001b[39m\u001b[34m(self, vocabulary, merges, sequence_length, add_prefix_space, unsplittable_tokens, dtype, **kwargs)\u001b[39m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_int_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_string_dtype(dtype):\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    282\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOutput dtype must be an integer type or a string. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    283\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived: dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    284\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28mself\u001b[39m.sequence_length = sequence_length\n\u001b[32m    288\u001b[39m \u001b[38;5;28mself\u001b[39m.add_prefix_space = add_prefix_space\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Panoptic System\\anaconda3\\envs\\seq2seq\\Lib\\site-packages\\keras_hub\\src\\tokenizers\\tokenizer.py:70\u001b[39m, in \u001b[36mTokenizer.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m     69\u001b[39m     \u001b[38;5;28mself\u001b[39m.config_file = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mconfig_file\u001b[39m\u001b[33m\"\u001b[39m, TOKENIZER_CONFIG_FILE)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mself\u001b[39m.file_assets = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Panoptic System\\anaconda3\\envs\\seq2seq\\Lib\\site-packages\\keras_hub\\src\\layers\\preprocessing\\preprocessing_layer.py:10\u001b[39m, in \u001b[36mPreprocessingLayer.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[43massert_tf_libs_installed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(**kwargs)\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# Don't convert inputs (we want tf tensors not backend tensors).\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Panoptic System\\anaconda3\\envs\\seq2seq\\Lib\\site-packages\\keras_hub\\src\\utils\\tensor_utils.py:262\u001b[39m, in \u001b[36massert_tf_libs_installed\u001b[39m\u001b[34m(symbol_name)\u001b[39m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34massert_tf_libs_installed\u001b[39m(symbol_name):\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tf_text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m tf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m    263\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires `tensorflow` and `tensorflow-text` for \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    264\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtext processing. Run `pip install tensorflow-text` to install \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    265\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mboth packages or visit https://www.tensorflow.org/install\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    266\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mIf `tensorflow-text` is already installed, try importing it \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    267\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min a clean python session. Your installation may have errors.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    268\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mKerasHub uses `tf.data` and `tensorflow-text` to preprocess text \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    269\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mon all Keras backends. If you are running on Jax or Torch, this \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    270\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33minstallation does not need GPU support.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    271\u001b[39m         )\n",
      "\u001b[31mImportError\u001b[39m: BytePairTokenizer requires `tensorflow` and `tensorflow-text` for text processing. Run `pip install tensorflow-text` to install both packages or visit https://www.tensorflow.org/install\n\nIf `tensorflow-text` is already installed, try importing it in a clean python session. Your installation may have errors.\n\nKerasHub uses `tf.data` and `tensorflow-text` to preprocess text on all Keras backends. If you are running on Jax or Torch, this installation does not need GPU support."
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "unsplittables = [\"//\", \"--\", \"##\", \"^^\", \"++\", \"::\", \"fs-\", \"ns-\", \"IX\", \"POSS\", \"SELF\"]\n",
    "\n",
    "with open(train_pairs, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split(\"\\t\")\n",
    "    target_text = gloss_cleaning(target_text)\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "\n",
    "tokenizer = keras_hub.tokenizers.BytePairTokenizer(\n",
    "    vocabulary=target_texts,\n",
    "    merges=None,\n",
    "    sequence_length=None,\n",
    "    add_prefix_space=False,\n",
    "    unsplittable_tokens=unsplittables,\n",
    "    dtype=\"int32\",\n",
    ")\n",
    "\n",
    "for char in tokenizer.get_vocabulary():\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype=\"float32\",\n",
    ")\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519de47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea67668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 1s/step - accuracy: 0.7249 - loss: 1.8860 - val_accuracy: 0.8805 - val_loss: 0.6468\n",
      "Epoch 2/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 1s/step - accuracy: 0.8617 - loss: 0.7717 - val_accuracy: 0.8805 - val_loss: 0.6178\n",
      "Epoch 3/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 1s/step - accuracy: 0.8607 - loss: 0.9351 - val_accuracy: 0.8808 - val_loss: 0.5797\n",
      "Epoch 4/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 1s/step - accuracy: 0.8576 - loss: 0.7252 - val_accuracy: 0.8815 - val_loss: 0.5341\n",
      "Epoch 5/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - accuracy: 0.8611 - loss: 0.6205 - val_accuracy: 0.8812 - val_loss: 0.5270\n",
      "Epoch 6/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2s/step - accuracy: 0.8612 - loss: 0.6288 - val_accuracy: 0.8821 - val_loss: 0.5174\n",
      "Epoch 7/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.8646 - loss: 0.5881 - val_accuracy: 0.8819 - val_loss: 0.5066\n",
      "Epoch 8/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2s/step - accuracy: 0.8528 - loss: 0.6268 - val_accuracy: 0.8820 - val_loss: 0.5006\n",
      "Epoch 9/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8593 - loss: 0.6313 - val_accuracy: 0.8807 - val_loss: 0.4997\n",
      "Epoch 10/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8671 - loss: 0.5551 - val_accuracy: 0.8814 - val_loss: 0.4972\n",
      "Epoch 11/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8642 - loss: 0.5660 - val_accuracy: 0.8820 - val_loss: 0.4914\n",
      "Epoch 12/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8661 - loss: 0.5544 - val_accuracy: 0.8826 - val_loss: 0.4867\n",
      "Epoch 13/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8638 - loss: 0.5592 - val_accuracy: 0.8805 - val_loss: 0.4844\n",
      "Epoch 14/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8635 - loss: 0.5525 - val_accuracy: 0.8829 - val_loss: 0.4771\n",
      "Epoch 15/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 2s/step - accuracy: 0.8670 - loss: 0.5354 - val_accuracy: 0.8831 - val_loss: 0.4713\n",
      "Epoch 16/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8578 - loss: 0.5690 - val_accuracy: 0.8833 - val_loss: 0.4731\n",
      "Epoch 17/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8662 - loss: 0.6073 - val_accuracy: 0.8833 - val_loss: 0.4642\n",
      "Epoch 18/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8652 - loss: 0.5303 - val_accuracy: 0.8831 - val_loss: 0.4567\n",
      "Epoch 19/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8664 - loss: 0.5233 - val_accuracy: 0.8853 - val_loss: 0.4482\n",
      "Epoch 20/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.8640 - loss: 0.5533 - val_accuracy: 0.8847 - val_loss: 0.4562\n",
      "Epoch 21/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8673 - loss: 0.5193 - val_accuracy: 0.8874 - val_loss: 0.4389\n",
      "Epoch 22/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8661 - loss: 0.5193 - val_accuracy: 0.8876 - val_loss: 0.4339\n",
      "Epoch 23/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8694 - loss: 0.5026 - val_accuracy: 0.8878 - val_loss: 0.5279\n",
      "Epoch 24/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8663 - loss: 0.6417 - val_accuracy: 0.8924 - val_loss: 0.4196\n",
      "Epoch 25/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8711 - loss: 0.4965 - val_accuracy: 0.8907 - val_loss: 0.4170\n",
      "Epoch 26/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8745 - loss: 0.4823 - val_accuracy: 0.8934 - val_loss: 0.4077\n",
      "Epoch 27/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8748 - loss: 0.4769 - val_accuracy: 0.8974 - val_loss: 0.4055\n",
      "Epoch 28/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8833 - loss: 0.4491 - val_accuracy: 0.9004 - val_loss: 0.3912\n",
      "Epoch 29/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8791 - loss: 0.4638 - val_accuracy: 0.9037 - val_loss: 0.3755\n",
      "Epoch 30/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8844 - loss: 0.4707 - val_accuracy: 0.8960 - val_loss: 0.4075\n",
      "Epoch 31/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.8769 - loss: 0.4752 - val_accuracy: 0.9047 - val_loss: 0.3742\n",
      "Epoch 32/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8884 - loss: 0.4331 - val_accuracy: 0.9067 - val_loss: 0.3625\n",
      "Epoch 33/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8912 - loss: 0.4170 - val_accuracy: 0.9067 - val_loss: 0.3548\n",
      "Epoch 34/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8906 - loss: 0.4201 - val_accuracy: 0.9083 - val_loss: 0.3504\n",
      "Epoch 35/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8944 - loss: 0.4070 - val_accuracy: 0.9099 - val_loss: 0.3430\n",
      "Epoch 36/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8937 - loss: 0.4032 - val_accuracy: 0.9110 - val_loss: 0.3361\n",
      "Epoch 37/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8949 - loss: 0.4021 - val_accuracy: 0.9126 - val_loss: 0.3299\n",
      "Epoch 38/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8961 - loss: 0.3945 - val_accuracy: 0.0475 - val_loss: 3.8323\n",
      "Epoch 39/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.7613 - loss: 0.9447 - val_accuracy: 0.9146 - val_loss: 0.3235\n",
      "Epoch 40/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.9002 - loss: 0.3774 - val_accuracy: 0.9162 - val_loss: 0.3153\n",
      "Epoch 41/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.9014 - loss: 0.3710 - val_accuracy: 0.9180 - val_loss: 0.3115\n",
      "Epoch 42/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.8982 - loss: 0.3821 - val_accuracy: 0.9178 - val_loss: 0.3115\n",
      "Epoch 43/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.9031 - loss: 0.3648 - val_accuracy: 0.9184 - val_loss: 0.3061\n",
      "Epoch 44/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.9010 - loss: 0.3690 - val_accuracy: 0.9185 - val_loss: 0.3038\n",
      "Epoch 45/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.9049 - loss: 0.3547 - val_accuracy: 0.9211 - val_loss: 0.2954\n",
      "Epoch 46/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.9057 - loss: 0.3501 - val_accuracy: 0.9190 - val_loss: 0.2981\n",
      "Epoch 47/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.9053 - loss: 0.3498 - val_accuracy: 0.9213 - val_loss: 0.2963\n",
      "Epoch 48/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.9090 - loss: 0.3364 - val_accuracy: 0.9215 - val_loss: 0.2900\n",
      "Epoch 49/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.9048 - loss: 0.3499 - val_accuracy: 0.9224 - val_loss: 0.2830\n",
      "Epoch 50/50\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2s/step - accuracy: 0.9067 - loss: 0.3424 - val_accuracy: 0.9228 - val_loss: 0.2844\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.2,\n",
    ")\n",
    "# Save model\n",
    "model.save(\"s2s_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6570ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "# Restore the model and construct the encoder and decoder.\n",
    "model = keras.models.load_model(\"s2s_model.keras\")\n",
    "\n",
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value, verbose=0\n",
    "        )\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a76a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: That first pathway has a visual-tactile orientation. The other path is audiovocal.\n",
      "Decoded sentence: IX-1p FISE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p \n",
      "-\n",
      "Input sentence: They talk about the superiority of vision, and the \"tyranny of the hegemony of sound\".\n",
      "Decoded sentence: IX-1p FISE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p \n",
      "-\n",
      "Input sentence: Deaf people made progress with the growth of signing. The progress could have been greater, but it wasn't permitted.\n",
      "Decoded sentence: IX-1p FISE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p \n",
      "-\n",
      "Input sentence: The Deaf experience has been shown to be really different. The Deaf experience the dominance of sound around them.\n",
      "Decoded sentence: IX-1p FISE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p \n",
      "-\n",
      "Input sentence: It's possible to enable people to learn about different orientations.\n",
      "Decoded sentence: IX-1p FISE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p \n",
      "-\n",
      "Input sentence: The social institutions influence everything from medication to education. It would be possible to train people to learn to see things in a different way.\n",
      "Decoded sentence: IX-1p FISE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p \n",
      "-\n",
      "Input sentence: Many colleagues in cultural and visual studies discuss the role of the senses in today's world.\n",
      "Decoded sentence: IX-1p FISE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p \n",
      "-\n",
      "Input sentence: The discussion so far then shows two different orientations - in relation to the development of language.\n",
      "Decoded sentence: IX-1p FISE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p \n",
      "-\n",
      "Input sentence: In the hierarchy of the senses, the visual is now placed above sound.\n",
      "Decoded sentence: IX-1p FISE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p \n",
      "-\n",
      "Input sentence: Often because judgments come from the audio-oral perspective, the sensory politics results in judgments of broken sounds and harsh noises, This needs to be improved.\n",
      "Decoded sentence: IX-1p FISE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p \n",
      "-\n",
      "Input sentence: In their lives, all Deaf people really experience the hegemony of sound.\n",
      "Decoded sentence: IX-1p FISE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FINE IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p FOT IX-1p \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m seq_index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m20\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;66;03m# Take one sequence (part of the training set)\u001b[39;00m\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# for trying out decoding.\u001b[39;00m\n\u001b[32m      4\u001b[39m     input_seq = encoder_input_data[seq_index : seq_index + \u001b[32m1\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     decoded_sentence = \u001b[43mdecode_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInput sentence:\u001b[39m\u001b[33m\"\u001b[39m, input_texts[seq_index])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mdecode_sequence\u001b[39m\u001b[34m(input_seq)\u001b[39m\n\u001b[32m     43\u001b[39m decoded_sentence = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stop_condition:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     output_tokens, h, c = \u001b[43mdecoder_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtarget_seq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mstates_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;66;03m# Sample a token\u001b[39;00m\n\u001b[32m     50\u001b[39m     sampled_token_index = np.argmax(output_tokens[\u001b[32m0\u001b[39m, -\u001b[32m1\u001b[39m, :])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Panoptic System\\anaconda3\\envs\\seq2seq\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Panoptic System\\anaconda3\\envs\\seq2seq\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:505\u001b[39m, in \u001b[36mTensorFlowTrainer.predict\u001b[39m\u001b[34m(self, x, batch_size, verbose, steps, callbacks)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;129m@traceback_utils\u001b[39m.filter_traceback\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(\n\u001b[32m    502\u001b[39m     \u001b[38;5;28mself\u001b[39m, x, batch_size=\u001b[38;5;28;01mNone\u001b[39;00m, verbose=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, steps=\u001b[38;5;28;01mNone\u001b[39;00m, callbacks=\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    503\u001b[39m ):\n\u001b[32m    504\u001b[39m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m     epoch_iterator = \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    514\u001b[39m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[32m    515\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module.CallbackList):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Panoptic System\\anaconda3\\envs\\seq2seq\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:726\u001b[39m, in \u001b[36mTFEpochIterator.__init__\u001b[39m\u001b[34m(self, distribute_strategy, *args, **kwargs)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(*args, **kwargs)\n\u001b[32m    725\u001b[39m \u001b[38;5;28mself\u001b[39m._distribute_strategy = distribute_strategy\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m dataset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata_adapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf.distribute.DistributedDataset):\n\u001b[32m    728\u001b[39m     dataset = \u001b[38;5;28mself\u001b[39m._distribute_strategy.experimental_distribute_dataset(\n\u001b[32m    729\u001b[39m         dataset\n\u001b[32m    730\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Panoptic System\\anaconda3\\envs\\seq2seq\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:235\u001b[39m, in \u001b[36mArrayDataAdapter.get_tf_dataset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shuffle == \u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    233\u001b[39m     indices_dataset = indices_dataset.map(tf.random.shuffle)\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m dataset = \u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    237\u001b[39m options = tf.data.Options()\n\u001b[32m    238\u001b[39m options.experimental_distribute.auto_shard_policy = (\n\u001b[32m    239\u001b[39m     tf.data.experimental.AutoShardPolicy.DATA\n\u001b[32m    240\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Panoptic System\\anaconda3\\envs\\seq2seq\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:197\u001b[39m, in \u001b[36mArrayDataAdapter.get_tf_dataset.<locals>.slice_inputs\u001b[39m\u001b[34m(indices_dataset, inputs)\u001b[39m\n\u001b[32m    191\u001b[39m inputs = array_slicing.convert_to_sliceable(\n\u001b[32m    192\u001b[39m     \u001b[38;5;28mself\u001b[39m._inputs, target_backend=\u001b[33m\"\u001b[39m\u001b[33mtensorflow\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    193\u001b[39m )\n\u001b[32m    194\u001b[39m inputs = tree.lists_to_tuples(inputs)\n\u001b[32m    196\u001b[39m dataset = tf.data.Dataset.zip(\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     (indices_dataset, \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m.repeat())\n\u001b[32m    198\u001b[39m )\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgrab_batch\u001b[39m(i, data):\n\u001b[32m    201\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgrab_one\u001b[39m(x):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Panoptic System\\anaconda3\\envs\\seq2seq\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:741\u001b[39m, in \u001b[36mDatasetV2.from_tensors\u001b[39m\u001b[34m(tensors, name)\u001b[39m\n\u001b[32m    737\u001b[39m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[32m    738\u001b[39m \u001b[38;5;66;03m# from_tensors_op -> dataset_ops).\u001b[39;00m\n\u001b[32m    739\u001b[39m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m from_tensors_op\n\u001b[32m--> \u001b[39m\u001b[32m741\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensors_op\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_from_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Panoptic System\\anaconda3\\envs\\seq2seq\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensors_op.py:23\u001b[39m, in \u001b[36m_from_tensors\u001b[39m\u001b[34m(tensors, name)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_from_tensors\u001b[39m(tensors, name):  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Panoptic System\\anaconda3\\envs\\seq2seq\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\from_tensors_op.py:35\u001b[39m, in \u001b[36m_TensorDataset.__init__\u001b[39m\u001b[34m(self, element, name)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mself\u001b[39m._tensors = structure.to_tensor_list(\u001b[38;5;28mself\u001b[39m._structure, element)\n\u001b[32m     34\u001b[39m \u001b[38;5;28mself\u001b[39m._name = name\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m variant_tensor = \u001b[43mgen_dataset_ops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstructure\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_flat_tensor_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_structure\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_metadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(variant_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Panoptic System\\anaconda3\\envs\\seq2seq\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:7711\u001b[39m, in \u001b[36mtensor_dataset\u001b[39m\u001b[34m(components, output_shapes, metadata, name)\u001b[39m\n\u001b[32m   7709\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tld.is_eager:\n\u001b[32m   7710\u001b[39m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m7711\u001b[39m     _result = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   7712\u001b[39m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTensorDataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moutput_shapes\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   7713\u001b[39m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   7714\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[32m   7715\u001b[39m   \u001b[38;5;28;01mexcept\u001b[39;00m _core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for seq_index in range(20):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index : seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(\"-\")\n",
    "    print(\"Input sentence:\", input_texts[seq_index])\n",
    "    print(\"Decoded sentence:\", decoded_sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seq2seq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
