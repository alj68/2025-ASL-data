{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43c1584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adrianajimenez/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/adrianajimenez/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import keras_hub\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "\n",
    "import tensorflow.data as tf_data\n",
    "from tensorflow_text.tools.wordpiece_vocab import (\n",
    "    bert_vocab_from_dataset as bert_vocab,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df1779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for pre-parsing\n",
    "\n",
    "from pyparsing import Word, alphas as pp_alpha, nums as pp_nums\n",
    "import pyparsing as pp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd3efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handshapes\n",
    "\n",
    "handshapelist = [\n",
    "  '1',\n",
    "  '3',\n",
    "  '4',\n",
    "  '5',\n",
    "  '6',\n",
    "  '7',\n",
    "  '8',\n",
    "  '9',\n",
    "  '10',\n",
    "  '25',\n",
    "  'A',\n",
    "  'B',\n",
    "  'C',\n",
    "  'D',\n",
    "  'E',\n",
    "  'F',\n",
    "  'G',\n",
    "  'H-U',\n",
    "  'I',\n",
    "  'K',\n",
    "  'L',\n",
    "  'M',\n",
    "  'N',\n",
    "  'O',\n",
    "  'R',\n",
    "  'S',\n",
    "  'T',\n",
    "  'U',\n",
    "  'V',\n",
    "  'W',\n",
    "  'X',\n",
    "  'Y',\n",
    "  'C-L',\n",
    "  'U-L',\n",
    "  'B-L',\n",
    "  'P-K',\n",
    "  'Q-G',\n",
    "  'L-X',\n",
    "  'I-L-Y',\n",
    "  '5-C',\n",
    "  '5-C-L',\n",
    "  '5-C-tt',\n",
    "  'alt-M',\n",
    "  'alt-N',\n",
    "  'alt-P',\n",
    "  'B-xd',\n",
    "  'baby-O',\n",
    "  'bent-1',\n",
    "  'bent-B',\n",
    "  'bent-B-L',\n",
    "  'bent-horns',\n",
    "  'bent-L',\n",
    "  'bent-M',\n",
    "  'bent-N',\n",
    "  'bent-U',\n",
    "  'cocked-8',\n",
    "  'cocked-F',\n",
    "  'cocked-S',\n",
    "  'crvd-5',\n",
    "  'crvd-B',\n",
    "  'crvd-flat-B',\n",
    "  'crvd-L',\n",
    "  'crvd-sprd-B',\n",
    "  'crvd-U',\n",
    "  'crvd-V',\n",
    "  'fanned-flat-O',\n",
    "  'flat-B',\n",
    "  'flat-C',\n",
    "  'flat-F',\n",
    "  'flat-G',\n",
    "  'flat-O-2',\n",
    "  'flat-O',\n",
    "  'full-M',\n",
    "  'horns',\n",
    "  'loose-E',\n",
    "  'O-2-horns',\n",
    "  'open-8',\n",
    "  'open-F',\n",
    "  'sml-C-3',\n",
    "  'tight-C-2',\n",
    "  'tight-C',\n",
    "  'X-over-thumb'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0a3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex rules\n",
    "\n",
    "number_regexp = r\"[0-9]+(?:\\.[0-9]+)?s?\"\n",
    "alpha_regexp = r\"(?!((?:THUMB-)?(?:IX-|POSS-|SELF-)))[A-Z0-9](?:[A-Z0-9/'-]*[A-Z0-9])?(?:\\.|:[0-9])?\"\n",
    "lookahead_regexp = r\"(?:(?![a-z])|(?=wg))\"\n",
    "\n",
    "word_all_regexp = r\"\"\"(?x)\n",
    "    (?: %s | %s )\n",
    "    %s\n",
    "\"\"\" % (number_regexp, alpha_regexp, lookahead_regexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbd4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conventions kept for parsing\n",
    "\n",
    "cl_prefix = pp.one_of([\"CL\", \"DCL\", \"LCL\", \"SCL\", \"BCL\", \"BPCL\", \"PCL\", \"ICL\"])\n",
    "ns_prefix = pp.Literal(\"ns\")\n",
    "fs_prefix = pp.Literal(\"fs\")\n",
    "lex_exceptions = pp.one_of([\"part\", \"WHAT\"])\n",
    "aspect_text = pp.Literal(\"aspect\")\n",
    "index_core_ix = pp.Literal(\"IX\")\n",
    "other_index_core = pp.one_of([\"POSS\", \"SELF\"])\n",
    "handshape = pp.one_of(handshapelist)\n",
    "person = pp.one_of([\"1p\", \"2p\", \"3p\"])\n",
    "arc = pp.Literal(\"arc\") \n",
    "loc = pp.Literal(\"loc\")\n",
    "pl = pp.Literal(\"pl\")\n",
    "compound = pp.Literal(\"+\")\n",
    "hashtag = pp.Literal(\"#\")\n",
    "sym = pp.Literal(\">\")\n",
    "par1 = pp.Literal(\"(\")\n",
    "par2 = pp.Literal(\")\")\n",
    "dash = pp.Literal(\"-\")\n",
    "contraction = pp.Literal(\"^\")\n",
    "colon = pp.Literal(\":\")\n",
    "omit_quote = pp.Literal(\"xx\")\n",
    "period = pp.Literal(\".\")\n",
    "alpha = Word(pp_alpha, max=1)\n",
    "num = Word(pp_nums, max=1)\n",
    "word = pp.Regex(word_all_regexp, flags=re.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b062760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammar rules\n",
    "\n",
    "full_grammar = pp.OneOrMore(\n",
    "    cl_prefix |               # classifiers like CL, DCL, etc.\n",
    "    word |\n",
    "    ns_prefix |               # non-specific ns\n",
    "    fs_prefix |               # fingerspelling fs\n",
    "    index_core_ix |           # IX\n",
    "    other_index_core |        # POSS, SELF\n",
    "    person |                  # 1p, 2p, 3p\n",
    "    lex_exceptions |          # part, WHAT\n",
    "    aspect_text |             # aspect\n",
    "    arc |                     # arc\n",
    "    loc |                     # loc\n",
    "    pl |                      # pl\n",
    "    handshape |               # handshapes like B, 1, 5, etc.\n",
    "    compound |                # +\n",
    "    hashtag |                 # #\n",
    "    sym |                     # >\n",
    "    contraction |             # ^\n",
    "    colon |                   # :\n",
    "    par1 | par2 |             # ( and )\n",
    "    omit_quote |              # xx\n",
    "    period |                  # .\n",
    "    dash |\n",
    "    num |                    # numbers last resort\n",
    "    alpha                     # fallback LAST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45096aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCL', ':', '1', 'xx']\n",
      "['IX', '-', '1p', 'BCL', 'xx', 'FIND/FIND-OUT', 'fs', '-', 'HER']\n"
     ]
    }
   ],
   "source": [
    "# testing grammar parsing\n",
    "\n",
    "trial = full_grammar.parse_string(\"SCL:1xx\", parse_all=True).asList()\n",
    "trial2 = full_grammar.parse_string(\"IX-1p BCLxx FIND/FIND-OUT fs-HER\", parse_all=True).asList()\n",
    "\n",
    "print(trial)\n",
    "print(trial2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258c52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize based on predefined grammar rules\n",
    "\n",
    "def custom_asl_tokenize(text):\n",
    "    try:\n",
    "        return full_grammar.parse_string(text, parse_all=True).asList()\n",
    "    except pp.ParseException as pe:\n",
    "        print(f\"Failed to parse: {pe}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03d3e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_eng_tokenize(text):\n",
    "    # Add spaces around punctuation (preserving it)\n",
    "    text = re.sub(r'([^\\w\\s])', r' \\1 ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Split on whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cc0cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCL', ':', '1', 'xx']\n",
      "['SCL', ':', '1', 'xx', 'SHOWER', 'WASH', 'FEEL', 'THUMBS-UP/GOOD']\n"
     ]
    }
   ],
   "source": [
    "# testing custom_asl_tokenize\n",
    "\n",
    "trial = custom_asl_tokenize(\"SCL:1xx\")\n",
    "trial2 = custom_asl_tokenize('SCL:1xx SHOWER WASH FEEL THUMBS-UP/GOOD')\n",
    "\n",
    "print(trial)\n",
    "print(trial2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e79a6b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters / hyperparameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30  # This should be at least 10 for convergence\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "ENG_VOCAB_SIZE = 1692 + 4\n",
    "ASL_VOCAB_SIZE = 1130 + 4\n",
    "num_samples = 1400\n",
    "\n",
    "EMBED_DIM = 128\n",
    "INTERMEDIATE_DIM = 1024\n",
    "NUM_HEADS = 8\n",
    "data_path = \"/Users/adrianajimenez/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/data/sent_pairs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3df98b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse: Expected end of text, found '/'  (at char 19), (line:1, col:20)\n",
      "Failed to parse: Expected end of text, found '/'  (at char 23), (line:1, col:24)\n",
      "eng_tokens: ['!', '\"', '$', \"'\", '(', ')', '*', ',', '-', '.', '/', '1', '10', '100', '11', '110', '150', '3', '30', '300', '350', '4', '45', '5', '50', '500', '50s', '60s', '7', '70', '75', '8', '80', '85', '88', '9', '90', '96', ':', ';', '?', 'a', 'abdominal', 'able', 'about', 'accepted', 'accident', 'across', 'act', 'action', 'actually', 'admit', 'advantage', 'adventure', 'advice', 'after', 'again', 'against', 'ago', 'agree', 'ah', 'ahead', 'ahh', 'ail', 'air', 'ali', 'all', 'alone', 'along', 'already', 'alright', 'also', 'am', 'amazing', 'an', 'and', 'angeles', 'angled', 'angry', 'animal', 'ankle', 'ann', 'another', 'answer', 'any', 'anymore', 'anyone', 'anything', 'anyway', 'anywhere', 'applaud', 'apple', 'approach', 'are', 'area', 'arm', 'arms', 'around', 'around12', 'arrest', 'arrested', 'arrive', 'arrived', 'arrives', 'arriving', 'as', 'aside', 'ask', 'asked', 'asks', 'assistant', 'at', 'ate', 'attack', 'attacker', 'attention', 'average', 'awake', 'away', 'awful', 'b', 'babies', 'baby', 'back', 'bad', 'bag', 'bags', 'ball', 'band', 'bandage', 'banged', 'banging', 'banks', 'bar', 'barely', 'basketball', 'bathroom', 'be', 'beat', 'beating', 'beautiful', 'became', 'because', 'become', 'bed', 'been', 'beer', 'before', 'began', 'begin', 'behind', 'being', 'believe', 'belong', 'belt', 'beneath', 'bent', 'best', 'better', 'between', 'beverly', 'bib', 'big', 'bigger', 'bill', 'birth', 'bit', 'bite', 'bladder', 'blames', 'bled', 'bleed', 'bleeding', 'blew', 'block', 'blood', 'blown', 'blue', 'boat', 'body', 'book', 'books', 'boots', 'border', 'bored', 'boring', 'born', 'borrowed', 'boss', 'boston', 'bostonians', 'both', 'bottle', 'bought', 'bowl', 'box', 'boxer', 'boxes', 'boxing', 'boy', 'boys', 'braced', 'brakes', 'bread', 'break', 'breakfast', 'breath', 'breathe', 'brick', 'bricks', 'bring', 'brings', 'broccoli', 'broke', 'broken', 'brother', 'brought', 'brownish', 'bruise', 'bucks', 'build', 'building', 'buildings', 'built', 'bumps', 'bun', 'bunch', 'bundled', 'burger', 'burn', 'burned', 'bus', 'bush', 'business', 'but', 'butcher', 'butts', 'buy', 'buying', 'by', 'cabin', 'cabinet', 'calfornia', 'california', 'call', 'called', 'calm', 'came', 'camp', 'can', 'candy', 'cannot', 'cape', 'car', 'care', 'carrying', 'cars', 'case', 'casino', 'catch', 'caught', 'cause', 'caused', 'celebrate', 'celebrated', 'celebrating', 'celebrations', 'chair', 'chairs', 'challenge', 'change', 'changed', 'changing', 'channel', 'chased', 'chasing', 'chat', 'chatted', 'check', 'checks', 'cheer', 'chemicals', 'chess', 'chicago', 'chicken', 'chimney', 'china', 'chocolate', 'choice', 'choices', 'chokes', 'choose', 'chopped', 'churning', 'cigarette', 'cigarettes', 'cities', 'city', 'class', 'clean', 'clear', 'clearly', 'climb', 'climbing', 'cliques', 'clock', 'close', 'closed', 'closer', 'clubs', 'clue', 'cnn', 'coach', 'coat', 'coats', 'cod', 'coffee', 'cofffee', 'coffin', 'cohesiveness', 'cold', 'colder', 'collar', 'collected', 'college', 'color', 'colorado', 'come', 'comfortable', 'coming', 'communicate', 'communicated', 'community', 'company', 'compare', 'compares', 'compelled', 'competition', 'complete', 'completely', 'confidence', 'confident', 'confusion', 'continue', 'continued', 'continues', 'contributed', 'control', 'cooked', 'cool', 'cooler', 'coordinator', 'cop', 'copies', 'cops', 'copy', 'corn', 'corner', 'costs', 'coughing', 'could', 'couldn', 'counselor', 'country', 'couple', 'course', 'cover', 'covers', 'cows', 'cpr', 'cracked', 'crashed', 'crashes', 'crazy', 'create', 'creative', 'creepy', 'crib', 'cried', 'crops', 'cross', 'crossing', 'cruiser', 'crying', 'csun', 'cuff', 'cultural', 'culture', 'curvy', 'cut', 'cute', 'cutting', 'd', 'dairy', 'daisy', 'dana', 'dance', 'danced', 'dark', 'date', 'dawn', 'day', 'days', 'dead', 'deaf', 'deafies', 'deal', 'december', 'decided', 'decorate', 'deep', 'deeply', 'deer', 'definitely', 'degrees', 'delicious', 'destroy', 'did', 'didn', 'didnt', 'died', 'difference', 'differences', 'different', 'dinner', 'direction', 'disappointed', 'disciplinary', 'discussed', 'disease', 'dislike', 'dissipate', 'distance', 'diversity', 'do', 'doctor', 'doctors', 'does', 'doesn', 'doing', 'dollars', 'don', 'done', 'donut', 'door', 'doorbell', 'doorknob', 'doors', 'dorm', 'dorms', 'down', 'downstairs', 'drag', 'drain', 'drank', 'dream', 'dreams', 'drinking', 'drinks', 'drive', 'drivers', 'drives', 'driving', 'drop', 'dropped', 'drops', 'drove', 'drugs', 'dry', 'dug', 'dumbfounded', 'dumped', 'during', 'each', 'ear', 'ears', 'easier', 'easy', 'eat', 'eaten', 'eating', 'economic', 'edge', 'effort', 'eight', 'either', 'eleven', 'elite', 'else', 'email', 'emergency', 'emotion', 'empty', 'encouraging', 'end', 'ended', 'ends', 'engine', 'english', 'engrossed', 'enjoy', 'enjoyed', 'enjoying', 'enjoys', 'enough', 'entered', 'environments', 'erick', 'escape', 'especially', 'etc', 'europe', 'even', 'eventually', 'ever', 'everwhere', 'every', 'everyday', 'everyone', 'everything', 'example', 'exchanged', 'exchanging', 'excited', 'excused', 'exhausted', 'exit', 'expected', 'expecting', 'expects', 'expensive', 'experiences', 'explain', 'explained', 'express', 'eye', 'face', 'facing', 'fall', 'falls', 'family', 'fan', 'fancy', 'fans', 'far', 'farm', 'farther', 'fascinated', 'fast', 'father', 'favorite', 'feed', 'feel', 'feeling', 'feels', 'feet', 'fell', 'felt', 'fenced', 'few', 'field', 'fifteen', 'finally', 'find', 'finds', 'fine', 'fined', 'finger', 'fingers', 'finish', 'finished', 'finishes', 'finishing', 'fire', 'fireplace', 'first', 'fist', 'fists', 'five', 'fixed', 'flashed', 'flat', 'flavorful', 'flew', 'flood', 'flooding', 'floor', 'flowers', 'fluent', 'focus', 'follow', 'following', 'food', 'foot', 'football', 'for', 'forest', 'forever', 'forget', 'forgot', 'former', 'forth', 'forward', 'found', 'four', 'francisco', 'frank', 'fraternity', 'freak', 'french', 'frends', 'fresh', 'fresher', 'friday', 'friend', 'friendly', 'friends', 'fries', 'from', 'front', 'frozen', 'fruit', 'fruits', 'frustrated', 'fumble', 'fun', 'funny', 'further', 'future', 'gain', 'gamble', 'gambled', 'gambling', 'game', 'gas', 'gather', 'gave', 'geez', 'general', 'generations', 'gesture', 'gestured', 'get', 'gets', 'getting', 'girl', 'girls', 'give', 'given', 'gives', 'giving', 'go', 'god', 'goes', 'going', 'gone', 'good', 'goose', 'got', 'gotten', 'grabbed', 'grandfather', 'grass', 'great', 'green', 'grew', 'gross', 'ground', 'group', 'grow', 'growing', 'guess', 'guy', 'guys', 'had', 'hadn', 'hah', 'haha', 'half', 'hall', 'hamburger', 'hand', 'handcuffed', 'handcuffs', 'handle', 'hands', 'hanging', 'happened', 'happening', 'happens', 'happily', 'happy', 'hard', 'has', 'hasn', 'hastily', 'hate', 'have', 'he', 'head', 'headache', 'headed', 'headlights', 'hear', 'heard', 'hearing', 'heart', 'held', 'helmet', 'her', 'here', 'herself', 'hey', 'hi', 'hid', 'high', 'highway', 'hill', 'hills', 'him', 'himself', 'his', 'hit', 'hitchhiking', 'hits', 'hitting', 'hold', 'holding', 'home', 'homes', 'hope', 'hoped', 'hoping', 'hospital', 'hospitals', 'hot', 'hotel', 'hotter', 'hours', 'house', 'how', 'however', 'huge', 'humid', 'humor', 'hung', 'hungry', 'hurried', 'hurry', 'hurts', 'husky', 'i', 'idea', 'idiot', 'if', 'ignored', 'illinois', 'imagination', 'imagine', 'imagined', 'imagines', 'impacted', 'impatient', 'important', 'impossible', 'in', 'incredulously', 'indiana', 'indicated', 'infection', 'inform', 'information', 'informed', 'ingore', 'inhaled', 'injections', 'injured', 'inside', 'inspiring', 'instructions', 'insurance', 'intense', 'interested', 'intersection', 'interstate', 'into', 'invited', 'involved', 'involves', 'iowa', 'ironic', 'is', 'isn', 'it', 'its', 'jail', 'jana', 'jersey', 'john', 'join', 'joined', 'joint', 'jumping', 'just', 'k', 'keep', 'keeps', 'kept', 'kicks', 'kids', 'killed', 'kind', 'king', 'knee', 'knew', 'know', 'knows', 'l', 'la', 'laboriously', 'laid', 'landed', 'landscaping', 'lane', 'lapd', 'last', 'lasts', 'late', 'lately', 'later', 'laughed', 'laughing', 'lays', 'lazy', 'leader', 'learn', 'learned', 'least', 'leave', 'leaves', 'leaving', 'led', 'left', 'leg', 'legal', 'let', 'lid', 'lies', 'life', 'lightning', 'like', 'liked', 'likes', 'liking', 'limit', 'limited', 'limp', 'lincoln', 'line', 'lips', 'list', 'lit', 'literally', 'littered', 'little', 'live', 'lived', 'll', 'loaf', 'location', 'locked', 'log', 'lombardi', 'long', 'look', 'looked', 'looking', 'looks', 'los', 'lost', 'lot', 'louder', 'loudly', 'lound', 'love', 'loves', 'luck', 'lucky', 'lunch', 'lurches', 'm', 'machine', 'made', 'magazine', 'maine', 'make', 'makers', 'man', 'many', 'march', 'market', 'markets', 'mary', 'match', 'matter', 'maybe', 'mcdonald', 'mcdonalds', 'me', 'mean', 'means', 'meat', 'mechanic', 'medium', 'men', 'menu', 'messing', 'metal', 'meter', 'mexican', 'mexico', 'middle', 'mike', 'miles', 'millions', 'mind', 'minded', 'mine', 'mingling', 'minutes', 'mischevious', 'misunderstood', 'mix', 'mixes', 'mom', 'money', 'more', 'morning', 'most', 'mostly', 'mother', 'motivating', 'motorcycle', 'motorcycles', 'motorcycling', 'mouth', 'move', 'moved', 'movie', 'movies', 'mph', 'mr', 'much', 'muhammad', 'muscles', 'must', 'my', 'myself', 'nah', 'nailed', 'nails', 'naked', 'name', 'named', 'near', 'nearby', 'nebraska', 'need', 'needed', 'needle', 'needs', 'negative', 'neither', 'nervous', 'nevada', 'never', 'new', 'news', 'next', 'nice', 'night', 'nightstick', 'no', 'nobody', 'north', 'northridge', 'not', 'nothing', 'notice', 'noticed', 'now', 'numb', 'number', 'nurse', 'o', 'oakland', 'obviously', 'occasionally', 'of', 'off', 'offense', 'offensive', 'office', 'officers', 'often', 'oh', 'ohio', 'oil', 'ok', 'okay', 'old', 'older', 'omaha', 'on', 'once', 'one', 'ones', 'only', 'onto', 'oozing', 'open', 'opened', 'opens', 'opinion', 'opportunity', 'options', 'or', 'orders', 'other', 'others', 'otherwise', 'our', 'out', 'outgoing', 'outside', 'outta', 'over', 'overnight', 'own', 'pace', 'paddle', 'pageant', 'pager', 'pain', 'panic', 'pans', 'pants', 'paper', 'parents', 'parkinson', 'party', 'pass', 'passed', 'past', 'path', 'patient', 'paul', 'pay', 'pee', 'peeked', 'pen', 'people', 'percent', 'perfect', 'perpendicular', 'person', 'personally', 'persuit', 'phone', 'pick', 'picked', 'picks', 'picture', 'pieces', 'pig', 'pigs', 'pile', 'pill', 'place', 'places', 'plain', 'planning', 'plans', 'plants', 'plastic', 'play', 'player', 'players', 'plays', 'plus', 'pocket', 'pointed', 'police', 'policies', 'policy', 'pond', 'popped', 'popping', 'positive', 'post', 'potatoes', 'pots', 'pounding', 'pounds', 'poured', 'pours', 'practically', 'prefer', 'preferred', 'prefers', 'pregnant', 'pressure', 'pretty', 'price', 'pries', 'probably', 'problem', 'problems', 'products', 'profusely', 'pry', 'public', 'pull', 'pulled', 'pulls', 'pump', 'punched', 'punches', 'punching', 'punished', 'punishment', 'pushed', 'put', 'puts', 'quarterback', 'question', 'quick', 'quickly', 'quote', 'ra', 'raft', 'rafting', 'raiders', 'rain', 'raining', 'rains', 'rainy', 'raised', 'ran', 'rang', 'rapids', 'ras', 'rather', 'ray', 're', 'reached', 'reaches', 'read', 'reading', 'reads', 'ready', 'real', 'realize', 'realized', 'really', 'reason', 'reassures', 'receiver', 'recommend', 'record', 'refused', 'regular', 'regularly', 'relief', 'relieved', 'remember', 'reminds', 'removes', 'repeatedly', 'replaces', 'replacing', 'reply', 'report', 'reporter', 'require', 'requires', 'residence', 'residences', 'restaurant', 'restaurants', 'result', 'results', 'ribs', 'riding', 'right', 'ring', 'rises', 'river', 'road', 'rock', 'rodney', 'rollerbladed', 'rollerblades', 'roof', 'room', 'roommates', 'rope', 'rose', 'route', 'row', 'rowing', 'rubber', 'rummaged', 'run', 'running', 'runs', 'rushes', 's', 'sack', 'sad', 'safe', 'said', 'same', 'san', 'sank', 'sarcastically', 'sat', 'saw', 'say', 'says', 'scared', 'school', 'scissors', 'score', 'scream', 'screamed', 'seafood', 'seasons', 'seat', 'seats', 'second', 'security', 'see', 'seeing', 'seem', 'seemed', 'seems', 'seen', 'sell', 'semester', 'send', 'sent', 'sentence', 'separated', 'september', 'serious', 'service', 'set', 'sets', 'setting', 'seven', 'sewer', 'sf', 'shake', 'shaken', 'shakes', 'shaking', 'shampoo', 'sharp', 'she', 'sheep', 'shines', 'shirt', 'shirts', 'shock', 'shocked', 'shoes', 'shone', 'shook', 'shoots', 'shop', 'shopping', 'shorts', 'shot', 'should', 'shoulder', 'shouldn', 'show', 'showed', 'shower', 'siberian', 'side', 'sign', 'signer', 'signing', 'silk', 'simple', 'since', 'siren', 'sister', 'sit', 'sitting', 'situation', 'six', 'sizes', 'skidding', 'slammed', 'slaughtering', 'sleep', 'sleeping', 'sleeps', 'sleeves', 'sliced', 'slide', 'slings', 'slow', 'slowly', 'small', 'smaller', 'smell', 'smoking', 'smoothly', 'snatch', 'sneak', 'snows', 'snuck', 'so', 'social', 'socially', 'sold', 'some', 'someone', 'something', 'sometimes', 'somewhere', 'soon', 'sore', 'sorry', 'sort', 'spare', 'speak', 'special', 'spedometer', 'speech', 'speed', 'spit', 'spoke', 'sport', 'sports', 'spots', 'sprained', 'spring', 'spun', 'staff', 'stand', 'standing', 'stared', 'staring', 'start', 'started', 'starts', 'state', 'states', 'station', 'stay', 'stayed', 'stays', 'steak', 'steam', 'stiill', 'still', 'stitched', 'stitches', 'stitching', 'stolen', 'stomach', 'stood', 'stop', 'stopped', 'stops', 'store', 'storm', 'story', 'straight', 'strange', 'strangers', 'strict', 'strikes', 'string', 'stripes', 'strong', 'struck', 'struggling', 'stuck', 'student', 'students', 'sudden', 'suddenly', 'sue', 'summer', 'sun', 'sundown', 'sunny', 'sunrise', 'sunset', 'super', 'superior', 'supposed', 'sure', 'surgeon', 'surprised', 'survive', 'swerved', 'swinging', 'switched', 'switching', 'system', 'systems', 't', 'table', 'tables', 'tag', 'tail', 'take', 'takes', 'taking', 'talk', 'talked', 'talking', 'tank', 'tape', 'tapped', 'tappee', 'tapper', 'taps', 'taste', 'tea', 'teach', 'teacher', 'team', 'teams', 'tease', 'teasing', 'tell', 'telling', 'tells', 'tempted', 'ten', 'tend', 'tended', 'tends', 'tents', 'test', 'than', 'thanks', 'that', 'thats', 'the', 'their', 'them', 'themselves', 'then', 'there', 'they', 'thick', 'thing', 'things', 'think', 'thinking', 'thinks', 'third', 'this', 'those', 'though', 'thought', 'thousand', 'thousands', 'three', 'threw', 'through', 'throw', 'throws', 'thrusts', 'ticked', 'tie', 'tied', 'tigers', 'tight', 'til', 'time', 'times', 'tiny', 'tire', 'tired', 'tires', 'to', 'today', 'together', 'told', 'tomorrow', 'tonight', 'tons', 'tony', 'too', 'took', 'tool', 'top', 'total', 'touch', 'touchdown', 'tougher', 'toward', 'towards', 'toys', 'tray', 'trees', 'tried', 'trip', 'trouble', 'trunk', 'trust', 'try', 'trying', 'turn', 'turned', 'turns', 'tv', 'twenty', 'twice', 'two', 'typical', 'u', 'ugh', 'uh', 'umbrellas', 'under', 'underage', 'understand', 'uneasy', 'unfortunately', 'university', 'unless', 'unpredictable', 'unsatified', 'until', 'untill', 'up', 'upset', 'upstairs', 'us', 'use', 'used', 'using', 'utah', 'various', 'vary', 've', 'vegetables', 'very', 'vibrating', 'victim', 'videotape', 'vince', 'violently', 'visit', 'visited', 'visiting', 'wait', 'waited', 'waiters', 'waiting', 'waitress', 'waive', 'walk', 'walked', 'walking', 'walks', 'wallet', 'want', 'wanted', 'wants', 'wards', 'warmly', 'warn', 'warned', 'was', 'wash', 'washed', 'wasn', 'wasnt', 'wasting', 'watch', 'watched', 'watches', 'watching', 'water', 'wave', 'waved', 'waving', 'way', 'ways', 'we', 'wear', 'wearing', 'weather', 'week', 'weekend', 'weigh', 'welcome', 'welcoming', 'well', 'went', 'were', 'wet', 'what', 'whatever', 'wheel', 'when', 'where', 'whether', 'which', 'while', 'whipping', 'white', 'who', 'whoa', 'whole', 'whom', 'whose', 'why', 'wide', 'wife', 'will', 'willing', 'win', 'winding', 'windows', 'winner', 'winning', 'wish', 'with', 'without', 'wnt', 'woke', 'woken', 'wolf', 'woman', 'won', 'wonder', 'wondered', 'wonderful', 'wondering', 'wood', 'word', 'words', 'work', 'worked', 'worker', 'working', 'worried', 'worry', 'worse', 'worst', 'worth', 'would', 'wouldn', 'wow', 'wrapped', 'write', 'writing', 'written', 'wrong', 'wrote', 'wymoing', 'wymoning', 'wyoming', 'x', 'yawned', 'year', 'years', 'yell', 'yelled', 'yes', 'yesterday', 'yet', 'york', 'you', 'young', 'your', 'yourself', 'zipped', 'zone']\n",
      "output_tokens ['#', '(', ')', '+', '-', '0', '1', '1.50', '100', '110', '150', '1p', '2', '25', '2p', '3', '300', '350', '3p', '4', '45', '5', '50', '50s', '60s', '70', '75', '80', '85', '88', '90', ':', '>', 'A', 'A-WAYS', 'ABOUT', 'ABUSE', 'ACCEPT', 'ACCIDENT', 'ACROSS', 'ADMIT', 'ADVANTAGE', 'ADVENTURE', 'ADVISE', 'ADVISER', 'AFTER', 'AGAIN', 'AGAINST', 'AGE', 'AGE-3', 'AGE-4', 'AGE-5', 'AGE-8', 'AGREE', 'AIR', 'ALI', 'ALL', 'ALL-DAY', 'ALL-GONE', 'ALL-NIGHT', 'ALL-THE-WAY', 'ALMOST', 'ALONE', 'ALRIGHT', 'AND', 'ANGRY', 'ANIMAL', 'ANKLE', 'ANN', 'ANY', 'ANYWAY/NOT-MATTER', 'APPEAR', 'APPLAUSE', 'APPLE', 'APPROACH', 'AREA', 'ARREST', 'ARRIVE', 'ASK', 'ASS', 'ASSISTANT', 'AT', 'AVERAGE', 'AWFUL', 'B', 'BABY', 'BACK', 'BAD', 'BAG', 'BAGS', 'BALL', 'BANDAGE', 'BANKS', 'BASKETBALL', 'BATHROOM', 'BCL', 'BE', 'BEAUTIFUL', 'BECAUSE', 'BECOME', 'BED', 'BEER', 'BEFORE', 'BELIEVE', 'BELONG', 'BELT', 'BEST', 'BETTER', 'BETWEEN', 'BEVERLY', 'BIB', 'BIG', 'BIGGER', 'BILL', 'BLACK', 'BLADDER', 'BLAME', 'BLOCK', 'BLOOD', 'BLOW', 'BLOWN-AWAY', 'BLUE', 'BOAT', 'BONE', 'BOOK', 'BOOT', 'BORDER', 'BORE', 'BORN', 'BORROW', 'BOSS', 'BOSTON', 'BOTH', 'BOX', 'BOXER', 'BOXING', 'BOY', 'BPCL', 'BRAKE', 'BREAD', 'BREAK', 'BREAK-DOWN', 'BREATHE', 'BRICK', 'BRING', 'BROCCOLI', 'BROTHER', 'BROWN', 'BUILD', 'BUILDING', 'BUS', 'BUSH', 'BUSINESS', 'BUT', 'BUTCHER-SHOP', 'BUTTS', 'BUY', 'BY', 'C', 'C-L', 'CABIN', 'CALIFORNIA', 'CALL', 'CALL-BY-PHONE', 'CAMP', 'CAN', 'CANDY', 'CANNOT', 'CAPE-COD', 'CAPECOD', 'CAR', 'CARE', 'CASINO', 'CAUSE', 'CELEBRATE', 'CHAIR', 'CHALLENGE', 'CHANGE', 'CHANNEL', 'CHARACTER', 'CHASE', 'CHAT', 'CHECK', 'CHEER', 'CHEMICAL', 'CHESS', 'CHICAGO', 'CHICKEN', 'CHINA', 'CHOCOLATE', 'CHOICE', 'CHOP', 'CIGARETTE', 'CITY/COMMUNITY', 'CL', 'CNN', 'COACH', 'COAT', 'COFFEE', 'COFFIN', 'COLD', 'COLLECT', 'COLLEGE', 'COLOR', 'COLORADO', 'COME', 'COME-ON', 'COMFORTABLE', 'COMMUNICATE', 'COMPANY', 'COMPARE', 'CONCEPT', 'CONFIDENT', 'CONFUSE', 'CONTACT', 'CONTINUE', 'COOK', 'COOL', 'COOLER', 'COORDINATOR', 'COP', 'COPY', 'CORN', 'CORNER', 'COUGH', 'COUNT-ON-FINGERS', 'COUNTRY', 'COW', 'CPR', 'CRACK', 'CRASH', 'CRAZY', 'CREATE/PRETEND', 'CREEPY', 'CRIB', 'CROPS', 'CRY', 'CSUN', 'CULTURE', 'CUT', 'CUTE', 'CUTTER', 'DAISY', 'DANA', 'DANCE', 'DARK', 'DATE', 'DAY', 'DCL', 'DEAD', 'DEAF', 'DEC', 'DECIDE', 'DECORATE', 'DEER', 'DEGREE', 'DELICIOUS', 'DEPART', 'DEPRESS', 'DESTROY', 'DEVIL', 'DIE', 'DIFFERENT', 'DISAPPOINT', 'DISCIPLINE', 'DISCONNECT', 'DISEASE', 'DO', 'DOCTOR', 'DOLLAR', 'DONUT', 'DOOR', 'DORM', 'DOWN', 'DOWN-THE-LIST', 'DREAM', 'DRINK', 'DRIVE', 'DROP', 'DRUG', 'DRUGS', 'DRY', 'DURING/WHILE', 'E', 'EACH', 'EAN', 'EAR', 'EASIER', 'EAT', 'EAT-UP', 'ECONOMIC', 'EIGHTY', 'ELEVEN', 'ELITE', 'EMAIL', 'EMERGENCY', 'EMOTION', 'EMPTY', 'END', 'ENGINE', 'ENGLISH', 'ENJOY', 'ENOUGH', 'ENTER', 'ENVIRONMENT', 'EQUAL', 'ERICK', 'ESCAPE', 'ETC.', 'EUROPE', 'EVEN', 'EVER', 'EVERY', 'EVERYDAY', 'EX', 'EXAM', 'EXAMPLE', 'EXCITE', 'EXCITED', 'EXCUSE', 'EXCUSE-GO', 'EXHAUST', 'EXPECT', 'EXPENSIVE', 'EXPERIENCE', 'EXPLAIN', 'EXPRESS', 'EXTEND', 'F', 'FACE', 'FALL', 'FALL-ASLEEP', 'FALL-INTO-IT', 'FALL-INTO-PLACE', 'FAMILY', 'FAN', 'FANCY', 'FANS', 'FAR', 'FARM', 'FASCINATED', 'FAST', 'FATHER', 'FAVORITE/PREFER', 'FEED', 'FEEL', 'FEET', 'FENCE', 'FEW', 'FF', 'FIELD', 'FIFTEEN', 'FILM', 'FINALLY/SUCCEED', 'FIND', 'FIND/FIND-OUT', 'FINE', 'FINISH', 'FIRE', 'FIRST', 'FIVE', 'FIVE-MINUTE', 'FIX', 'FLAT', 'FLOOD', 'FLOOR', 'FLOWER', 'FOCUS', 'FOLLOW', 'FOOD', 'FOOT', 'FOOTBALL', 'FOR', 'FOREVER', 'FORGET', 'FORGET-IT', 'FORMERLY', 'FORTUNATELY', 'FOUR', 'FOUR-DAY', 'FOUR-THIRTY', 'FRANK', 'FRATERNITY', 'FREAK', 'FRESH', 'FRIDAY', 'FRIEND', 'FRIENDLY', 'FROM', 'FROM-NOW-ON', 'FRONT', 'FRUGAL', 'FRUIT', 'FS-ZONE', 'FULL', 'FUMBLE', 'FUN', 'FUNNY', 'FUTURE', 'G', 'GAMBLE', 'GAME', 'GAS', 'GENERAL', 'GESTURE', 'GET', 'GET-IN-BED', 'GET-UP', 'GIFT', 'GIRL', 'GIVE', 'GO', 'GO-AHEAD', 'GO-AWAY', 'GO-OUT', 'GOD', 'GONE', 'GOOD', 'GRAB-CHANCE', 'GRANDFATHER', 'GRASS', 'GREAT', 'GREEN', 'GROUND', 'GROUP/TOGETHER', 'GROW', 'GROW-UP', 'GUESS', 'HALF', 'HALL', 'HAMBURGER', 'HANDLE', 'HAPPEN', 'HAPPY', 'HARD', 'HAVE', 'HAVE-TO', 'HEAD', 'HEAD-COLD', 'HEAD-TRIP', 'HEADACHE', 'HEAR', 'HEARING', 'HEART', 'HELMET', 'HER', 'HERE', 'HIDE', 'HIGH', 'HIGHWAY', 'HILL', 'HILLS', 'HIT', 'HITCH-HIKE', 'HOLD', 'HOME', 'HOPE', 'HOSPITAL', 'HOT', 'HOTEL', 'HOUR', 'HOURS', 'HOUSE', 'HOW', 'HOW-MANY', 'HS', 'HUMB-IX', 'HUMID', 'HUMOR', 'HUNGRY', 'HURRY', 'HURT', 'I', 'I-80', 'I-90', 'ICL', 'IDEA', 'IDIOT', 'IF', 'IGNORE', 'ILL', 'IMAGINE', 'IMB', 'IMPACT', 'IMPORTANT', 'IN', 'INCLUDE/ALL-INCLUDED', 'INCREASE', 'IND', 'INFORM', 'INFORMATION', 'INJECT', 'INSPIRE', 'INSURANCE/INFECTION', 'INTERACT/COMMUNICATE', 'INVITE', 'INVOLVE', 'IOWA', 'IS', 'IT', 'ITS', 'IX', 'J', 'JAIL', 'JANA', 'JOHN', 'JOIN', 'JOINT', 'JUMP', 'K', 'KID', 'KILL', 'KIND', 'KING', 'KISS-FIST', 'KNEE', 'KNOW', 'KNOW-THAT', 'L', 'LA', 'LANDSCAPE', 'LANDSCAPING', 'LAPD', 'LAST-WEEK', 'LAST-YEAR', 'LATE', 'LATER', 'LAUGH', 'LAZY', 'LCL', 'LEAD', 'LEADER', 'LEAF', 'LEARN', 'LEAVE-THERE', 'LEG', 'LEGAL/LAW', 'LIFE', 'LIGHTNING', 'LIKE', 'LIMIT', 'LINCOLN', 'LINE', 'LIP', 'LIST', 'LITTLE-BIT', 'LIVE', 'LOCK', 'LODI', 'LOG', 'LOMBARDI', 'LOMBIDI', 'LOMDI', 'LONG-LIST', 'LONG-SLEEVE', 'LONG-TERM', 'LOOK', 'LOOK-AROUND', 'LOOK-AT', 'LOOK-BACK', 'LOOK-FOR', 'LOOK-LIKE', 'LOOK-UP', 'LOSE', 'LOUD', 'LOVE', 'LOW/LOWER', 'LUCK', 'LUCKY', 'LVINCE', 'LY', 'MACHINE', 'MAGAZINE', 'MAIL', 'MAINE', 'MAKE', 'MAKER', 'MAN', 'MANY', 'MARKET', 'MARY', 'MAYBE', 'MCDONALD', 'MEAN', 'MEAT', 'MEDIUM', 'MELT/SOLVE', 'MENU', 'METAL', 'MEXICO/SPAIN', 'MIDDLE', 'MIKE', 'MILES', 'MILLION', 'MIND', 'MINUTE', 'MINUTES', 'MISS', 'MISTAKE', 'MISUNDERSTAND', 'MONEY', 'MORE', 'MORNING', 'MOST', 'MOTHER', 'MOTIVATE', 'MOTORCYCLE', 'MOVE', 'MOVIE', 'MP', 'MPH', 'MR', 'MUCH', 'MUHAMMAD', 'MUST', 'NAKED', 'NAME', 'NAUSEA', 'NEAR', 'NEB', 'NEED', 'NEGATIVE', 'NERVOUS', 'NEV', 'NEVADA', 'NEVER', 'NEW', 'NEW-YORK', 'NEWS', 'NEXT', 'NEXT-TO', 'NEXT-WEEK', 'NICE', 'NIGHT', 'NINETY', 'NINETY-SIX', 'NO', 'NONE', 'NONE/NOTHING', 'NOON', 'NORTH', 'NORTHRIDGE', 'NOT', 'NOT-CARE', 'NOT-KNOW', 'NOT-LIKE', 'NOT-MIND', 'NOT-WANT', 'NOT-YET', 'NOTHING', 'NOTICE', 'NOW', 'NUMB', 'NUMBER', 'NURSE', 'O', 'OAKLAND', 'OBVIOUS', 'OF', 'OF-COURSE', 'OFFENSE', 'OFFICE', 'OFFICERS', 'OH-I-SEE', 'OHIO', 'OIL', 'OK', 'OLD', 'OLDER', 'OMAHA', 'ON', 'ONCE', 'ONCE-IN-A-WHILE', 'ONE', 'ONE-THOUSAND', 'ONLY', 'OPEN', 'OPEN-BOOK', 'OPINION', 'OPPORTUNITY', 'OR', 'OSE', 'OSE-CALL', 'OTHER', 'OUT', 'OUTGOING', 'OUTSIDE', 'OVER-NIGHT', 'OVER/AFTER', 'OWN', 'P', 'PACE', 'PACE/PROGRESS', 'PAGEANT', 'PAGER', 'PANS', 'PANT', 'PAPER', 'PARADE', 'PARKINSONS', 'PART', 'PARTY', 'PASS-DOWN', 'PAST', 'PATIENT', 'PAUL', 'PAY', 'PAY-ATTENTION', 'PCL', 'PEE', 'PEN', 'PEOPLE', 'PERCENT', 'PERFECT', 'PERSON', 'PHONE', 'PICK-UP', 'PICK/SELECT', 'PICTURE', 'PIG', 'PITCH-IN', 'PITY', 'PKSON', 'PLACE', 'PLAN', 'PLANT', 'PLASTIC', 'PLAY', 'PLAY-AGAINST', 'PLAYER', 'PLAYS', 'PLUS', 'POLE', 'POLICY', 'POSITIVE', 'POSS', 'POSSIBLE', 'POTATO', 'POTS', 'POUND', 'POUR-SWEAT', 'PREDICT', 'PREFER', 'PREGNANT', 'PRESSURE', 'PRETEND', 'PRETTY', 'PRICE', 'PROBLEM', 'PROCEED', 'PRODUCT', 'PUBLIC', 'PULL', 'PUMP', 'PUNCH', 'PUNISH', 'PUSH', 'PUT-AWAY', 'QB', 'QM', 'QUARTERBACK', 'QUESTION', 'QUOTE', 'R', 'RA', 'RAFT', 'RAIDERS', 'RAIN', 'READ', 'READY', 'REALIZE', 'REALLY', 'REASON', 'RECENT-PAST', 'RECORD', 'REFUSE', 'REGULAR', 'REMEMBER', 'REMIND', 'REPLY', 'REPORT', 'REPORTER', 'REQUIRE', 'RESIDENCE/ADDRESS', 'RESTAURANT', 'RESULT', 'RIBS', 'RIDE', 'RIGHT', 'RIGHT-HERE', 'RING', 'RIVER', 'ROAD', 'ROCK', 'RODNEY', 'ROLLERBLADE', 'ROOF', 'ROOM', 'ROOMMATE', 'ROPE', 'ROW/PADDLE', 'RUBBER', 'RUN', 'RUNNING-BACK', 'S', 'SACK', 'SAD', 'SAFE', 'SAME', 'SARCASM', 'SATISFIED', 'SAY', 'SB', 'SCARE', 'SCISSOR', 'SCL', 'SCREAM', 'SEAFOOD', 'SEARCH-FOR', 'SEASON', 'SEASONS', 'SECOND', 'SEE', 'SEE-SEE', 'SEEM', 'SELECT', 'SELF', 'SELL', 'SEMESTER', 'SEND', 'SENTENCE', 'SEPARATE', 'SEPT', 'SERIOUS', 'SERVICE', 'SET-ASIDE', 'SET-UP', 'SEVEN', 'SEVEN-THIRTY', 'SEW', 'SEWER', 'SF', 'SHAKE', 'SHAMPOO', 'SHARP', 'SHEEP', 'SHIRT', 'SHOCK', 'SHOE', 'SHOOT', 'SHORT', 'SHOULD', 'SHOW', 'SHOWER', 'SIBERIAN', 'SICK', 'SIDE', 'SIGN', 'SILK', 'SIMPLE', 'SIREN', 'SISTER', 'SIT', 'SITUATION', 'SIX', 'SIX-DAY', 'SIZE', 'SKILL', 'SLEEP', 'SLICE', 'SLOW', 'SMALL', 'SMELL', 'SMOKE', 'SNEAK', 'SNOW', 'SO', 'SOCIAL/INTERACT', 'SOCIALIZE', 'SOME', 'SOMETHING/ONE', 'SOMETIMES', 'SOON', 'SORRY', 'SPARE', 'SPECIAL/EXCEPT', 'SPECIALIZATION', 'SPECIALTY', 'SPEECH/LECTURE', 'SPEED', 'SPELL', 'SPIN', 'SPIT', 'SPORT', 'SPRAIN', 'SPRING', 'SS', 'STAFF', 'STAND', 'STAND-UP', 'START', 'STATE', 'STATION', 'STAY', 'STAY-AWAKE', 'STAY-AWAKE-ALL-NIGHT', 'STEAK', 'STEAL', 'STICK', 'STILL', 'STITCH', 'STOMACH', 'STOOL', 'STOP', 'STORE', 'STORM', 'STORY', 'STRANGE', 'STRANGER', 'STRICT', 'STRING', 'STRIPE', 'STRONG', 'STUCK', 'STUDENT', 'SUE', 'SUGGEST', 'SUMMER', 'SUN', 'SUNNY', 'SUNRISE', 'SUNSET', 'SUPERIOR', 'SUPPOSE', 'SURGEON', 'SWITCH', 'SYSTEM', 'T', 'TABLE', 'TAG', 'TAKE', 'TAKE-BREAK', 'TAKE-OFF', 'TAKE-OVER', 'TALK', 'TANK-TOP', 'TASTE', 'TD', 'TE', 'TEA', 'TEACH', 'TEACHER', 'TEAM', 'TEASE', 'TELL', 'TEMPT', 'TEN', 'TEND', 'TEST', 'THAN', 'THAT', 'THEN', 'THICK', 'THING', 'THINK', 'THIRD', 'THOUSAND', 'THREE', 'THREE-DAY', 'THREE-HOUR', 'THROUGH', 'THROW', 'THUMBS-UP/GOOD', 'TIE', 'TIGER', 'TIME', 'TIRE', 'TO', 'TODAY', 'TOGETHER', 'TOMORROW', 'TONY', 'TOO', 'TOTAL', 'TOUCHDOWN', 'TOUGH', 'TOY', 'TRAY', 'TREE', 'TRIP', 'TROUBLE', 'TRUE-BUSINESS', 'TRUNK', 'TRUST', 'TRY', 'TURN', 'TURN-OFF', 'TWENTY', 'TWICE', 'TWO', 'UB', 'UE', 'UMBRELLA', 'UN', 'UNDER', 'UNDERSTAND', 'UNIVERSITY', 'UNLESS', 'UNTIL', 'UP', 'UP-TO-NOW', 'UPSET', 'US', 'USE', 'USE-SIGN-LANGUAGE', 'UTAH', 'V', 'VARIOUS', 'VARY', 'VEGETABLE', 'VERY', 'VIBRATE', 'VIDEOTAPE', 'VINCE', 'VISIT', 'VOMIT', 'WAIT', 'WAITRESS', 'WAIVE', 'WAKE-UP', 'WALK', 'WALLET', 'WANT', 'WARN', 'WAS', 'WASH', 'WASTE', 'WATCH', 'WATCH-TV', 'WATER', 'WAY', 'WEAR', 'WEATHER', 'WEEKEND', 'WELCOME', 'WET', 'WHA', 'WHAT', 'WHEEL', 'WHEN', 'WHERE', 'WHEW/RELIEVED', 'WHICH', 'WHITE', 'WHO', 'WHOLE', 'WHY', 'WIDE-RECEIVER', 'WIFE', 'WIN', 'WINNER', 'WINNING', 'WISH', 'WITH', 'WOLF', 'WOMAN', 'WONDER', 'WONDERFUL', 'WOOD', 'WORD', 'WORK', 'WORK-OUT', 'WORRY', 'WORSE', 'WORTH', 'WOW', 'WOW/AWFUL', 'WRITE', 'WRONG', 'WYOMING', 'X', 'XRAY', 'Y', 'YEAR', 'YEAR-LONG', 'YES', 'YESTERDAY', 'YOUNG', 'ZONE', '^', 'a', 'arc', 'b', 'bent-1', 'bent-B', 'bent-B-L', 'c', 'crvd-5', 'crvd-B', 'crvd-L', 'crvd-V', 'crvd-sprd-B', 'd', 'e', 'f', 'fanned-flat-O', 'flat-B', 'flat-O', 'fs', 'g', 'h', 'i', 'j', 'k', 'l', 'loc', 'm', 'n', 'ns', 'o', 'p', 'part', 'pl', 'r', 's', 't', 'u', 'v', 'w', 'xx']\n",
      "num_eng_tokens 1692\n",
      "num_asl_tokens 1130\n",
      "['#', '(', ')', '+', '-', '0', '1', '1.50', '100', '110', '150', '1p', '2', '25', '2p', '3', '300', '350', '3p', '4', '45', '5', '50', '50s', '60s', '70', '75', '80', '85', '88', '90', ':', '>', 'A', 'A-WAYS', 'ABOUT', 'ABUSE', 'ACCEPT', 'ACCIDENT', 'ACROSS', 'ADMIT', 'ADVANTAGE', 'ADVENTURE', 'ADVISE', 'ADVISER', 'AFTER', 'AGAIN', 'AGAINST', 'AGE', 'AGE-3', 'AGE-4', 'AGE-5', 'AGE-8', 'AGREE', 'AIR', 'ALI', 'ALL', 'ALL-DAY', 'ALL-GONE', 'ALL-NIGHT', 'ALL-THE-WAY', 'ALMOST', 'ALONE', 'ALRIGHT', 'AND', 'ANGRY', 'ANIMAL', 'ANKLE', 'ANN', 'ANY', 'ANYWAY/NOT-MATTER', 'APPEAR', 'APPLAUSE', 'APPLE', 'APPROACH', 'AREA', 'ARREST', 'ARRIVE', 'ASK', 'ASS', 'ASSISTANT', 'AT', 'AVERAGE', 'AWFUL', 'B', 'BABY', 'BACK', 'BAD', 'BAG', 'BAGS', 'BALL', 'BANDAGE', 'BANKS', 'BASKETBALL', 'BATHROOM', 'BCL', 'BE', 'BEAUTIFUL', 'BECAUSE', 'BECOME', 'BED', 'BEER', 'BEFORE', 'BELIEVE', 'BELONG', 'BELT', 'BEST', 'BETTER', 'BETWEEN', 'BEVERLY', 'BIB', 'BIG', 'BIGGER', 'BILL', 'BLACK', 'BLADDER', 'BLAME', 'BLOCK', 'BLOOD', 'BLOW', 'BLOWN-AWAY', 'BLUE', 'BOAT', 'BONE', 'BOOK', 'BOOT', 'BORDER', 'BORE', 'BORN', 'BORROW', 'BOSS', 'BOSTON', 'BOTH', 'BOX', 'BOXER', 'BOXING', 'BOY', 'BPCL', 'BRAKE', 'BREAD', 'BREAK', 'BREAK-DOWN', 'BREATHE', 'BRICK', 'BRING', 'BROCCOLI', 'BROTHER', 'BROWN', 'BUILD', 'BUILDING', 'BUS', 'BUSH', 'BUSINESS', 'BUT', 'BUTCHER-SHOP', 'BUTTS', 'BUY', 'BY', 'C', 'C-L', 'CABIN', 'CALIFORNIA', 'CALL', 'CALL-BY-PHONE', 'CAMP', 'CAN', 'CANDY', 'CANNOT', 'CAPE-COD', 'CAPECOD', 'CAR', 'CARE', 'CASINO', 'CAUSE', 'CELEBRATE', 'CHAIR', 'CHALLENGE', 'CHANGE', 'CHANNEL', 'CHARACTER', 'CHASE', 'CHAT', 'CHECK', 'CHEER', 'CHEMICAL', 'CHESS', 'CHICAGO', 'CHICKEN', 'CHINA', 'CHOCOLATE', 'CHOICE', 'CHOP', 'CIGARETTE', 'CITY/COMMUNITY', 'CL', 'CNN', 'COACH', 'COAT', 'COFFEE', 'COFFIN', 'COLD', 'COLLECT', 'COLLEGE', 'COLOR', 'COLORADO', 'COME', 'COME-ON', 'COMFORTABLE', 'COMMUNICATE', 'COMPANY', 'COMPARE', 'CONCEPT', 'CONFIDENT', 'CONFUSE', 'CONTACT', 'CONTINUE', 'COOK', 'COOL', 'COOLER', 'COORDINATOR', 'COP', 'COPY', 'CORN', 'CORNER', 'COUGH', 'COUNT-ON-FINGERS', 'COUNTRY', 'COW', 'CPR', 'CRACK', 'CRASH', 'CRAZY', 'CREATE/PRETEND', 'CREEPY', 'CRIB', 'CROPS', 'CRY', 'CSUN', 'CULTURE', 'CUT', 'CUTE', 'CUTTER', 'DAISY', 'DANA', 'DANCE', 'DARK', 'DATE', 'DAY', 'DCL', 'DEAD', 'DEAF', 'DEC', 'DECIDE', 'DECORATE', 'DEER', 'DEGREE', 'DELICIOUS', 'DEPART', 'DEPRESS', 'DESTROY', 'DEVIL', 'DIE', 'DIFFERENT', 'DISAPPOINT', 'DISCIPLINE', 'DISCONNECT', 'DISEASE', 'DO', 'DOCTOR', 'DOLLAR', 'DONUT', 'DOOR', 'DORM', 'DOWN', 'DOWN-THE-LIST', 'DREAM', 'DRINK', 'DRIVE', 'DROP', 'DRUG', 'DRUGS', 'DRY', 'DURING/WHILE', 'E', 'EACH', 'EAN', 'EAR', 'EASIER', 'EAT', 'EAT-UP', 'ECONOMIC', 'EIGHTY', 'ELEVEN', 'ELITE', 'EMAIL', 'EMERGENCY', 'EMOTION', 'EMPTY', 'END', 'ENGINE', 'ENGLISH', 'ENJOY', 'ENOUGH', 'ENTER', 'ENVIRONMENT', 'EQUAL', 'ERICK', 'ESCAPE', 'ETC.', 'EUROPE', 'EVEN', 'EVER', 'EVERY', 'EVERYDAY', 'EX', 'EXAM', 'EXAMPLE', 'EXCITE', 'EXCITED', 'EXCUSE', 'EXCUSE-GO', 'EXHAUST', 'EXPECT', 'EXPENSIVE', 'EXPERIENCE', 'EXPLAIN', 'EXPRESS', 'EXTEND', 'F', 'FACE', 'FALL', 'FALL-ASLEEP', 'FALL-INTO-IT', 'FALL-INTO-PLACE', 'FAMILY', 'FAN', 'FANCY', 'FANS', 'FAR', 'FARM', 'FASCINATED', 'FAST', 'FATHER', 'FAVORITE/PREFER', 'FEED', 'FEEL', 'FEET', 'FENCE', 'FEW', 'FF', 'FIELD', 'FIFTEEN', 'FILM', 'FINALLY/SUCCEED', 'FIND', 'FIND/FIND-OUT', 'FINE', 'FINISH', 'FIRE', 'FIRST', 'FIVE', 'FIVE-MINUTE', 'FIX', 'FLAT', 'FLOOD', 'FLOOR', 'FLOWER', 'FOCUS', 'FOLLOW', 'FOOD', 'FOOT', 'FOOTBALL', 'FOR', 'FOREVER', 'FORGET', 'FORGET-IT', 'FORMERLY', 'FORTUNATELY', 'FOUR', 'FOUR-DAY', 'FOUR-THIRTY', 'FRANK', 'FRATERNITY', 'FREAK', 'FRESH', 'FRIDAY', 'FRIEND', 'FRIENDLY', 'FROM', 'FROM-NOW-ON', 'FRONT', 'FRUGAL', 'FRUIT', 'FS-ZONE', 'FULL', 'FUMBLE', 'FUN', 'FUNNY', 'FUTURE', 'G', 'GAMBLE', 'GAME', 'GAS', 'GENERAL', 'GESTURE', 'GET', 'GET-IN-BED', 'GET-UP', 'GIFT', 'GIRL', 'GIVE', 'GO', 'GO-AHEAD', 'GO-AWAY', 'GO-OUT', 'GOD', 'GONE', 'GOOD', 'GRAB-CHANCE', 'GRANDFATHER', 'GRASS', 'GREAT', 'GREEN', 'GROUND', 'GROUP/TOGETHER', 'GROW', 'GROW-UP', 'GUESS', 'HALF', 'HALL', 'HAMBURGER', 'HANDLE', 'HAPPEN', 'HAPPY', 'HARD', 'HAVE', 'HAVE-TO', 'HEAD', 'HEAD-COLD', 'HEAD-TRIP', 'HEADACHE', 'HEAR', 'HEARING', 'HEART', 'HELMET', 'HER', 'HERE', 'HIDE', 'HIGH', 'HIGHWAY', 'HILL', 'HILLS', 'HIT', 'HITCH-HIKE', 'HOLD', 'HOME', 'HOPE', 'HOSPITAL', 'HOT', 'HOTEL', 'HOUR', 'HOURS', 'HOUSE', 'HOW', 'HOW-MANY', 'HS', 'HUMB-IX', 'HUMID', 'HUMOR', 'HUNGRY', 'HURRY', 'HURT', 'I', 'I-80', 'I-90', 'ICL', 'IDEA', 'IDIOT', 'IF', 'IGNORE', 'ILL', 'IMAGINE', 'IMB', 'IMPACT', 'IMPORTANT', 'IN', 'INCLUDE/ALL-INCLUDED', 'INCREASE', 'IND', 'INFORM', 'INFORMATION', 'INJECT', 'INSPIRE', 'INSURANCE/INFECTION', 'INTERACT/COMMUNICATE', 'INVITE', 'INVOLVE', 'IOWA', 'IS', 'IT', 'ITS', 'IX', 'J', 'JAIL', 'JANA', 'JOHN', 'JOIN', 'JOINT', 'JUMP', 'K', 'KID', 'KILL', 'KIND', 'KING', 'KISS-FIST', 'KNEE', 'KNOW', 'KNOW-THAT', 'L', 'LA', 'LANDSCAPE', 'LANDSCAPING', 'LAPD', 'LAST-WEEK', 'LAST-YEAR', 'LATE', 'LATER', 'LAUGH', 'LAZY', 'LCL', 'LEAD', 'LEADER', 'LEAF', 'LEARN', 'LEAVE-THERE', 'LEG', 'LEGAL/LAW', 'LIFE', 'LIGHTNING', 'LIKE', 'LIMIT', 'LINCOLN', 'LINE', 'LIP', 'LIST', 'LITTLE-BIT', 'LIVE', 'LOCK', 'LODI', 'LOG', 'LOMBARDI', 'LOMBIDI', 'LOMDI', 'LONG-LIST', 'LONG-SLEEVE', 'LONG-TERM', 'LOOK', 'LOOK-AROUND', 'LOOK-AT', 'LOOK-BACK', 'LOOK-FOR', 'LOOK-LIKE', 'LOOK-UP', 'LOSE', 'LOUD', 'LOVE', 'LOW/LOWER', 'LUCK', 'LUCKY', 'LVINCE', 'LY', 'MACHINE', 'MAGAZINE', 'MAIL', 'MAINE', 'MAKE', 'MAKER', 'MAN', 'MANY', 'MARKET', 'MARY', 'MAYBE', 'MCDONALD', 'MEAN', 'MEAT', 'MEDIUM', 'MELT/SOLVE', 'MENU', 'METAL', 'MEXICO/SPAIN', 'MIDDLE', 'MIKE', 'MILES', 'MILLION', 'MIND', 'MINUTE', 'MINUTES', 'MISS', 'MISTAKE', 'MISUNDERSTAND', 'MONEY', 'MORE', 'MORNING', 'MOST', 'MOTHER', 'MOTIVATE', 'MOTORCYCLE', 'MOVE', 'MOVIE', 'MP', 'MPH', 'MR', 'MUCH', 'MUHAMMAD', 'MUST', 'NAKED', 'NAME', 'NAUSEA', 'NEAR', 'NEB', 'NEED', 'NEGATIVE', 'NERVOUS', 'NEV', 'NEVADA', 'NEVER', 'NEW', 'NEW-YORK', 'NEWS', 'NEXT', 'NEXT-TO', 'NEXT-WEEK', 'NICE', 'NIGHT', 'NINETY', 'NINETY-SIX', 'NO', 'NONE', 'NONE/NOTHING', 'NOON', 'NORTH', 'NORTHRIDGE', 'NOT', 'NOT-CARE', 'NOT-KNOW', 'NOT-LIKE', 'NOT-MIND', 'NOT-WANT', 'NOT-YET', 'NOTHING', 'NOTICE', 'NOW', 'NUMB', 'NUMBER', 'NURSE', 'O', 'OAKLAND', 'OBVIOUS', 'OF', 'OF-COURSE', 'OFFENSE', 'OFFICE', 'OFFICERS', 'OH-I-SEE', 'OHIO', 'OIL', 'OK', 'OLD', 'OLDER', 'OMAHA', 'ON', 'ONCE', 'ONCE-IN-A-WHILE', 'ONE', 'ONE-THOUSAND', 'ONLY', 'OPEN', 'OPEN-BOOK', 'OPINION', 'OPPORTUNITY', 'OR', 'OSE', 'OSE-CALL', 'OTHER', 'OUT', 'OUTGOING', 'OUTSIDE', 'OVER-NIGHT', 'OVER/AFTER', 'OWN', 'P', 'PACE', 'PACE/PROGRESS', 'PAGEANT', 'PAGER', 'PANS', 'PANT', 'PAPER', 'PARADE', 'PARKINSONS', 'PART', 'PARTY', 'PASS-DOWN', 'PAST', 'PATIENT', 'PAUL', 'PAY', 'PAY-ATTENTION', 'PCL', 'PEE', 'PEN', 'PEOPLE', 'PERCENT', 'PERFECT', 'PERSON', 'PHONE', 'PICK-UP', 'PICK/SELECT', 'PICTURE', 'PIG', 'PITCH-IN', 'PITY', 'PKSON', 'PLACE', 'PLAN', 'PLANT', 'PLASTIC', 'PLAY', 'PLAY-AGAINST', 'PLAYER', 'PLAYS', 'PLUS', 'POLE', 'POLICY', 'POSITIVE', 'POSS', 'POSSIBLE', 'POTATO', 'POTS', 'POUND', 'POUR-SWEAT', 'PREDICT', 'PREFER', 'PREGNANT', 'PRESSURE', 'PRETEND', 'PRETTY', 'PRICE', 'PROBLEM', 'PROCEED', 'PRODUCT', 'PUBLIC', 'PULL', 'PUMP', 'PUNCH', 'PUNISH', 'PUSH', 'PUT-AWAY', 'QB', 'QM', 'QUARTERBACK', 'QUESTION', 'QUOTE', 'R', 'RA', 'RAFT', 'RAIDERS', 'RAIN', 'READ', 'READY', 'REALIZE', 'REALLY', 'REASON', 'RECENT-PAST', 'RECORD', 'REFUSE', 'REGULAR', 'REMEMBER', 'REMIND', 'REPLY', 'REPORT', 'REPORTER', 'REQUIRE', 'RESIDENCE/ADDRESS', 'RESTAURANT', 'RESULT', 'RIBS', 'RIDE', 'RIGHT', 'RIGHT-HERE', 'RING', 'RIVER', 'ROAD', 'ROCK', 'RODNEY', 'ROLLERBLADE', 'ROOF', 'ROOM', 'ROOMMATE', 'ROPE', 'ROW/PADDLE', 'RUBBER', 'RUN', 'RUNNING-BACK', 'S', 'SACK', 'SAD', 'SAFE', 'SAME', 'SARCASM', 'SATISFIED', 'SAY', 'SB', 'SCARE', 'SCISSOR', 'SCL', 'SCREAM', 'SEAFOOD', 'SEARCH-FOR', 'SEASON', 'SEASONS', 'SECOND', 'SEE', 'SEE-SEE', 'SEEM', 'SELECT', 'SELF', 'SELL', 'SEMESTER', 'SEND', 'SENTENCE', 'SEPARATE', 'SEPT', 'SERIOUS', 'SERVICE', 'SET-ASIDE', 'SET-UP', 'SEVEN', 'SEVEN-THIRTY', 'SEW', 'SEWER', 'SF', 'SHAKE', 'SHAMPOO', 'SHARP', 'SHEEP', 'SHIRT', 'SHOCK', 'SHOE', 'SHOOT', 'SHORT', 'SHOULD', 'SHOW', 'SHOWER', 'SIBERIAN', 'SICK', 'SIDE', 'SIGN', 'SILK', 'SIMPLE', 'SIREN', 'SISTER', 'SIT', 'SITUATION', 'SIX', 'SIX-DAY', 'SIZE', 'SKILL', 'SLEEP', 'SLICE', 'SLOW', 'SMALL', 'SMELL', 'SMOKE', 'SNEAK', 'SNOW', 'SO', 'SOCIAL/INTERACT', 'SOCIALIZE', 'SOME', 'SOMETHING/ONE', 'SOMETIMES', 'SOON', 'SORRY', 'SPARE', 'SPECIAL/EXCEPT', 'SPECIALIZATION', 'SPECIALTY', 'SPEECH/LECTURE', 'SPEED', 'SPELL', 'SPIN', 'SPIT', 'SPORT', 'SPRAIN', 'SPRING', 'SS', 'STAFF', 'STAND', 'STAND-UP', 'START', 'STATE', 'STATION', 'STAY', 'STAY-AWAKE', 'STAY-AWAKE-ALL-NIGHT', 'STEAK', 'STEAL', 'STICK', 'STILL', 'STITCH', 'STOMACH', 'STOOL', 'STOP', 'STORE', 'STORM', 'STORY', 'STRANGE', 'STRANGER', 'STRICT', 'STRING', 'STRIPE', 'STRONG', 'STUCK', 'STUDENT', 'SUE', 'SUGGEST', 'SUMMER', 'SUN', 'SUNNY', 'SUNRISE', 'SUNSET', 'SUPERIOR', 'SUPPOSE', 'SURGEON', 'SWITCH', 'SYSTEM', 'T', 'TABLE', 'TAG', 'TAKE', 'TAKE-BREAK', 'TAKE-OFF', 'TAKE-OVER', 'TALK', 'TANK-TOP', 'TASTE', 'TD', 'TE', 'TEA', 'TEACH', 'TEACHER', 'TEAM', 'TEASE', 'TELL', 'TEMPT', 'TEN', 'TEND', 'TEST', 'THAN', 'THAT', 'THEN', 'THICK', 'THING', 'THINK', 'THIRD', 'THOUSAND', 'THREE', 'THREE-DAY', 'THREE-HOUR', 'THROUGH', 'THROW', 'THUMBS-UP/GOOD', 'TIE', 'TIGER', 'TIME', 'TIRE', 'TO', 'TODAY', 'TOGETHER', 'TOMORROW', 'TONY', 'TOO', 'TOTAL', 'TOUCHDOWN', 'TOUGH', 'TOY', 'TRAY', 'TREE', 'TRIP', 'TROUBLE', 'TRUE-BUSINESS', 'TRUNK', 'TRUST', 'TRY', 'TURN', 'TURN-OFF', 'TWENTY', 'TWICE', 'TWO', 'UB', 'UE', 'UMBRELLA', 'UN', 'UNDER', 'UNDERSTAND', 'UNIVERSITY', 'UNLESS', 'UNTIL', 'UP', 'UP-TO-NOW', 'UPSET', 'US', 'USE', 'USE-SIGN-LANGUAGE', 'UTAH', 'V', 'VARIOUS', 'VARY', 'VEGETABLE', 'VERY', 'VIBRATE', 'VIDEOTAPE', 'VINCE', 'VISIT', 'VOMIT', 'WAIT', 'WAITRESS', 'WAIVE', 'WAKE-UP', 'WALK', 'WALLET', 'WANT', 'WARN', 'WAS', 'WASH', 'WASTE', 'WATCH', 'WATCH-TV', 'WATER', 'WAY', 'WEAR', 'WEATHER', 'WEEKEND', 'WELCOME', 'WET', 'WHA', 'WHAT', 'WHEEL', 'WHEN', 'WHERE', 'WHEW/RELIEVED', 'WHICH', 'WHITE', 'WHO', 'WHOLE', 'WHY', 'WIDE-RECEIVER', 'WIFE', 'WIN', 'WINNER', 'WINNING', 'WISH', 'WITH', 'WOLF', 'WOMAN', 'WONDER', 'WONDERFUL', 'WOOD', 'WORD', 'WORK', 'WORK-OUT', 'WORRY', 'WORSE', 'WORTH', 'WOW', 'WOW/AWFUL', 'WRITE', 'WRONG', 'WYOMING', 'X', 'XRAY', 'Y', 'YEAR', 'YEAR-LONG', 'YES', 'YESTERDAY', 'YOUNG', 'ZONE', '^', 'a', 'arc', 'b', 'bent-1', 'bent-B', 'bent-B-L', 'c', 'crvd-5', 'crvd-B', 'crvd-L', 'crvd-V', 'crvd-sprd-B', 'd', 'e', 'f', 'fanned-flat-O', 'flat-B', 'flat-O', 'fs', 'g', 'h', 'i', 'j', 'k', 'l', 'loc', 'm', 'n', 'ns', 'o', 'p', 'part', 'pl', 'r', 's', 't', 'u', 'v', 'w', 'xx']\n"
     ]
    }
   ],
   "source": [
    "# generate\n",
    "    # 1) list of eng-asl sentence pairs\n",
    "    # 2) set of unique english vocab\n",
    "    # 3) set of unique asl vocab\n",
    "\n",
    "text_pairs = []\n",
    "eng_tokens = set()\n",
    "asl_tokens = set()\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    " \n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    pair = []\n",
    "    eng_text, asl_text = line.split(\"\\t\")\n",
    "    pair.append(eng_text)\n",
    "    pair.append(asl_text)\n",
    "    text_pairs.append(pair)\n",
    "\n",
    "for pair in text_pairs:\n",
    "    sent_tokens = custom_eng_tokenize(pair[0])\n",
    "    for token in sent_tokens:\n",
    "        if token not in eng_tokens:\n",
    "            eng_tokens.add(token)\n",
    "            \n",
    "for pair in text_pairs:\n",
    "    sent_tokens = custom_asl_tokenize(pair[1])\n",
    "    for token in sent_tokens:\n",
    "        if token not in asl_tokens:\n",
    "            asl_tokens.add(token)\n",
    "\n",
    "eng_tokens = sorted(list(eng_tokens))\n",
    "asl_tokens = sorted(list(asl_tokens))\n",
    "\n",
    "print(\"eng_tokens:\", eng_tokens)\n",
    "print(\"output_tokens\", asl_tokens)\n",
    "num_encoder_tokens = len(eng_tokens)\n",
    "num_decoder_tokens = len(asl_tokens)\n",
    "print(\"num_eng_tokens\", num_encoder_tokens)\n",
    "print(\"num_asl_tokens\", num_decoder_tokens)\n",
    "print(asl_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b587712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John will have seen Mary.', 'fs-JOHN FUTURE FINISH SEE fs-MARY']\n",
      "['Especially when football is the only legal sport where you can hit someone that hard and not get fined for it.', 'SPECIAL/EXCEPT+fs-LY WHEN IX-3p FOOTBALL REALLY ONLY LEGAL/LAW SPORT IX-2p CAN BPCL:bent-Bxx SOMETHING/ONE #SO HARD AND NOT GET PRICE FOR fs-IT']\n",
      "['There was waving while we were rowing, and people fell off the raft.', 'ICLxx SCL:B-Lxx SOME SCL:bent-Vxx FINISH']\n",
      "[\"The teacher likes books, but doesn't like movies.\", 'TEACHER LIKE BOOK MOVIE NOT']\n",
      "['Sue is buying that blue car.', 'fs-SUE BUY IX-3p CAR BLUE']\n"
     ]
    }
   ],
   "source": [
    "# glimpse pairs\n",
    "\n",
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c010cbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400 total pairs\n",
      "980 training pairs\n",
      "210 validation pairs\n",
      "210 test pairs\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5fba5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_hub.tokenizers.compute_word_piece_vocabulary(\n",
    "        word_piece_ds.batch(500).prefetch(2),\n",
    "        vocabulary_size=vocab_size,\n",
    "        reserved_tokens=reserved_tokens,\n",
    "    )\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5335f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 15:06:20.580747: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-07-15 15:06:21.484469: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "asl_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "asl_vocab = train_word_piece(asl_samples, ASL_VOCAB_SIZE, reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0e3764c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Tokens:  ['house', 'buy', 'teacher', 'for', 'on', 'said', '##es', 'will', 'you', 'have']\n",
      "ASL Tokens:  ['##T', 'HOUSE', '##N', 'BCLxx', '##Y', 'SAME', 'arc', 'GIFT', '##ER', 'ICLxx']\n"
     ]
    }
   ],
   "source": [
    "print(\"English Tokens: \", eng_vocab[100:110])\n",
    "print(\"ASL Tokens: \", asl_vocab[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9895e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=eng_vocab, lowercase=False\n",
    ")\n",
    "asl_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=asl_vocab, lowercase=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb29eb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:  The sun sets.  Lightning strikes.  Rain falls.\n",
      "Tokens:  tf.Tensor(\n",
      "[ 82  68 440  99  68 335  79  11  36 196 338  68  94 164 308 363  11  41\n",
      " 153 230  55 178  79  11], shape=(24,), dtype=int32)\n",
      "Recovered text after detokenizing:  The sun sets . Lightning strikes . Rain falls .\n",
      "\n",
      "ASL input: 'SUNSET LIGHTNING RAIN'\n",
      "ASL Gloss sentence:  SUNSET LIGHTNING RAIN\n",
      "Tokens:  tf.Tensor([ 41 362 102 324  34 359 226 172 100 268 210], shape=(11,), dtype=int32)\n",
      "Recovered text after detokenizing:  SUNSET LIGHTNING RAIN\n"
     ]
    }
   ],
   "source": [
    "eng_input_ex = text_pairs[0][0]\n",
    "eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\n",
    "print(\"English sentence: \", eng_input_ex)\n",
    "print(\"Tokens: \", eng_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    eng_tokenizer.detokenize(eng_tokens_ex),\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "asl_input_ex = text_pairs[0][1]\n",
    "print(f\"ASL input: '{asl_input_ex}'\") \n",
    "asl_tokens_ex = asl_tokenizer.tokenize(asl_input_ex)\n",
    "print(\"ASL Gloss sentence: \", asl_input_ex)\n",
    "print(\"Tokens: \", asl_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    asl_tokenizer.detokenize(asl_tokens_ex),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3c4a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(eng, asl):\n",
    "    batch_size = ops.shape(asl)[0]\n",
    "\n",
    "    eng = eng_tokenizer(eng)\n",
    "    asl = asl_tokenizer(asl)\n",
    "\n",
    "    # Pad `eng` to `MAX_SEQUENCE_LENGTH`.\n",
    "    eng_start_end_packer = keras_hub.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    eng = eng_start_end_packer(eng)\n",
    "\n",
    "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `asl` and pad it as well.\n",
    "    asl_start_end_packer = keras_hub.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
    "        start_value=asl_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value=asl_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value=asl_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    asl = asl_start_end_packer(asl)\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": eng,\n",
    "            \"decoder_inputs\": asl[:, :-1],\n",
    "        },\n",
    "        asl[:, 1:],\n",
    "    )\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, asl_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    asl_texts = list(asl_texts)\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, asl_texts))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2a04a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (64, 128)\n",
      "inputs[\"decoder_inputs\"].shape: (64, 128)\n",
      "targets.shape: (64, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 15:06:23.031659: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebed136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=ENG_VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_hub.layers.TransformerEncoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(inputs=x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=ASL_VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(decoder_inputs)\n",
    "\n",
    "x = keras_hub.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(ASL_VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoded_seq_inputs,\n",
    "    ],\n",
    "    decoder_outputs,\n",
    ")\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"transformer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c923e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)        </span><span style=\"font-weight: bold\"> Output Shape      </span><span style=\"font-weight: bold\">    Param # </span><span style=\"font-weight: bold\"> Connected to      </span>\n",
       "\n",
       " encoder_inputs       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " token_and_position  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     <span style=\"color: #00af00; text-decoration-color: #00af00\">233,472</span>  encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionE</span>                                                   \n",
       "\n",
       " decoder_inputs       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>  -                 \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)                                                          \n",
       "\n",
       " transformer_encoder  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     <span style=\"color: #00af00; text-decoration-color: #00af00\">329,856</span>  token_and_positi \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode</span>                                                   \n",
       "\n",
       " functional_1         (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,          <span style=\"color: #00af00; text-decoration-color: #00af00\">703,982</span>  decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       " (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)         <span style=\"color: #00af00; text-decoration-color: #00af00\">1134</span>)                          transformer_enco \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " encoder_inputs       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " token_and_position  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)     \u001b[38;5;34m233,472\u001b[0m  encoder_inputs[\u001b[38;5;34m0\u001b[0m \n",
       " (\u001b[38;5;33mTokenAndPositionE\u001b[0m                                                   \n",
       "\n",
       " decoder_inputs       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                \u001b[38;5;34m0\u001b[0m  -                 \n",
       " (\u001b[38;5;33mInputLayer\u001b[0m)                                                          \n",
       "\n",
       " transformer_encoder  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)     \u001b[38;5;34m329,856\u001b[0m  token_and_positi \n",
       " (\u001b[38;5;33mTransformerEncode\u001b[0m                                                   \n",
       "\n",
       " functional_1         (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,          \u001b[38;5;34m703,982\u001b[0m  decoder_inputs[\u001b[38;5;34m0\u001b[0m \n",
       " (\u001b[38;5;33mFunctional\u001b[0m)         \u001b[38;5;34m1134\u001b[0m)                          transformer_enco \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,267,310</span> (4.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,267,310\u001b[0m (4.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,267,310</span> (4.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,267,310\u001b[0m (4.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 411ms/step - accuracy: 0.6887 - loss: 3.1069 - val_accuracy: 0.8832 - val_loss: 0.8971\n",
      "Epoch 2/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 379ms/step - accuracy: 0.8802 - loss: 0.9302 - val_accuracy: 0.8919 - val_loss: 0.6672\n",
      "Epoch 3/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 382ms/step - accuracy: 0.8878 - loss: 0.7162 - val_accuracy: 0.8978 - val_loss: 0.5901\n",
      "Epoch 4/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 379ms/step - accuracy: 0.8919 - loss: 0.6283 - val_accuracy: 0.9004 - val_loss: 0.5474\n",
      "Epoch 5/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 379ms/step - accuracy: 0.8946 - loss: 0.5783 - val_accuracy: 0.9010 - val_loss: 0.5213\n",
      "Epoch 6/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 384ms/step - accuracy: 0.8961 - loss: 0.5486 - val_accuracy: 0.9031 - val_loss: 0.5032\n",
      "Epoch 7/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 410ms/step - accuracy: 0.8973 - loss: 0.5260 - val_accuracy: 0.9036 - val_loss: 0.4897\n",
      "Epoch 8/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 434ms/step - accuracy: 0.8975 - loss: 0.5083 - val_accuracy: 0.9034 - val_loss: 0.4790\n",
      "Epoch 9/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 403ms/step - accuracy: 0.8984 - loss: 0.4951 - val_accuracy: 0.9044 - val_loss: 0.4711\n",
      "Epoch 10/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 395ms/step - accuracy: 0.8996 - loss: 0.4856 - val_accuracy: 0.9048 - val_loss: 0.4637\n",
      "Epoch 11/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 395ms/step - accuracy: 0.8996 - loss: 0.4736 - val_accuracy: 0.9055 - val_loss: 0.4575\n",
      "Epoch 12/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 389ms/step - accuracy: 0.9004 - loss: 0.4646 - val_accuracy: 0.9055 - val_loss: 0.4526\n",
      "Epoch 13/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 400ms/step - accuracy: 0.9008 - loss: 0.4591 - val_accuracy: 0.9060 - val_loss: 0.4478\n",
      "Epoch 14/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 385ms/step - accuracy: 0.9016 - loss: 0.4510 - val_accuracy: 0.9063 - val_loss: 0.4431\n",
      "Epoch 15/30\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 396ms/step - accuracy: 0.9018 - loss: 0.4454 - val_accuracy: 0.9068 - val_loss: 0.4395\n",
      "Epoch 16/30\n",
      "\u001b[1m 4/16\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 373ms/step - accuracy: 0.9013 - loss: 0.4398"
     ]
    }
   ],
   "source": [
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad617331",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752606133.576587 9373366 service.cc:152] XLA service 0x145474ae0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1752606133.576616 9373366 service.cc:160]   StreamExecutor device (0): Host, Default Version\n",
      "I0000 00:00:1752606133.856157 9373366 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Example 0 **\n",
      "Mother did not buy the car for John.\n",
      "fs - JOHN READ BOOK\n",
      "\n",
      "** Example 1 **\n",
      "All of sudden, a strong storm hit.  I was a bit nervous driving through it. \n",
      "IX - 1p LOOK STRE IX - 1p 5xx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def decode_sequences(input_sentences):\n",
    "    batch_size = 1\n",
    "\n",
    "    # Tokenize the encoder input.\n",
    "    encoder_input_tokens = ops.convert_to_tensor(eng_tokenizer(input_sentences))\n",
    "    if len(encoder_input_tokens[0]) < MAX_SEQUENCE_LENGTH:\n",
    "        pads = ops.full((1, MAX_SEQUENCE_LENGTH - len(encoder_input_tokens[0])), 0)\n",
    "        encoder_input_tokens = ops.concatenate(\n",
    "            [encoder_input_tokens, pads], 1\n",
    "        )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def next(prompt, cache, index):\n",
    "        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "        # Ignore hidden states for now; only needed for contrastive search.\n",
    "        hidden_states = None\n",
    "        return logits, hidden_states, cache\n",
    "\n",
    "    # Build a prompt of length 40 with a start token and padding tokens.\n",
    "    length = 40\n",
    "    start = ops.full((batch_size, 1), asl_tokenizer.token_to_id(\"[START]\"))\n",
    "    pad = ops.full((batch_size, length - 1), asl_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    prompt = ops.concatenate((start, pad), axis=-1)\n",
    "\n",
    "    generated_tokens = keras_hub.samplers.GreedySampler()(\n",
    "        next,\n",
    "        prompt,\n",
    "        stop_token_ids=[asl_tokenizer.token_to_id(\"[END]\")],\n",
    "        index=1,  # Start sampling after start token.\n",
    "    )\n",
    "    generated_sentences = asl_tokenizer.detokenize(generated_tokens)\n",
    "    return generated_sentences\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(2):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequences([input_sentence])\n",
    "    translated = decode_sequences([input_sentence])[0]\n",
    "    translated = (\n",
    "        translated.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    print(f\"** Example {i} **\")\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ff7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 15:02:18.655586: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Expected size[0] in [0, 50], but got 69\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling PositionEmbedding.call().\n\n\u001b[1m{{function_node __wrapped__Slice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected size[0] in [0, 50], but got 69 [Op:Slice]\u001b[0m\n\nArguments received by PositionEmbedding.call():\n   inputs=tf.Tensor(shape=(1, 69, 128), dtype=float32)\n   start_index=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m input_sentence \u001b[38;5;241m=\u001b[39m test_pair[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m reference_sentence \u001b[38;5;241m=\u001b[39m test_pair[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m translated_sentence \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_sentence\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m translated_sentence \u001b[38;5;241m=\u001b[39m decode_sequences([input_sentence])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m translated_sentence \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     11\u001b[0m     translated_sentence\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[START]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[END]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     15\u001b[0m )\n",
      "Cell \u001b[0;32mIn[24], line 26\u001b[0m, in \u001b[0;36mdecode_sequences\u001b[0;34m(input_sentences)\u001b[0m\n\u001b[1;32m     23\u001b[0m pad \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mfull((batch_size, length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), asl_tokenizer\u001b[38;5;241m.\u001b[39mtoken_to_id(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     24\u001b[0m prompt \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconcatenate((start, pad), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_hub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamplers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGreedySampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_token_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43masl_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_to_id\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[END]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Start sampling after start token.\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m generated_sentences \u001b[38;5;241m=\u001b[39m asl_tokenizer\u001b[38;5;241m.\u001b[39mdetokenize(generated_tokens)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m generated_sentences\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/keras_hub/src/samplers/sampler.py:120\u001b[0m, in \u001b[0;36mSampler.__call__\u001b[0;34m(self, next, prompt, cache, index, mask, stop_token_ids, hidden_states, model)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# Return the next prompt, cache and incremented index.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (prompt, cache, index \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 120\u001b[0m prompt, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/keras_hub/src/samplers/sampler.py:201\u001b[0m, in \u001b[0;36mSampler.run_loop\u001b[0;34m(self, cond, body, model, loop_vars, maximum_iterations)\u001b[0m\n\u001b[1;32m    199\u001b[0m         ref_v\u001b[38;5;241m.\u001b[39massign(v)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     loop_vars \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loop_vars\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/keras/src/ops/core.py:575\u001b[0m, in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, maximum_iterations)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.ops.while_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwhile_loop\u001b[39m(\n\u001b[1;32m    535\u001b[0m     cond,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m     maximum_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    539\u001b[0m ):\n\u001b[1;32m    540\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"While loop implementation.\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m    10, 11\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/core.py:622\u001b[0m, in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, maximum_iterations)\u001b[0m\n\u001b[1;32m    619\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m body(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(outputs) \u001b[38;5;28;01mif\u001b[39;00m is_tuple \u001b[38;5;28;01melse\u001b[39;00m (outputs,)\n\u001b[0;32m--> 622\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs \u001b[38;5;28;01mif\u001b[39;00m is_tuple \u001b[38;5;28;01melse\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:660\u001b[0m, in \u001b[0;36mdeprecated_arg_values.<locals>.deprecated_wrapper.<locals>.new_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    652\u001b[0m           _PRINTED_WARNING[(func, arg_name)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    653\u001b[0m         _log_deprecation(\n\u001b[1;32m    654\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFrom \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m (from \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is deprecated and \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    655\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwill be removed \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mInstructions for updating:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124min a future version\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m date \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m    659\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m date), instructions)\n\u001b[0;32m--> 660\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/tensorflow/python/ops/while_loop.py:241\u001b[0m, in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile_loop\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m     36\u001b[0m \u001b[38;5;129m@deprecation\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated_arg_values(\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m                   maximum_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m                   name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     54\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Repeat `body` while the condition `cond` is true.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m  Note: This op is automatically used in a `tf.function` to convert Python for-\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m \n\u001b[1;32m    240\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwhile_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m      \u001b[49m\u001b[43mloop_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshape_invariants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape_invariants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m      \u001b[49m\u001b[43mparallel_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m      \u001b[49m\u001b[43mback_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mback_prop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m      \u001b[49m\u001b[43mswap_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswap_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmaximum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximum_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m      \u001b[49m\u001b[43mreturn_same_structure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/tensorflow/python/ops/while_loop.py:488\u001b[0m, in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m    485\u001b[0m loop_var_structure \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(type_spec\u001b[38;5;241m.\u001b[39mtype_spec_from_value,\n\u001b[1;32m    486\u001b[0m                                         \u001b[38;5;28mlist\u001b[39m(loop_vars))\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cond(\u001b[38;5;241m*\u001b[39mloop_vars):\n\u001b[0;32m--> 488\u001b[0m   loop_vars \u001b[38;5;241m=\u001b[39m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mloop_vars\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    489\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m try_to_pack \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loop_vars, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m    490\u001b[0m     packed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/tensorflow/python/ops/while_loop.py:479\u001b[0m, in \u001b[0;36mwhile_loop.<locals>.<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m    476\u001b[0m     loop_vars \u001b[38;5;241m=\u001b[39m (counter, loop_vars)\n\u001b[1;32m    477\u001b[0m     cond \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m i, lv: (  \u001b[38;5;66;03m# pylint: disable=g-long-lambda\u001b[39;00m\n\u001b[1;32m    478\u001b[0m         math_ops\u001b[38;5;241m.\u001b[39mlogical_and(i \u001b[38;5;241m<\u001b[39m maximum_iterations, orig_cond(\u001b[38;5;241m*\u001b[39mlv)))\n\u001b[0;32m--> 479\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m i, lv: (i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[43morig_body\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlv\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    480\u001b[0m   try_to_pack \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m executing_eagerly:\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/keras/src/backend/tensorflow/core.py:619\u001b[0m, in \u001b[0;36mwhile_loop.<locals>._body\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_body\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 619\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mbody\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(outputs) \u001b[38;5;28;01mif\u001b[39;00m is_tuple \u001b[38;5;28;01melse\u001b[39;00m (outputs,)\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/keras_hub/src/samplers/sampler.py:106\u001b[0m, in \u001b[0;36mSampler.__call__.<locals>.body\u001b[0;34m(prompt, cache, index)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbody\u001b[39m(prompt, cache, index):\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Compute the softmax distribution for the next token.\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     logits, _, cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_probabilities(logits)\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# Compute the next token.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 15\u001b[0m, in \u001b[0;36mdecode_sequences.<locals>.next\u001b[0;34m(prompt, cache, index)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnext\u001b[39m(prompt, cache, index):\n\u001b[0;32m---> 15\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mencoder_input_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[:, index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, :]\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Ignore hidden states for now; only needed for contrastive search.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/keras_hub/src/layers/modeling/token_and_position_embedding.py:125\u001b[0m, in \u001b[0;36mTokenAndPositionEmbedding.call\u001b[0;34m(self, inputs, start_index)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, start_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    124\u001b[0m     embedded_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoken_embedding(inputs)\n\u001b[0;32m--> 125\u001b[0m     embedded_positions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedded_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m embedded_tokens \u001b[38;5;241m+\u001b[39m embedded_positions\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/keras_hub/src/layers/modeling/position_embedding.py:101\u001b[0m, in \u001b[0;36mPositionEmbedding.call\u001b[0;34m(self, inputs, start_index)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# trim to match the length of the input sequence, which might be less\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# than the sequence_length of the layer.\u001b[39;00m\n\u001b[1;32m    100\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings)\n\u001b[0;32m--> 101\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_length\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mbroadcast_to(position_embeddings, shape)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling PositionEmbedding.call().\n\n\u001b[1m{{function_node __wrapped__Slice_device_/job:localhost/replica:0/task:0/device:CPU:0}} Expected size[0] in [0, 50], but got 69 [Op:Slice]\u001b[0m\n\nArguments received by PositionEmbedding.call():\n   inputs=tf.Tensor(shape=(1, 69, 128), dtype=float32)\n   start_index=0"
     ]
    }
   ],
   "source": [
    "rouge_1 = keras_hub.metrics.RougeN(order=1)\n",
    "rouge_2 = keras_hub.metrics.RougeN(order=2)\n",
    "\n",
    "for test_pair in test_pairs[:30]:\n",
    "    input_sentence = test_pair[0]\n",
    "    reference_sentence = test_pair[1]\n",
    "\n",
    "    translated_sentence = decode_sequences([input_sentence])\n",
    "    translated_sentence = decode_sequences([input_sentence])[0]\n",
    "    translated_sentence = (\n",
    "        translated_sentence.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    rouge_1(reference_sentence, translated_sentence)\n",
    "    rouge_2(reference_sentence, translated_sentence)\n",
    "\n",
    "print(\"ROUGE-1 Score: \", rouge_1.result())\n",
    "print(\"ROUGE-2 Score: \", rouge_2.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
