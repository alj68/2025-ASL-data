{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43c1584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adrianajimenez/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/adrianajimenez/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports for model\n",
    "\n",
    "import keras_hub\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df1779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for pre-parsing\n",
    "\n",
    "from pyparsing import Word, alphas, nums\n",
    "import pyparsing as pp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd3efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handshapes\n",
    "\n",
    "handshapelist = [\n",
    "  '1',\n",
    "  '3',\n",
    "  '4',\n",
    "  '5',\n",
    "  '6',\n",
    "  '7',\n",
    "  '8',\n",
    "  '9',\n",
    "  '10',\n",
    "  '25',\n",
    "  'A',\n",
    "  'B',\n",
    "  'C',\n",
    "  'D',\n",
    "  'E',\n",
    "  'F',\n",
    "  'G',\n",
    "  'H-U',\n",
    "  'I',\n",
    "  'K',\n",
    "  'L',\n",
    "  'M',\n",
    "  'N',\n",
    "  'O',\n",
    "  'R',\n",
    "  'S',\n",
    "  'T',\n",
    "  'U',\n",
    "  'V',\n",
    "  'W',\n",
    "  'X',\n",
    "  'Y',\n",
    "  'C-L',\n",
    "  'U-L',\n",
    "  'B-L',\n",
    "  'P-K',\n",
    "  'Q-G',\n",
    "  'L-X',\n",
    "  'I-L-Y',\n",
    "  '5-C',\n",
    "  '5-C-L',\n",
    "  '5-C-tt',\n",
    "  'alt-M',\n",
    "  'alt-N',\n",
    "  'alt-P',\n",
    "  'B-xd',\n",
    "  'baby-O',\n",
    "  'bent-1',\n",
    "  'bent-B',\n",
    "  'bent-B-L',\n",
    "  'bent-horns',\n",
    "  'bent-L',\n",
    "  'bent-M',\n",
    "  'bent-N',\n",
    "  'bent-U',\n",
    "  'cocked-8',\n",
    "  'cocked-F',\n",
    "  'cocked-S',\n",
    "  'crvd-5',\n",
    "  'crvd-B',\n",
    "  'crvd-flat-B',\n",
    "  'crvd-L',\n",
    "  'crvd-sprd-B',\n",
    "  'crvd-U',\n",
    "  'crvd-V',\n",
    "  'fanned-flat-O',\n",
    "  'flat-B',\n",
    "  'flat-C',\n",
    "  'flat-F',\n",
    "  'flat-G',\n",
    "  'flat-O-2',\n",
    "  'flat-O',\n",
    "  'full-M',\n",
    "  'horns',\n",
    "  'loose-E',\n",
    "  'O-2-horns',\n",
    "  'open-8',\n",
    "  'open-F',\n",
    "  'sml-C-3',\n",
    "  'tight-C-2',\n",
    "  'tight-C',\n",
    "  'X-over-thumb'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0a3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex rules\n",
    "\n",
    "number_regexp = r\"\"\"\n",
    "    [0-9]+                      # one or more digits\n",
    "    (?:\\.[0-9]+)?               # optional decimal\n",
    "    s?                          # optional trailing 's'\n",
    "\"\"\"\n",
    "\n",
    "alpha_regexp = r\"\"\"\n",
    "    (?!                         # negative lookahead for:\n",
    "        (?:THUMB-)?             # optional THUMB-\n",
    "        (?:IX-|POSS-|SELF-)     # followed by IX-, POSS-, SELF-\n",
    "    )\n",
    "    [A-Z0-9]                    # starts with uppercase or digit\n",
    "    (?:                         # optionally followed by:\n",
    "        [A-Z0-9'-]*             #   more letters, digits, hyphen or apostrophe\n",
    "        [A-Z0-9]                #   ends on a letter or digit\n",
    "    )?\n",
    "    (?:                         # optionally followed by:\n",
    "        \\.                      #   a period\n",
    "      |                         # or\n",
    "        :[0-9]                  #   colon-digit (e.g., :2)\n",
    "    )?\n",
    "\"\"\"\n",
    "\n",
    "lookahead_regexp = r\"\"\"\n",
    "    (?:\n",
    "        (?![a-z])               # not followed by lowercase letter\n",
    "      | (?=wg)                  # unless it's specifically \"wg\"\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "word_all_regexp = r\"\"\"\n",
    "    (?: {number_regexp} | {alpha_regexp} )\n",
    "    {lookalookahead_regexphead}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conventions kept for parsing\n",
    "\n",
    "cl_prefix = pp.one_of([\"CL\", \"DCL\", \"LCL\", \"SCL\", \"BCL\", \"BPCL\", \"PCL\", \"ICL\"])\n",
    "ns_prefix = pp.Literal(\"ns\")\n",
    "fs_prefix = pp.Literal(\"fs\")\n",
    "lex_exceptions = pp.one_of([\"part\", \"WHAT\"])\n",
    "aspect_text = pp.Literal(\"aspect\")\n",
    "index_core_ix = pp.Literal(\"IX\")\n",
    "other_index_core = pp.one_of([\"POSS\", \"SELF\"])\n",
    "handshape = pp.one_of(handshapelist)\n",
    "person = pp.one_of([\"1p\", \"2p\", \"3p\"])\n",
    "dash = pp.Literal(\"-\")\n",
    "arc = pp.Literal(\"arc\") \n",
    "loc = pp.Literal(\"loc\")\n",
    "pl = pp.Literal(\"pl\")\n",
    "compound = pp.Literal(\"+\")\n",
    "hashtag = pp.Literal(\"#\")\n",
    "choice = pp.Literal(\"/\")\n",
    "sym = pp.Literal(\">\")\n",
    "par1 = pp.Literal(\"(\")\n",
    "par2 = pp.Literal(\")\")\n",
    "contraction = pp.Literal(\"^\")\n",
    "colon = pp.Literal(\":\")\n",
    "omit_quote = pp.Literal(\"xx\")\n",
    "period = pp.Literal(\".\")\n",
    "alphas = pp.Word(alphas, max=1)\n",
    "nums = pp.Word(nums, max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b062760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammar rules\n",
    "\n",
    "full_grammar = pp.OneOrMore(\n",
    "    cl_prefix |               # classifiers like CL, DCL, etc.\n",
    "    ns_prefix |               # non-specific ns\n",
    "    fs_prefix |               # fingerspelling fs\n",
    "    index_core_ix |           # IX\n",
    "    other_index_core |        # POSS, SELF\n",
    "    person |                  # 1p, 2p, 3p\n",
    "    lex_exceptions |          # part, WHAT\n",
    "    aspect_text |             # aspect\n",
    "    arc |                     # arc\n",
    "    loc |                     # loc\n",
    "    pl |                      # pl\n",
    "    handshape |               # handshapes like B, 1, 5, etc.\n",
    "    compound |                # +\n",
    "    hashtag |                 # #\n",
    "    choice |                  # /\n",
    "    sym |                     # >\n",
    "    contraction |             # ^\n",
    "    colon |                   # :\n",
    "    dash |                    # -\n",
    "    par1 | par2 |             # ( and )\n",
    "    omit_quote |              # xx\n",
    "    period |                  # .\n",
    "    nums |                    # numbers last resort\n",
    "    alphas                     # fallback LAST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45096aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCL', ':', '1', 'xx']\n",
      "['IX', '-', '1p', '-', 'pl', '-', '2', 'W', 'O', 'R', 'K', 'L', 'A', 'N', 'D', 'S', 'C', 'A', 'P', 'E', 'fs', '-', 'L', 'A', 'N', 'D', 'S', 'C', 'A', 'P', 'I', 'N', 'G', 'IX', '-', '1p', '5', 'xx']\n"
     ]
    }
   ],
   "source": [
    "# testing grammar parsing\n",
    "\n",
    "trial = full_grammar.parse_string(\"SCL:1xx\", parse_all=True).asList()\n",
    "trial2 = full_grammar.parse_string(\"IX-1p-pl-2 WORK LANDSCAPE fs-LANDSCAPING IX-1p 5xx\", parse_all=True).as_list()\n",
    "\n",
    "print(trial)\n",
    "print(trial2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258c52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize based on predefined grammar rules\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    try:\n",
    "        return full_grammar.parse_string(text, parse_all=True).as_list()\n",
    "    except pp.ParseException as pe:\n",
    "        print(f\"Failed to parse: {pe}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9cc0cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCL', ':', '1', 'xx']\n",
      "['SCL', ':', '1', 'xx', 'S', 'H', 'O', 'W', 'E', 'R', 'W', 'A', 'S', 'H', 'F', 'E', 'E', 'L', 'T', 'H', 'U', 'M', 'B', 'S', '-', 'U', 'P', '/', 'G', 'O', 'O', 'D']\n"
     ]
    }
   ],
   "source": [
    "# testing custom_tokenize\n",
    "\n",
    "trial = custom_tokenize(\"SCL:1xx\")\n",
    "trial2 = custom_tokenize('SCL:1xx SHOWER WASH FEEL THUMBS-UP/GOOD')\n",
    "\n",
    "print(trial)\n",
    "print(trial2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e79a6b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters / hyperparameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30  # This should be at least 10 for convergence\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "ENG_VOCAB_SIZE = 5056\n",
    "ASL_VOCAB_SIZE = 2283\n",
    "num_samples = 1400\n",
    "\n",
    "EMBED_DIM = 128\n",
    "INTERMEDIATE_DIM =1024\n",
    "NUM_HEADS = 8\n",
    "data_path = \"/Users/adrianajimenez/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/data/sent_pairs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3df98b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_tokens: [' ', '!', '\"', '$', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "output_tokens ['#', '(', ')', '+', '-', '.', '/', '0', '1', '10', '1p', '2', '25', '2p', '3', '3p', '4', '5', '5-C', '5-C-L', '6', '7', '8', '9', ':', '>', 'A', 'B', 'B-L', 'BCL', 'BPCL', 'C', 'CL', 'D', 'DCL', 'E', 'F', 'G', 'H', 'I', 'ICL', 'IX', 'J', 'K', 'L', 'LCL', 'M', 'N', 'O', 'P', 'PCL', 'POSS', 'Q', 'R', 'S', 'SCL', 'SELF', 'T', 'U', 'V', 'W', 'WHAT', 'X', 'X-over-thumb', 'Y', 'Z', '^', 'a', 'arc', 'b', 'bent-1', 'bent-B', 'bent-B-L', 'c', 'crvd-5', 'crvd-B', 'crvd-L', 'crvd-V', 'crvd-sprd-B', 'd', 'e', 'f', 'fanned-flat-O', 'flat-B', 'flat-O', 'fs', 'g', 'h', 'i', 'j', 'k', 'l', 'loc', 'm', 'n', 'ns', 'o', 'p', 'part', 'pl', 'r', 's', 't', 'u', 'v', 'w', 'xx']\n",
      "num_eng_tokens 76\n",
      "num_asl_tokens 107\n",
      "['#', '(', ')', '+', '-', '.', '/', '0', '1', '10', '1p', '2', '25', '2p', '3', '3p', '4', '5', '5-C', '5-C-L', '6', '7', '8', '9', ':', '>', 'A', 'B', 'B-L', 'BCL', 'BPCL', 'C', 'CL', 'D', 'DCL', 'E', 'F', 'G', 'H', 'I', 'ICL', 'IX', 'J', 'K', 'L', 'LCL', 'M', 'N', 'O', 'P', 'PCL', 'POSS', 'Q', 'R', 'S', 'SCL', 'SELF', 'T', 'U', 'V', 'W', 'WHAT', 'X', 'X-over-thumb', 'Y', 'Z', '^', 'a', 'arc', 'b', 'bent-1', 'bent-B', 'bent-B-L', 'c', 'crvd-5', 'crvd-B', 'crvd-L', 'crvd-V', 'crvd-sprd-B', 'd', 'e', 'f', 'fanned-flat-O', 'flat-B', 'flat-O', 'fs', 'g', 'h', 'i', 'j', 'k', 'l', 'loc', 'm', 'n', 'ns', 'o', 'p', 'part', 'pl', 'r', 's', 't', 'u', 'v', 'w', 'xx']\n"
     ]
    }
   ],
   "source": [
    "# generate files of\n",
    "    # 1) eng-asl sentence pairs\n",
    "    # 2) list of unique english vocab\n",
    "    # 3) list of unique asl vocab (main glosses only)\n",
    "text_pairs = []\n",
    "\n",
    "input_tokens = set()\n",
    "target_tokens = set()\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "   \n",
    "tokens_used = [] \n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    pair = []\n",
    "    input_text, target_text = line.split(\"\\t\")\n",
    "    pair.append(input_text)\n",
    "    if custom_tokenize(target_text):\n",
    "        pair.append(target_text)\n",
    "    text_pairs.append(pair)\n",
    "    for token in input_text:\n",
    "        if token not in input_tokens:\n",
    "            input_tokens.add(token)\n",
    "            \n",
    "            \n",
    "for pair in text_pairs:\n",
    "    sent_tokens = custom_tokenize(pair[1])\n",
    "    for token in sent_tokens:\n",
    "        if token not in target_tokens:\n",
    "            target_tokens.add(token)\n",
    "\n",
    "\n",
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))\n",
    "\n",
    "print(\"input_tokens:\", input_tokens)\n",
    "print(\"output_tokens\", target_tokens)\n",
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "print(\"num_eng_tokens\", num_encoder_tokens)\n",
    "print(\"num_asl_tokens\", num_decoder_tokens)\n",
    "print(target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b587712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The headlights got closer and closer to the deer.', 'DCL:fanned-flat-Oxx DEER DCL:fanned-flat-Oxx']\n",
      "['What did John buy yesterday?', 'fs-JOHN BUY YESTERDAY xx']\n",
      "['John bought many books.', 'IX-3p fs-JOHN BUY MANY BOOK']\n",
      "['Take Rodney King, for example.  ', 'FOR EXAMPLE fs-RODNEY fs-KING FOR EXAMPLE 5xx']\n",
      "['That book, John finished reading it yesterday.', 'THAT BOOK fs-JOHN FINISH READ BOOK YESTERDAY']\n"
     ]
    }
   ],
   "source": [
    "# glimpse pairs\n",
    "\n",
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c010cbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400 total pairs\n",
      "980 training pairs\n",
      "210 validation pairs\n",
      "210 test pairs\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32be0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train_word_piece\n",
    "\n",
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_hub.tokenizers.compute_word_piece_vocabulary(\n",
    "        word_piece_ds.batch(1000).prefetch(2),\n",
    "        vocabulary_size=vocab_size,\n",
    "        reserved_tokens=reserved_tokens,\n",
    "    )\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5335f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 18:32:23.601225: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-07-14 18:32:25.134238: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "asl_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "asl_vocab = train_word_piece(asl_samples, ASL_VOCAB_SIZE, reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0e3764c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Tokens:  ['##n', 'of', '##er', 'for', 'said', 'you', 'house', 'teacher', 'on', '##es']\n",
      "ASL Tokens:  ['THAT', 'LOOK', 'ICLxx', 'arc', 'LIKE', '##N', '##xx', 'TEACHER', 'SAME', 'HOUSE']\n"
     ]
    }
   ],
   "source": [
    "print(\"English Tokens: \", eng_vocab[100:110])\n",
    "print(\"ASL Tokens: \", asl_vocab[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9895e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=eng_vocab, lowercase=False\n",
    ")\n",
    "asl_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=asl_vocab, lowercase=False, split=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb29eb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:  Then I got back into the shower and washed up. It felt so good!\n",
      "Tokens:  tf.Tensor(\n",
      "[274  35 159 139 281  78  70 144 166 102  81  83 144  87 110  13 118 345\n",
      " 173 214   4], shape=(21,), dtype=int32)\n",
      "Recovered text after detokenizing:  Then I got back into the shower and washed up . It felt so good !\n",
      "\n",
      "ASL input: 'SCL:1xx SHOWER WASH FEEL THUMBS-UP/GOOD'\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__EnsureShape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shape of tensor input [] is not compatible with expected shape [?]. [Op:EnsureShape] name: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m asl_input_ex \u001b[38;5;241m=\u001b[39m text_pairs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL input: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00masl_input_ex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m---> 14\u001b[0m asl_tokens_ex \u001b[38;5;241m=\u001b[39m \u001b[43masl_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43masl_input_ex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mASL Gloss sentence: \u001b[39m\u001b[38;5;124m\"\u001b[39m, asl_input_ex)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokens: \u001b[39m\u001b[38;5;124m\"\u001b[39m, asl_tokens_ex)\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/keras_hub/src/utils/tensor_utils.py:57\u001b[0m, in \u001b[0;36mpreprocessing_function.<locals>.wrapper\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m convert_preprocessing_inputs(x)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m no_convert_scope():\n\u001b[0;32m---> 57\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m convert_preprocessing_outputs(x)\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/keras_hub/src/tokenizers/word_piece_tokenizer.py:501\u001b[0m, in \u001b[0;36mWordPieceTokenizer.tokenize\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unbatched:\n\u001b[1;32m    500\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39msqueeze(tokens, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 501\u001b[0m     \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mensure_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:6006\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6004\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   6005\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 6006\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__EnsureShape_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shape of tensor input [] is not compatible with expected shape [?]. [Op:EnsureShape] name: "
     ]
    }
   ],
   "source": [
    "eng_input_ex = text_pairs[0][0]\n",
    "eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\n",
    "print(\"English sentence: \", eng_input_ex)\n",
    "print(\"Tokens: \", eng_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    eng_tokenizer.detokenize(eng_tokens_ex),\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "asl_input_ex = text_pairs[0][1]\n",
    "print(f\"ASL input: '{asl_input_ex}'\") \n",
    "asl_tokens_ex = asl_tokenizer.tokenize(asl_input_ex)\n",
    "print(\"ASL Gloss sentence: \", asl_input_ex)\n",
    "print(\"Tokens: \", asl_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    asl_tokenizer.detokenize(asl_tokens_ex),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c4a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(eng, asl):\n",
    "    batch_size = ops.shape(asl)[0]\n",
    "\n",
    "    eng = eng_tokenizer(eng)\n",
    "    asl = asl_tokenizer(asl)\n",
    "\n",
    "    # Pad `eng` to `MAX_SEQUENCE_LENGTH`.\n",
    "    eng_start_end_packer = keras_hub.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    eng = eng_start_end_packer(eng)\n",
    "\n",
    "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `asl` and pad it as well.\n",
    "    asl_start_end_packer = keras_hub.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
    "        start_value=asl_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value=asl_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value=asl_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    asl = asl_start_end_packer(asl)\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": eng,\n",
    "            \"decoder_inputs\": asl[:, :-1],\n",
    "        },\n",
    "        asl[:, 1:],\n",
    "    )\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, asl_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    asl_texts = list(asl_texts)\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, asl_texts))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a04a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=ENG_VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_hub.layers.TransformerEncoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(inputs=x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=ASL_VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(decoder_inputs)\n",
    "\n",
    "x = keras_hub.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(ASL_VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoded_seq_inputs,\n",
    "    ],\n",
    "    decoder_outputs,\n",
    ")\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"transformer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad617331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequences(input_sentences):\n",
    "    batch_size = 1\n",
    "\n",
    "    # Tokenize the encoder input.\n",
    "    encoder_input_tokens = ops.convert_to_tensor(eng_tokenizer(input_sentences))\n",
    "    if len(encoder_input_tokens[0]) < MAX_SEQUENCE_LENGTH:\n",
    "        pads = ops.full((1, MAX_SEQUENCE_LENGTH - len(encoder_input_tokens[0])), 0)\n",
    "        encoder_input_tokens = ops.concatenate(\n",
    "            [encoder_input_tokens.to_tensor(), pads], 1\n",
    "        )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def next(prompt, cache, index):\n",
    "        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "        # Ignore hidden states for now; only needed for contrastive search.\n",
    "        hidden_states = None\n",
    "        return logits, hidden_states, cache\n",
    "\n",
    "    # Build a prompt of length 40 with a start token and padding tokens.\n",
    "    length = 40\n",
    "    start = ops.full((batch_size, 1), asl_tokenizer.token_to_id(\"[START]\"))\n",
    "    pad = ops.full((batch_size, length - 1), asl_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    prompt = ops.concatenate((start, pad), axis=-1)\n",
    "\n",
    "    generated_tokens = keras_hub.samplers.GreedySampler()(\n",
    "        next,\n",
    "        prompt,\n",
    "        stop_token_ids=[asl_tokenizer.token_to_id(\"[END]\")],\n",
    "        index=1,  # Start sampling after start token.\n",
    "    )\n",
    "    generated_sentences = asl_tokenizer.detokenize(generated_tokens)\n",
    "    return generated_sentences\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(2):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequences([input_sentence])\n",
    "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
    "    translated = (\n",
    "        translated.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    print(f\"** Example {i} **\")\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ff7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_1 = keras_hub.metrics.RougeN(order=1)\n",
    "rouge_2 = keras_hub.metrics.RougeN(order=2)\n",
    "\n",
    "for test_pair in test_pairs[:30]:\n",
    "    input_sentence = test_pair[0]\n",
    "    reference_sentence = test_pair[1]\n",
    "\n",
    "    translated_sentence = decode_sequences([input_sentence])\n",
    "    translated_sentence = translated_sentence.numpy()[0].decode(\"utf-8\")\n",
    "    translated_sentence = (\n",
    "        translated_sentence.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    rouge_1(reference_sentence, translated_sentence)\n",
    "    rouge_2(reference_sentence, translated_sentence)\n",
    "\n",
    "print(\"ROUGE-1 Score: \", rouge_1.result())\n",
    "print(\"ROUGE-2 Score: \", rouge_2.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
