{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43c1584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adrianajimenez/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import keras_hub\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow as tf\n",
    "from tensorflow_text.tools.wordpiece_vocab import (\n",
    "    bert_vocab_from_dataset,\n",
    ")\n",
    "import pandas as pd\n",
    "from keras_nlp.samplers import TopKSampler\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df1779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for pre-parsing\n",
    "from pyparsing import Word, alphas as pp_alpha, nums as pp_nums\n",
    "import pyparsing as pp\n",
    "pp.ParserElement.enablePackrat()\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a6a8a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictTokenizer:\n",
    "    def __init__(self, vocab, tokenizer_fn):\n",
    "        self.token_to_id_map = vocab\n",
    "        self.id_to_token_map = {i: t for t, i in vocab.items()}\n",
    "        self.tokenizer_fn = tokenizer_fn\n",
    "\n",
    "    def __call__(self, text_batch):\n",
    "        return [\n",
    "            [self.token_to_id_map.get(tok, self.token_to_id_map.get(\"[UNK]\", 0)) \n",
    "             for tok in self.tokenizer_fn(text)]\n",
    "            for text in text_batch\n",
    "        ]\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        return [self.token_to_id_map.get(tok, self.token_to_id_map.get(\"[UNK]\", 0)) \n",
    "                for tok in self.tokenizer_fn(text)]\n",
    "\n",
    "    def detokenize(self, token_ids):\n",
    "        if isinstance(token_ids, tf.Tensor):\n",
    "            token_ids = token_ids.numpy()\n",
    "        elif isinstance(token_ids, tf.RaggedTensor):\n",
    "            token_ids = token_ids.to_tensor().numpy()\n",
    "        elif isinstance(token_ids, int):\n",
    "            token_ids = [token_ids]\n",
    "\n",
    "        return \" \".join([self.id_to_token_map.get(int(tok_id), \"[UNK]\") for tok_id in token_ids])\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        return self.token_to_id_map.get(token, self.token_to_id_map.get(\"[UNK]\", 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0a3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex rules\n",
    "\n",
    "alpha_regexp = r\"\"\"\n",
    "(?!((?:THUMB-)?(?:IX|POSS|SELF)))   # negative lookahead for blocked glosses\n",
    "[A-Z]                               # must start with uppercase\n",
    "(?:                                 # optional middle section\n",
    "    (?:                             # non-capturing group for allowed connectors\n",
    "        (?:[-/][A-Z])               # hyphen or slash must be followed by uppercase\n",
    "      | (?:_[0-9])                  # underscore must be followed by digit\n",
    "      | (?:\\+(?:[A-Z#]|fs-))       # plus + (uppercase OR # OR the literal fs-)\n",
    "      | [A-Z0-9]                    # regular letter/digit continuation\n",
    "    )\n",
    ")*                                  # repeatable\n",
    "(?:\\.)?                             # optional trailing period\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbd4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conventions kept for parsing\n",
    "\n",
    "cl_prefix = pp.one_of([\"CL\", \"DCL\", \"LCL\", \"SCL\", \"BCL\", \"BPCL\", \"PCL\", \"ICL\"])\n",
    "fs_prefix = pp.Literal(\"fs-\")\n",
    "index_core_ix = pp.Literal(\"IX\")\n",
    "other_index_core = pp.one_of([\"POSS\", \"SELF\"])\n",
    "hashtag = pp.Literal(\"#\")\n",
    "dash = pp.Literal(\"-\")\n",
    "contraction = pp.Literal(\"^\")\n",
    "period = pp.Literal(\".\")\n",
    "alpha = pp.Word(pp_alpha, max=1)\n",
    "num = pp.Word(pp_nums, max=1)\n",
    "word = pp.Regex(alpha_regexp, flags=re.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b062760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammar rules\n",
    "\n",
    "full_grammar = pp.OneOrMore(\n",
    "    fs_prefix |               # fingerspelling fs\n",
    "    word |\n",
    "    cl_prefix |               # classifiers like CL, DCL, etc.\n",
    "    index_core_ix |           # IX\n",
    "    other_index_core |        # POSS, SELF\n",
    "    hashtag |                 # #\n",
    "    contraction |             # ^\n",
    "    period |                  # .\n",
    "    dash |\n",
    "    num |\n",
    "    alpha                     # fallback LAST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "258c52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize based on predefined grammar rules\n",
    "\n",
    "def custom_asl_tokenize(text):\n",
    "    try:\n",
    "        if \"'\" in text:\n",
    "            text = text.replace(\"'\", \"\")\n",
    "        if \"++\" in text:\n",
    "            text = text.replace(\"++\", \"+\")\n",
    "        return full_grammar.parse_string(text, parse_all=True).asList()\n",
    "    except pp.ParseException as pe:\n",
    "        print(text)\n",
    "        print(f\"Failed to parse: {pe}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03d3e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_eng_tokenize(text):\n",
    "    # Perserve punctuation and digits\n",
    "    text = re.sub(r'([^\\w\\s]|\\d)', r' \\1 ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Split on whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3df98b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng_tokens: ['!', '\"', '$', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '[END]', '[PAD]', '[START]', '[UNK]', '`', 'a', 'aaa', 'aaahhh', 'abcs', 'abdominal', 'able', 'about', 'above', 'abuse', 'accept', 'acceptable', 'acceptance', 'accepted', 'accident', 'accidentally', 'accommodations', 'accomplishment', 'across', 'act', 'acting', 'action', 'actions', 'activities', 'actor', 'actors', 'actually', 'added', 'addicted', 'addicts', 'admit', 'adopt', 'adopted', 'adopts', 'advantage', 'adventure', 'advice', 'advisior', 'ae', 'afford', 'afraid', 'after', 'afternoon', 'again', 'against', 'age', 'ago', 'agree', 'agreed', 'ah', 'ahead', 'ahh', 'ail', 'air', 'airline', 'airlines', 'airplanes', 'alarm', 'alcohol', 'alec', 'ali', 'alike', 'alive', 'all', 'allergic', 'allow', 'allowed', 'almost', 'alone', 'along', 'alphabetical', 'already', 'alright', 'also', 'always', 'am', 'amazing', 'american', 'amy', 'an', 'analyze', 'analyzed', 'analyzing', 'ancestors', 'and', 'angela', 'angeles', 'anger', 'angled', 'angry', 'animal', 'animals', 'ankle', 'ann', 'anniversary', 'annotation', 'announced', 'annoying', 'annual', 'another', 'answer', 'answers', 'any', 'anybody', 'anymore', 'anyone', 'anything', 'anyway', 'anywhere', 'apartment', 'applaud', 'apple', 'apples', 'applied', 'apply', 'appointment', 'appreciated', 'approach', 'approached', 'approches', 'appropriate', 'appropriately', 'are', 'area', 'areas', 'aren', 'arm', 'arms', 'around', 'arrest', 'arrested', 'arried', 'arrive', 'arrived', 'arrives', 'arriving', 'article', 'as', 'asia', 'asian', 'aside', 'ask', 'asked', 'asking', 'asks', 'asl', 'asleep', 'aspect', 'assaulted', 'assess', 'assigned', 'assistant', 'associated', 'associating', 'assorted', 'assume', 'assumed', 'assumption', 'astonished', 'at', 'ate', 'attack', 'attacker', 'attempt', 'attending', 'attention', 'audio', 'audiologist', 'audiovocal', 'audition', 'aunt', 'australia', 'automatic', 'automatically', 'average', 'avoid', 'awake', 'award', 'aware', 'away', 'awe', 'awful', 'awfully', 'awkward', 'b', 'babies', 'baby', 'back', 'background', 'bad', 'bag', 'bagel', 'bags', 'bake', 'balance', 'bald', 'baldwin', 'ball', 'band', 'bandage', 'banged', 'banging', 'banks', 'bar', 'barack', 'barbaric', 'barbeque', 'barely', 'baritone', 'baseball', 'based', 'basically', 'basketball', 'bathroom', 'bauhman', 'bauman', 'bawls', 'bbq', 'be', 'beach', 'beacuse', 'bear', 'beat', 'beating', 'beats', 'beautiful', 'became', 'because', 'become', 'becomes', 'becoming', 'bed', 'been', 'beer', 'before', 'began', 'begin', 'beginning', 'behaving', 'behind', 'being', 'believe', 'belong', 'below', 'belt', 'ben', 'benard', 'beneath', 'bent', 'berry', 'best', 'beth', 'better', 'between', 'beverly', 'beyond', 'bib', 'bible', 'bicycle', 'big', 'bigger', 'bike', 'bill', 'birth', 'birthday', 'bit', 'bite', 'black', 'bladder', 'blamed', 'blames', 'bled', 'bleed', 'bleeding', 'blew', 'block', 'blocks', 'blood', 'bloomberg', 'blow', 'blown', 'blue', 'blueberry', 'boat', 'bob', 'body', 'boldly', 'bonfire', 'bonfires', 'book', 'books', 'boots', 'border', 'bored', 'boring', 'born', 'borrow', 'borrowed', 'boss', 'boston', 'bostonians', 'both', 'bother', 'bothered', 'bottle', 'bought', 'bowl', 'box', 'boxer', 'boxes', 'boxing', 'boy', 'boys', 'braced', 'bracelet', 'bragg', 'brakes', 'bread', 'break', 'breakdown', 'breakfast', 'breaking', 'breaks', 'breath', 'breathe', 'breathing', 'brick', 'bricks', 'bride', 'bring', 'brings', 'broccoli', 'broke', 'broken', 'brookline', 'brother', 'brothers', 'brought', 'brownish', 'bruce', 'bruise', 'bu', 'bubbles', 'bucket', 'bucks', 'build', 'building', 'buildings', 'built', 'bumps', 'bun', 'bunch', 'bundled', 'burger', 'burn', 'burned', 'burp', 'bus', 'bush', 'business', 'but', 'butcher', 'butter', 'butts', 'buy', 'buying', 'buys', 'by', 'c', 'cabin', 'cabinet', 'cabinets', 'cafeteria', 'cake', 'cakes', 'calfornia', 'california', 'call', 'called', 'calling', 'calls', 'calm', 'calmly', 'came', 'camp', 'campground', 'camping', 'can', 'cancel', 'canceled', 'cancelled', 'cancer', 'candidate', 'candy', 'cannot', 'capable', 'cape', 'car', 'card', 'care', 'career', 'careful', 'carol', 'carried', 'carrying', 'cars', 'case', 'cases', 'casino', 'cat', 'catch', 'categories', 'categorizations', 'category', 'caucus', 'caught', 'cause', 'caused', 'caviar', 'cds', 'celebrate', 'celebrated', 'celebrating', 'celebrations', 'cereal', 'chair', 'chairs', 'chalkboard', 'challenge', 'challenging', 'chance', 'change', 'changed', 'changing', 'channel', 'chapter', 'character', 'characteristics', 'charge', 'charles', 'chased', 'chasing', 'chat', 'chatted', 'chatting', 'cheap', 'cheaper', 'cheat', 'cheating', 'check', 'checked', 'checklist', 'checks', 'cheer', 'cheered', 'cheering', 'cheese', 'chemicals', 'chess', 'chicago', 'chicken', 'child', 'children', 'chimney', 'china', 'chocolate', 'choice', 'choices', 'chokes', 'choose', 'chopped', 'christmas', 'chrome', 'church', 'churning', 'cigarette', 'cigarettes', 'cities', 'city', 'clanking', 'class', 'classroom', 'clean', 'cleaned', 'cleaning', 'cleans', 'clear', 'clearly', 'climb', 'climbing', 'clip', 'cliques', 'clock', 'clooney', 'close', 'closed', 'closer', 'closes', 'closing', 'clothes', 'club', 'clubs', 'clue', 'cnn', 'coach', 'coat', 'coats', 'cochlear', 'cod', 'coda', 'codas', 'code', 'coffee', 'cofffee', 'coffin', 'cognitive', 'cohesiveness', 'cold', 'colder', 'collaborate', 'collar', 'collars', 'colleague', 'colleagues', 'collect', 'collected', 'college', 'collided', 'color', 'colorado', 'colors', 'come', 'comedy', 'comes', 'comfortable', 'coming', 'commendable', 'comment', 'comments', 'communicate', 'communicated', 'communication', 'community', 'commute', 'commutes', 'commuting', 'company', 'compare', 'compared', 'compares', 'comparison', 'compatible', 'compelled', 'competition', 'complete', 'completed', 'completely', 'completing', 'comprehensible', 'computer', 'computers', 'concept', 'conception', 'concert', 'conditioning', 'conduct', 'confidence', 'confident', 'conflict', 'confusion', 'connected', 'connection', 'conntected', 'considered', 'considering', 'constantly', 'contact', 'contemptible', 'context', 'continue', 'continued', 'continues', 'contributed', 'contro', 'control', 'controlled', 'controlling', 'conversation', 'cooked', 'cookies', 'cooking', 'cool', 'cooler', 'coordinator', 'cop', 'copies', 'cops', 'copy', 'corn', 'corner', 'correlated', 'costs', 'costume', 'cough', 'coughing', 'coughs', 'could', 'couldn', 'counselor', 'countries', 'country', 'couple', 'course', 'cousin', 'cover', 'covers', 'cow', 'cows', 'cpr', 'cracked', 'crash', 'crashed', 'crashes', 'crashing', 'crazy', 'cream', 'create', 'creative', 'creatively', 'creepy', 'crib', 'cried', 'crisis', 'criteria', 'criticize', 'crops', 'cross', 'crossing', 'crowd', 'crowded', 'cruiser', 'crying', 'csun', 'cued', 'cuff', 'cultrual', 'cultural', 'culturally', 'culture', 'cultures', 'cup', 'cups', 'curious', 'current', 'currently', 'curvy', 'cut', 'cute', 'cutting', 'd', 'dad', 'dairy', 'daisy', 'dana', 'dance', 'danced', 'dark', 'date', 'daughter', 'dawn', 'day', 'days', 'dead', 'deaf', 'deafies', 'deal', 'dean', 'dear', 'deathly', 'december', 'decide', 'decided', 'decides', 'decision', 'decorate', 'decrease', 'deep', 'deeply', 'deer', 'definitely', 'degrees', 'delicious', 'delighted', 'demi', 'democrat', 'depend', 'depends', 'depressed', 'depth', 'described', 'desire', 'dessert', 'destroy', 'destroyed', 'determine', 'develop', 'developed', 'development', 'developments', 'devices', 'diagnoses', 'diagnosis', 'dialogue', 'did', 'didn', 'didnt', 'died', 'diego', 'difference', 'differences', 'different', 'differently', 'difficult', 'difficulty', 'dimwitted', 'dinner', 'direct', 'direction', 'directions', 'disappointed', 'disciplinary', 'disconnected', 'discuss', 'discussed', 'discussing', 'discussion', 'disease', 'disgusted', 'dislike', 'disorder', 'disordered', 'disorders', 'displayed', 'disruption', 'dissipate', 'distance', 'disturbed', 'diversity', 'do', 'doctor', 'doctors', 'does', 'doesn', 'dog', 'doing', 'dollar', 'dollars', 'dominance', 'don', 'donalds', 'done', 'donna', 'donut', 'door', 'doorbell', 'doorknob', 'doors', 'dorm', 'dorms', 'double', 'doubt', 'down', 'downhill', 'downstairs', 'drag', 'drain', 'dramas', 'drank', 'dream', 'dreams', 'dress', 'drink', 'drinking', 'drinks', 'drive', 'driver', 'drivers', 'drives', 'driving', 'drop', 'dropped', 'dropping', 'drops', 'drove', 'drown', 'drug', 'drugs', 'drunk', 'dry', 'due', 'dug', 'dumbfounded', 'dumped', 'during', 'dvd', 'dye', 'dyed', 'dying', 'each', 'ear', 'earlier', 'early', 'ears', 'easier', 'easily', 'easy', 'eat', 'eaten', 'eating', 'eats', 'economic', 'economy', 'edge', 'education', 'effort', 'eight', 'either', 'elbow', 'elected', 'election', 'elementary', 'eleven', 'eliminate', 'elite', 'else', 'email', 'embarrassed', 'embarrassing', 'embraces', 'emergency', 'emit', 'emits', 'emitting', 'emotion', 'empty', 'enable', 'encounter', 'encouraging', 'end', 'ended', 'ends', 'enemy', 'engine', 'english', 'engrossed', 'enjoy', 'enjoyed', 'enjoying', 'enjoys', 'ennis', 'enough', 'enrolled', 'enter', 'entered', 'entering', 'enters', 'envelope', 'environment', 'environments', 'equal', 'equally', 'erick', 'escape', 'escapes', 'especially', 'establishing', 'estimation', 'etc', 'europe', 'even', 'event', 'eventually', 'ever', 'everwhere', 'every', 'everyday', 'everyone', 'everything', 'everytime', 'exactly', 'exam', 'example', 'exceed', 'except', 'excess', 'excessive', 'exchange', 'exchanged', 'exchanging', 'excited', 'excused', 'exercise', 'exhausted', 'exist', 'exit', 'expanded', 'expectation', 'expectations', 'expected', 'expecting', 'expects', 'expenses', 'expensive', 'experience', 'experienced', 'experiences', 'experiment', 'expert', 'explain', 'explained', 'explaining', 'express', 'expression', 'extra', 'extremely', 'eye', 'eyes', 'f', 'face', 'facial', 'facing', 'fact', 'fail', 'fall', 'falls', 'family', 'famous', 'fan', 'fancy', 'fans', 'far', 'farm', 'farmland', 'fart', 'farther', 'fascinated', 'fast', 'father', 'fault', 'favorite', 'federation', 'fedex', 'feed', 'feeding', 'feel', 'feeling', 'feels', 'feet', 'fell', 'felt', 'fenced', 'few', 'field', 'fifteen', 'fight', 'fighting', 'figure', 'filled', 'films', 'final', 'finally', 'find', 'finds', 'fine', 'fined', 'finger', 'fingernails', 'fingers', 'finish', 'finished', 'finishes', 'finishing', 'fire', 'fired', 'fireplace', 'fires', 'firewood', 'fireworks', 'first', 'fish', 'fishing', 'fist', 'fists', 'fit', 'five', 'fix', 'fixed', 'flash', 'flashed', 'flat', 'flavorful', 'flew', 'flies', 'flight', 'flip', 'flood', 'flooding', 'floor', 'flops', 'florida', 'flowers', 'fluent', 'flushed', 'fly', 'flying', 'focus', 'focused', 'focusing', 'follow', 'following', 'food', 'foot', 'football', 'for', 'forbidden', 'forest', 'forever', 'forget', 'forgot', 'forks', 'form', 'formal', 'former', 'forth', 'forward', 'found', 'four', 'fourth', 'france', 'francisco', 'frank', 'fraternity', 'freak', 'free', 'french', 'frends', 'frequently', 'fresh', 'fresher', 'freshman', 'friday', 'friend', 'friendly', 'friends', 'fries', 'frisbee', 'from', 'front', 'frozen', 'fruit', 'fruits', 'frustrated', 'fuck', 'fucked', 'full', 'fumble', 'fun', 'funny', 'further', 'future', 'gain', 'gaining', 'gallaudet', 'gamble', 'gambled', 'gambling', 'game', 'games', 'garage', 'gas', 'gate', 'gather', 'gave', 'gazpacho', 'geez', 'gender', 'general', 'generally', 'generations', 'genius', 'gently', 'george', 'germany', 'gesture', 'gestured', 'gestures', 'get', 'gets', 'getting', 'ghosts', 'gift', 'gilbert', 'girl', 'girlfriend', 'girls', 'give', 'given', 'gives', 'giving', 'glass', 'glasses', 'glitter', 'globe', 'go', 'gobble', 'god', 'goes', 'going', 'gold', 'golf', 'gone', 'good', 'google', 'goose', 'got', 'gotten', 'grabbed', 'graduate', 'graduated', 'grandfather', 'grandma', 'grandstand', 'grants', 'grass', 'great', 'greatechwaech', 'greater', 'green', 'grew', 'grief', 'grin', 'grocery', 'groom', 'gross', 'ground', 'group', 'grouped', 'groups', 'grow', 'growing', 'growth', 'grun', 'grunt', 'gruntal', 'grunting', 'grunts', 'guarantee', 'guaranteed', 'guess', 'guest', 'guidelines', 'gulped', 'guy', 'guys', 'gym', 'habits', 'had', 'hadn', 'hah', 'haha', 'hair', 'half', 'hall', 'hallway', 'hamburger', 'hamburgers', 'hand', 'handcuffed', 'handcuffs', 'handle', 'hands', 'hang', 'hanging', 'hangups', 'happen', 'happened', 'happening', 'happens', 'happily', 'happiness', 'happy', 'hard', 'harry', 'harsh', 'has', 'hasn', 'hastily', 'hate', 'hates', 'haunted', 'have', 'haven', 'having', 'hawaii', 'he', 'head', 'headache', 'headband', 'headed', 'headlights', 'heads', 'healthy', 'hear', 'heard', 'hearing', 'hears', 'heart', 'heavy', 'hegemony', 'height', 'heights', 'held', 'hello', 'helmet', 'help', 'helping', 'helps', 'her', 'here', 'herself', 'hey', 'hi', 'hid', 'hidden', 'hierarchy', 'high', 'highly', 'highway', 'hill', 'hills', 'him', 'himself', 'his', 'history', 'hit', 'hitchhiking', 'hits', 'hitting', 'hmm', 'hold', 'holding', 'holds', 'holes', 'home', 'homes', 'homework', 'honestly', 'honor', 'hooked', 'hope', 'hoped', 'hoping', 'horrible', 'horrified', 'horror', 'horse', 'horses', 'hospital', 'hospitals', 'host', 'hosted', 'hosting', 'hosts', 'hot', 'hotel', 'hotter', 'hour', 'hours', 'house', 'how', 'howard', 'however', 'huge', 'humid', 'humm', 'humming', 'humor', 'hundred', 'hung', 'hungry', 'hurricane', 'hurried', 'hurry', 'hurts', 'husband', 'husky', 'i', 'ibm', 'ice', 'idea', 'ideas', 'identified', 'identify', 'identities', 'identity', 'idiot', 'if', 'ignored', 'ill', 'illinois', 'illness', 'imagination', 'imagine', 'imagined', 'imagines', 'immediately', 'impacted', 'impatient', 'implant', 'implications', 'implicit', 'implies', 'impolite', 'important', 'impossible', 'improved', 'in', 'inches', 'include', 'included', 'including', 'incredulously', 'india', 'indiana', 'indicated', 'individual', 'individuals', 'infection', 'influence', 'influences', 'inform', 'information', 'informed', 'informing', 'informs', 'ingore', 'inhale', 'inhaled', 'initial', 'injections', 'injured', 'innocent', 'ins', 'inside', 'inspiring', 'instances', 'instead', 'institute', 'institutions', 'instructions', 'insurance', 'intelligence', 'intelligent', 'intelligible', 'intense', 'interested', 'interesting', 'international', 'internet', 'interpret', 'interpretated', 'interpretation', 'interpretations', 'interpreted', 'interpreter', 'interpreting', 'interprets', 'intersection', 'interstate', 'intervene', 'intervening', 'intervention', 'interview', 'interviewed', 'intimidating', 'into', 'introduce', 'introduced', 'introducing', 'introduction', 'invention', 'invitation', 'invite', 'invited', 'involve', 'involved', 'involvement', 'involves', 'involving', 'iowa', 'iowans', 'ipods', 'irish', 'ironic', 'irritating', 'is', 'isn', 'isnt', 'issues', 'it', 'italian', 'items', 'its', 'itself', 'jack', 'jail', 'jamaica', 'jana', 'jane', 'janet', 'jason', 'jen', 'jersey', 'jessica', 'jim', 'job', 'joe', 'john', 'join', 'joined', 'joining', 'joint', 'journal', 'journey', 'judge', 'judges', 'judging', 'judgment', 'judgments', 'juice', 'july', 'jumping', 'jumps', 'junior', 'just', 'k', 'ked', 'keep', 'keeps', 'kept', 'kernels', 'key', 'keys', 'kick', 'kicked', 'kicks', 'kid', 'kids', 'killed', 'kind', 'kinds', 'king', 'kitchen', 'knee', 'knew', 'know', 'knowing', 'known', 'knows', 'kudos', 'l', 'la', 'laboriously', 'lack', 'laid', 'lake', 'landed', 'landscaping', 'lane', 'language', 'languages', 'lapd', 'laptop', 'laptops', 'last', 'lasts', 'late', 'lately', 'later', 'laugh', 'laughed', 'laughing', 'laughter', 'lawn', 'lay', 'lays', 'lazy', 'lead', 'leader', 'learn', 'learned', 'learning', 'learns', 'least', 'leave', 'leaves', 'leaving', 'lecture', 'lecturing', 'led', 'left', 'leg', 'legal', 'less', 'lesson', 'let', 'letter', 'letters', 'letting', 'level', 'library', 'license', 'lid', 'lie', 'lied', 'lies', 'life', 'lifting', 'light', 'lightning', 'lights', 'like', 'liked', 'likes', 'liking', 'limit', 'limited', 'limp', 'lincoln', 'line', 'linguistics', 'link', 'lips', 'list', 'listen', 'listened', 'listens', 'lists', 'lit', 'literally', 'literature', 'littered', 'little', 'live', 'lived', 'lives', 'living', 'liz', 'll', 'loaf', 'local', 'located', 'location', 'locations', 'locked', 'locker', 'locks', 'log', 'logs', 'lombardi', 'lonely', 'long', 'longer', 'look', 'looked', 'looking', 'looks', 'loop', 'los', 'lose', 'losing', 'lost', 'lot', 'lotion', 'lots', 'lottery', 'loud', 'louder', 'loudly', 'lound', 'lousy', 'love', 'loves', 'lowered', 'lox', 'luck', 'lucky', 'lunch', 'lurches', 'lying', 'm', 'mac', 'machine', 'mad', 'made', 'magazine', 'magazines', 'mail', 'maine', 'maintained', 'make', 'makers', 'makes', 'making', 'mall', 'man', 'managed', 'manager', 'manila', 'many', 'maority', 'marathon', 'march', 'market', 'markets', 'married', 'mary', 'match', 'matches', 'math', 'matter', 'may', 'maybe', 'mayor', 'mc', 'mcdonald', 'mcdonalds', 'me', 'mean', 'meaning', 'meanings', 'means', 'meant', 'meat', 'mechanic', 'medication', 'medium', 'meet', 'meeting', 'meetings', 'member', 'memorizing', 'men', 'mentioned', 'mentor', 'menu', 'message', 'messed', 'messing', 'met', 'metal', 'meter', 'mexican', 'mexico', 'miami', 'mid', 'middle', 'might', 'mike', 'miles', 'milk', 'millions', 'mind', 'minded', 'mine', 'mingling', 'minnesota', 'minute', 'minutes', 'mischevious', 'misinterpretation', 'misinterpreted', 'miss', 'missed', 'missing', 'mistake', 'misunderstandings', 'misunderstood', 'mitt', 'mix', 'mixes', 'mmmmmm', 'mmmmmmm', 'model', 'mom', 'moment', 'monday', 'money', 'monitor', 'month', 'months', 'mood', 'moore', 'more', 'mormon', 'morning', 'most', 'mostly', 'mother', 'motivate', 'motivating', 'motor', 'motorcycle', 'motorcycles', 'motorcycling', 'mouse', 'mouth', 'mouths', 'move', 'moved', 'movie', 'movies', 'moving', 'mow', 'mowing', 'mph', 'mr', 'much', 'muffle', 'muhammad', 'muscles', 'music', 'must', 'mute', 'my', 'myself', 'n', 'nadal', 'nagivate', 'nah', 'nailed', 'nails', 'naked', 'name', 'named', 'names', 'nasal', 'natural', 'nauseated', 'nauseous', 'near', 'nearby', 'nearly', 'nebraska', 'necessarily', 'necessary', 'necessity', 'need', 'needed', 'needle', 'needs', 'negative', 'negatively', 'neglect', 'neglects', 'neighbor', 'neighbors', 'neither', 'nervous', 'netflix', 'nevada', 'never', 'new', 'news', 'newton', 'next', 'nice', 'night', 'nights', 'nightstick', 'nighttime', 'no', 'nobody', 'noise', 'noises', 'noisy', 'none', 'nope', 'norm', 'normal', 'normally', 'norms', 'north', 'northridge', 'not', 'notes', 'nothing', 'notice', 'noticed', 'notices', 'now', 'nowadays', 'ntid', 'nudged', 'numb', 'number', 'nurse', 'nyc', 'o', 'oakland', 'obama', 'obey', 'obeyed', 'objects', 'obliviously', 'observe', 'observed', 'obviously', 'occasion', 'occasionally', 'occurences', 'ocean', 'of', 'off', 'offense', 'offensive', 'offer', 'office', 'officers', 'often', 'oftentimes', 'oh', 'ohh', 'ohio', 'oil', 'oj', 'ok', 'okay', 'old', 'older', 'omaha', 'on', 'once', 'one', 'ones', 'online', 'only', 'onto', 'oozing', 'open', 'opened', 'opening', 'opens', 'opinion', 'opportunities', 'opportunity', 'options', 'or', 'oral', 'oralism', 'orange', 'order', 'orders', 'organizations', 'orientation', 'orientations', 'oriented', 'other', 'others', 'otherwise', 'our', 'out', 'outgoing', 'outing', 'outside', 'outta', 'over', 'overall', 'overnight', 'own', 'owns', 'pace', 'paddle', 'page', 'pageant', 'pager', 'pages', 'paid', 'pain', 'painting', 'paintings', 'pale', 'palin', 'panic', 'pans', 'pants', 'paper', 'papers', 'parents', 'park', 'parked', 'parker', 'parkf', 'parkinson', 'part', 'party', 'pass', 'passed', 'passes', 'past', 'pasta', 'path', 'pathway', 'patient', 'paul', 'pay', 'paying', 'pc', 'peanut', 'peanuts', 'pee', 'peed', 'peeing', 'peeked', 'peel', 'peeled', 'peeves', 'pen', 'people', 'perceive', 'percent', 'perfect', 'perfectly', 'perform', 'perhaps', 'permission', 'permitted', 'perpendicular', 'perry', 'person', 'persona', 'personal', 'personality', 'personally', 'perspective', 'persuit', 'pet', 'pete', 'petrone', 'phil', 'philadelphia', 'phone', 'phrase', 'physics', 'pick', 'picked', 'picking', 'picks', 'picture', 'pie', 'pieces', 'pies', 'pig', 'pigs', 'pile', 'pill', 'pineapple', 'piss', 'pissing', 'pitch', 'pitched', 'pizza', 'place', 'placed', 'places', 'plaid', 'plain', 'plan', 'plane', 'planned', 'planning', 'plans', 'plants', 'plastic', 'plate', 'play', 'played', 'player', 'players', 'playing', 'plays', 'please', 'pleased', 'plus', 'pm', 'pocket', 'pocketed', 'point', 'pointed', 'points', 'police', 'policies', 'policy', 'polite', 'politeness', 'politics', 'pond', 'pop', 'popcorn', 'popped', 'popping', 'positive', 'possible', 'possiblility', 'post', 'postcard', 'poster', 'postpone', 'potatoes', 'pots', 'potter', 'pounding', 'pounds', 'poured', 'pours', 'power', 'powerful', 'practically', 'praise', 'praised', 'precisely', 'prefer', 'preferred', 'prefers', 'pregnant', 'prepared', 'presence', 'presented', 'presenter', 'presenting', 'president', 'pressure', 'presumption', 'pretty', 'price', 'pries', 'primary', 'prince', 'principal', 'print', 'pro', 'probably', 'problem', 'problems', 'proceeded', 'process', 'proctor', 'products', 'professional', 'professionals', 'professor', 'proficient', 'profusely', 'progress', 'project', 'properly', 'properties', 'proven', 'provides', 'pry', 'public', 'puerto', 'pull', 'pulled', 'pulls', 'pump', 'pumpkin', 'punched', 'punches', 'punching', 'punished', 'punishment', 'puppet', 'puppies', 'purple', 'pushed', 'put', 'puts', 'putting', 'quality', 'quarterback', 'quesiton', 'question', 'questions', 'quick', 'quicker', 'quickly', 'quiet', 'quieter', 'quietly', 'quit', 'quiz', 'quote', 'r', 'ra', 'race', 'raft', 'rafting', 'raiders', 'rain', 'rained', 'raining', 'rains', 'rainy', 'raised', 'ran', 'rang', 'range', 'ranges', 'ranking', 'rapids', 'ras', 'rather', 'ray', 're', 'reached', 'reaches', 'react', 'read', 'reading', 'reads', 'ready', 'real', 'reality', 'realize', 'realized', 'realizing', 'reallhy', 'really', 'rear', 'reason', 'reasonaly', 'reassures', 'received', 'receiver', 'recently', 'recognize', 'recommend', 'record', 'records', 'red', 'reduce', 'refer', 'references', 'referred', 'reflects', 'refuse', 'refused', 'refuses', 'regarded', 'regathered', 'regret', 'regular', 'regularly', 'related', 'relates', 'relation', 'relationship', 'relaxed', 'released', 'relief', 'relieved', 'rely', 'remark', 'remarks', 'remember', 'remembering', 'remembers', 'remind', 'reminds', 'remove', 'removes', 'rent', 'rented', 'reopen', 'repairs', 'repeatedly', 'replaces', 'replacing', 'reply', 'report', 'reported', 'reporter', 'reporters', 'republican', 'require', 'required', 'requires', 'research', 'researches', 'reservation', 'residence', 'residences', 'resonance', 'resources', 'respect', 'responsible', 'restauarnt', 'restaurant', 'restaurants', 'resteraunt', 'result', 'resulted', 'results', 'resume', 'retire', 'return', 'ribs', 'rick', 'rico', 'ride', 'riding', 'right', 'ring', 'rip', 'ripped', 'rise', 'rises', 'risk', 'rit', 'river', 'road', 'rochester', 'rock', 'rodney', 'role', 'rollerbladed', 'rollerblades', 'romney', 'roof', 'room', 'roommate', 'roommates', 'rope', 'rose', 'roses', 'route', 'row', 'rowdy', 'rowing', 'rsd', 'rubber', 'rude', 'rules', 'rummaged', 'run', 'running', 'runs', 'rushes', 'rustle', 'rustling', 'ruthless', 's', 'sack', 'sad', 'safe', 'safely', 'said', 'sale', 'sally', 'same', 'san', 'sanctuary', 'sandwich', 'sank', 'sanucatory', 'sarah', 'sarcastically', 'sat', 'save', 'saw', 'say', 'saying', 'says', 'scare', 'scared', 'scary', 'scheduled', 'school', 'schooled', 'scissors', 'score', 'scout', 'scratched', 'scratching', 'scream', 'screamed', 'screaming', 'screening', 'seafood', 'seal', 'seasons', 'seat', 'seats', 'seattle', 'second', 'security', 'see', 'seeing', 'seem', 'seemed', 'seems', 'seen', 'seles', 'sell', 'semester', 'send', 'sends', 'senior', 'sense', 'sensed', 'senses', 'sensory', 'sent', 'sentence', 'separated', 'separating', 'september', 'serious', 'serve', 'service', 'services', 'serving', 'session', 'set', 'sets', 'setting', 'settled', 'seven', 'several', 'sewer', 'sexual', 'sf', 'shake', 'shaken', 'shakes', 'shaking', 'shampoo', 'share', 'shared', 'shares', 'sharing', 'sharp', 'sharpova', 'she', 'sheep', 'shh', 'shines', 'shining', 'shirt', 'shirts', 'shitfaced', 'shock', 'shocked', 'shoes', 'shone', 'shook', 'shoots', 'shop', 'shopping', 'shore', 'short', 'shorts', 'shot', 'should', 'shoulder', 'shouldn', 'show', 'showed', 'shower', 'shown', 'shows', 'shrit', 'shut', 'siberian', 'sick', 'sickness', 'side', 'sigh', 'sign', 'signed', 'signer', 'signing', 'signs', 'silber', 'silent', 'silk', 'silly', 'similar', 'similarly', 'simple', 'simply', 'since', 'siren', 'sister', 'sit', 'sites', 'sitting', 'situation', 'situations', 'six', 'size', 'sized', 'sizes', 'ski', 'skidding', 'skiing', 'skills', 'slammed', 'slaughtering', 'sleep', 'sleeping', 'sleeps', 'sleeves', 'slept', 'sliced', 'slide', 'slings', 'slow', 'slower', 'slowly', 'slumped', 'small', 'smaller', 'smart', 'smashed', 'smell', 'smirk', 'smoking', 'smooth', 'smoothly', 'snatch', 'sneak', 'snobby', 'snow', 'snowing', 'snows', 'snuck', 'so', 'social', 'socialize', 'socialized', 'socially', 'sold', 'some', 'somebody', 'someone', 'something', 'sometimes', 'somewhere', 'son', 'song', 'soon', 'sore', 'sorry', 'sort', 'sound', 'sounded', 'sounds', 'soup', 'south', 'spa', 'span', 'spare', 'speak', 'speakers', 'speaking', 'speaks', 'special', 'specialist', 'specialists', 'specific', 'spectrum', 'spedometer', 'speech', 'speed', 'spell', 'spend', 'spent', 'spinach', 'spit', 'spoiled', 'spoke', 'sport', 'sports', 'spots', 'sprained', 'spray', 'spring', 'spun', 'st', 'staff', 'stall', 'stamps', 'stand', 'standing', 'stare', 'stared', 'staring', 'start', 'started', 'starting', 'starts', 'state', 'states', 'station', 'status', 'stay', 'stayed', 'staying', 'stays', 'steak', 'steam', 'stems', 'stewart', 'sticky', 'stiill', 'still', 'stitched', 'stitches', 'stitching', 'stole', 'stolen', 'stomach', 'stood', 'stop', 'stopped', 'stopping', 'stops', 'store', 'stories', 'storm', 'story', 'straight', 'strange', 'strangers', 'strategy', 'stratiy', 'strict', 'strikes', 'string', 'strip', 'striped', 'stripes', 'strong', 'struck', 'struggled', 'struggles', 'struggling', 'stuck', 'student', 'students', 'studies', 'study', 'studying', 'styrofoam', 'subsequent', 'subtelny', 'succeed', 'succeeds', 'succesfully', 'successful', 'such', 'sudden', 'suddenly', 'sue', 'suggest', 'suiting', 'summer', 'summons', 'sun', 'sunbathe', 'sunbathing', 'sunburn', 'sunday', 'sundown', 'sunny', 'sunrise', 'sunscreen', 'sunset', 'suntan', 'super', 'superior', 'superiority', 'support', 'supporting', 'suppose', 'supposed', 'supreme', 'sure', 'surf', 'surface', 'surfing', 'surgeon', 'surprise', 'surprised', 'surprises', 'survive', 'sushi', 'suspected', 'swam', 'swashbuckler', 'sweetheart', 'swerved', 'swim', 'swimming', 'swinging', 'switch', 'switched', 'switching', 'syntax', 'system', 'systems', 't', 'table', 'tables', 'taboo', 'tactic', 'tactile', 'tag', 'tail', 'take', 'takes', 'taking', 'talk', 'talked', 'talking', 'talks', 'tall', 'tan', 'tank', 'tape', 'tapes', 'tapped', 'tappee', 'tapper', 'taps', 'tasks', 'taste', 'tasted', 'tastes', 'taught', 'tea', 'teach', 'teacher', 'teachers', 'team', 'teams', 'tease', 'teasing', 'technology', 'teddy', 'television', 'tell', 'telling', 'tells', 'tempo', 'tempted', 'ten', 'tend', 'tended', 'tendency', 'tends', 'tennessee', 'tennis', 'tense', 'tents', 'term', 'terminal', 'terrible', 'terribly', 'test', 'testimonies', 'texas', 'text', 'texting', 'than', 'thank', 'thanking', 'thanks', 'that', 'thats', 'the', 'theater', 'theatre', 'their', 'them', 'themselves', 'then', 'there', 'therefore', 'these', 'they', 'thick', 'thief', 'thing', 'things', 'think', 'thinking', 'thinks', 'third', 'thirdly', 'thirteen', 'thirty', 'this', 'those', 'though', 'thought', 'thousand', 'thousands', 'threatened', 'threatening', 'three', 'threw', 'thrilled', 'throat', 'through', 'throughout', 'throw', 'thrown', 'throws', 'thrusts', 'thursday', 'ticked', 'ticket', 'tickets', 'tie', 'tied', 'tigers', 'tight', 'til', 'time', 'times', 'tiny', 'tire', 'tired', 'tires', 'to', 'today', 'together', 'toilet', 'told', 'tomorrow', 'tone', 'tonight', 'tons', 'tony', 'too', 'took', 'tool', 'top', 'topic', 'toppings', 'torn', 'tossed', 'total', 'touch', 'touchdown', 'tough', 'tougher', 'toward', 'towards', 'toys', 'tracking', 'trader', 'tradition', 'traffic', 'train', 'trained', 'transfer', 'transferred', 'travel', 'traveled', 'traveling', 'travelling', 'travels', 'tray', 'treating', 'tree', 'trees', 'trickiling', 'tried', 'trigger', 'trip', 'trouble', 'truck', 'true', 'trunk', 'trust', 'truth', 'try', 'trying', 'tubing', 'turn', 'turned', 'turning', 'turns', 'tv', 'twelve', 'twenty', 'twice', 'two', 'types', 'typical', 'tyranny', 'u', 'ugh', 'ugly', 'uh', 'umbrella', 'umbrellas', 'uncertain', 'uncle', 'uncontrollable', 'under', 'underage', 'underlying', 'understand', 'understood', 'undoubtedly', 'uneasy', 'unfinished', 'unfortunately', 'uniform', 'university', 'unless', 'unpredictable', 'unsatified', 'until', 'untill', 'up', 'upon', 'upset', 'upstairs', 'urinate', 'urinated', 'urinating', 'us', 'usage', 'use', 'used', 'uses', 'using', 'usually', 'utah', 'utter', 'utterly', 'vacation', 'vacations', 'value', 'variations', 'varies', 'variety', 'various', 'vary', 've', 'vegetable', 'vegetables', 'vegetarian', 'ventriloquism', 'vermont', 'very', 'via', 'vibrating', 'victim', 'video', 'videogame', 'videophoning', 'videotape', 'view', 'viewed', 'viewing', 'vince', 'violently', 'vision', 'visit', 'visited', 'visiting', 'visits', 'vists', 'visual', 'visualize', 'visualized', 'vocal', 'vocalization', 'vocalizations', 'vocals', 'voice', 'voices', 'volunteer', 'vomiting', 'vomitting', 'vote', 'voucher', 'vp', 'wait', 'waited', 'waiters', 'waiting', 'waitress', 'waive', 'wake', 'walk', 'walked', 'walking', 'walks', 'wallet', 'want', 'wanted', 'wants', 'wards', 'warm', 'warmly', 'warn', 'warned', 'warning', 'warnings', 'was', 'wash', 'washed', 'wasn', 'wasnt', 'waste', 'wasting', 'watch', 'watched', 'watches', 'watching', 'water', 'wave', 'waved', 'waves', 'waving', 'way', 'ways', 'we', 'wear', 'wearing', 'wears', 'weather', 'web', 'websites', 'wedding', 'week', 'weekend', 'weekends', 'weigh', 'weight', 'weights', 'weird', 'welcome', 'welcoming', 'well', 'went', 'were', 'weren', 'wet', 'what', 'whatever', 'wheel', 'when', 'whenever', 'where', 'whereas', 'whether', 'which', 'whichever', 'while', 'whipping', 'white', 'who', 'whoa', 'whoever', 'whole', 'whom', 'whose', 'why', 'wide', 'wife', 'will', 'willing', 'willis', 'win', 'winding', 'window', 'windows', 'winner', 'winning', 'wins', 'winter', 'wintertime', 'wish', 'with', 'within', 'without', 'witty', 'wnt', 'woke', 'woken', 'wolf', 'woman', 'won', 'wonder', 'wondered', 'wonderful', 'wondering', 'wood', 'word', 'words', 'work', 'worked', 'worker', 'working', 'works', 'world', 'worried', 'worry', 'worse', 'worst', 'worth', 'would', 'wouldn', 'wow', 'wrapped', 'wrestle', 'wrestling', 'write', 'writing', 'written', 'wrong', 'wrote', 'wymoing', 'wymoning', 'wyoming', 'x', 'yards', 'yawned', 'year', 'years', 'yell', 'yelled', 'yelling', 'yes', 'yesterday', 'yet', 'york', 'you', 'young', 'your', 'yourself', 'yuck', 'zipped', 'zombie', 'zone', '—', '“', '”', '…']\n",
      "asl_tokens ['#', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'A-LEVEL-ABOVE', 'A-LEVEL-BELOW', 'A-LOT', 'A-OK', 'A-WAYS', 'AA', 'AAA', 'ABANDON', 'ABC', 'ABOUT', 'ABOVE', 'ABOVE_3', 'ABUSE', 'AC', 'ACCEPT', 'ACCIDENT', 'ACCOMMODATE', 'ACROSS', 'ACT', 'ACT+AGENT', 'ACTION', 'ADD-TO', 'ADDICTED', 'ADMIT', 'ADULT-TALL', 'ADVANTAGE', 'ADVENTURE', 'ADVISE', 'ADVISE/INFLUENCE', 'ADVISE/INFLUENCE+AGENT', 'ADVISER', 'AFRAID', 'AFTER', 'AFTERNOON_3', 'AGAIN', 'AGAINST', 'AGE', 'AGE-SIX+HALF', 'AGE-THIRTEEN', 'AGE-TWENTY-ONE', 'AGE-TWENTY-ONE_2', 'AGENT', 'AGREE', 'AIR', 'AIRPLANE', 'ALARM', 'ALCOHOL', 'ALEC-BALDWIN', 'ALI', 'ALL', 'ALL-DAY', 'ALL-GONE', 'ALL-NIGHT', 'ALL-NIGHT_3', 'ALL-THE-WAY', 'ALL-YEARS-HS', 'ALLERGY', 'ALLOW', 'ALL_2', 'ALMOST', 'ALONE', 'ALRIGHT', 'ALSO', 'ALWAYS', 'AMERICAN-AIRLINES', 'AMONG', 'AMONG_2', 'AMY', 'ANALYZE', 'ANALYZE_2', 'ANALYZE_3', 'AND', 'ANGELA', 'ANGRY', 'ANIMAL', 'ANKLE', 'ANN', 'ANNOUNCE', 'ANSWER', 'ANSWER+AGENT', 'ANY', 'ANY+MORE', 'ANY+ONE', 'ANY+THING', 'ANY+TIME', 'ANY+WHERE', 'ANYWAY/NOT-MATTER', 'APPEAR', 'APPLAUSE', 'APPLE', 'APPLY', 'APPLY_2', 'APPOINTMENT', 'APPROACH', 'APT', 'AREA', 'ARGUE', 'AROUND', 'ARREST', 'ARRIVE', 'ARTICLE', 'ASIA', 'ASK', 'ASK_2', 'ASL', 'ASSAULT', 'ASSISTANT', 'AT', 'ATTEMPT', 'ATTENTION-WAVE', 'AUDIENCE', 'AUDIO', 'AUDIOLOGY+AGENT', 'AUDIOVOCAL', 'AUNT', 'AUSTRALIA', 'AUTUMN', 'AVERAGE', 'AVERAGE/RANGE', 'AWFUL', 'AWKWARD', 'B', 'B-L', 'BABY', 'BACK', 'BACK-TO-TOPIC', 'BACKWARD_2', 'BACK_2', 'BAD', 'BAG', 'BAGEL', 'BAGS', 'BAKE/OVEN', 'BAKING-SPRINKLES', 'BALANCE', 'BALD', 'BALD_2', 'BALL', 'BANDAGE', 'BANKS', 'BARACK-OBAMA', 'BARBARIC', 'BARITONE', 'BASEBALL', 'BASKETBALL', 'BATHROOM', 'BAUMAN', 'BAWL-OUT', 'BBQ', 'BCL', 'BE', 'BEACH', 'BEAR', 'BEAT', 'BEAT_2', 'BEAUTIFUL', 'BECAUSE', 'BECOME', 'BED', 'BEER', 'BEFORE', 'BEFORE_2', 'BEFORE_3', 'BEHAVIOR', 'BEHIND', 'BEING', 'BELIEVE', 'BELONG', 'BELOW_2', 'BELT', 'BEN', 'BERNARD', 'BERRY', 'BEST', 'BEST-FRIEND', 'BETH', 'BETTER', 'BETWEEN', 'BETWEEN/SHARE', 'BEVERLY', 'BIB', 'BICYCLE', 'BIG', 'BIGGER', 'BIG_2', 'BILL', 'BILL/OWE', 'BIRD', 'BIRTHDAY', 'BIRTHDAY_3', 'BLACK', 'BLADDER', 'BLAME', 'BLANK-OUT', 'BLOCK', 'BLOOD', 'BLOW', 'BLOWN-AWAY', 'BLUE', 'BLUE+BERRY', 'BLUE+fs-BERRY', 'BOAT', 'BOAT_2', 'BOB', 'BODY', 'BOMB', 'BONE', 'BONFIRE', 'BOOK', 'BOOK-PAGE', 'BOOK_2', 'BOOT', 'BORDER', 'BORE', 'BORN', 'BORROW', 'BOSS', 'BOSTON', 'BOSTON+PERSON', 'BOTH', 'BOTHER', 'BOX', 'BOX/ROOM', 'BOXER', 'BOXING', 'BOY', 'BOY+SCOUT', 'BPCL', 'BQQ', 'BRACELET', 'BRAGG', 'BRAKE', 'BRAVE/RECOVER', 'BREAD', 'BREAK', 'BREAK+ENTER', 'BREAK-DOWN', 'BREAKFAST', 'BREATHE', 'BRICK', 'BRIDE', 'BRING', 'BROCCOLI', 'BROOKLINE', 'BROTHER', 'BROWN', 'BRUCE-WILLIS', 'BU', 'BUBBLES', 'BUCKET', 'BUILD', 'BUILDING', 'BUOY', 'BUOY_2', 'BUOY_7', 'BURN', 'BURP', 'BURST', 'BUS', 'BUSH', 'BUSINESS', 'BUSINESS_2', 'BUSTED', 'BUT', 'BUTCHER-SHOP', 'BUTTER', 'BUTTER_2', 'BUTTS', 'BUY', 'BY', 'BY-A-HAIR', 'C', 'CABIN', 'CAFETERIA', 'CAKES', 'CALIFORNIA', 'CALIFORNIA_2', 'CALL', 'CALL-BY-PHONE', 'CALL-ON-RELAY/VRS', 'CAMP', 'CAN', 'CANCEL/CRITICIZE', 'CANCER', 'CANDY', 'CANNOT', 'CAPE-COD', 'CAPECOD', 'CAR', 'CAR+AGENT', 'CARD', 'CARE', 'CAREER', 'CARE_2', 'CAROL', 'CARRY', 'CASINO', 'CAT', 'CAUCUS', 'CAUSE', 'CAVIAR', 'CD', 'CELEBRATE', 'CENTER', 'CEREAL', 'CHAIR', 'CHALLENGE', 'CHANGE', 'CHANNEL', 'CHAPTER', 'CHARACTER', 'CHARLES', 'CHASE', 'CHAT', 'CHEAP', 'CHEAP+MOST', 'CHECK', 'CHECK+fs-LIST', 'CHEER', 'CHEESE', 'CHEMICAL', 'CHESS', 'CHICAGO', 'CHICKEN', 'CHILD', 'CHILDREN', 'CHINA', 'CHOCOLATE', 'CHOICE', 'CHOOSE', 'CHOP', 'CHRISTMAS', 'CHROME', 'CHURCH', 'CIGARETTE', 'CITY', 'CITY/COMMUNITY', 'CITY/COMMUNITY_3', 'CL', 'CLASS', 'CLASS_2', 'CLASS_2+BOX/ROOM', 'CLEAN', 'CLEAR', 'CLEVER', 'CLICK', 'CLIMB', 'CLOSE', 'CLOSE-BOOK', 'CLOSE-CALL', 'CLOSE-DOOR', 'CLOSE-GATE', 'CLUB', 'CLUE', 'CNN', 'COACH', 'COAT', 'COCHLEAR-IMPLANT', 'CODA', 'CODE', 'COFFEE', 'COFFIN', 'COG', 'COLD', 'COLLAR', 'COLLARS', 'COLLEAGUE', 'COLLECT', 'COLLEGE', 'COLLIDED', 'COLOR', 'COLORADO', 'COME', 'COME-ON', 'COMEDY', 'COMFORTABLE', 'COMMUNICATE', 'COMMUTE', 'COMPANY', 'COMPARE', 'COMPETITION', 'COMPUTER', 'COMPUTER_2', 'COMPUTER_5', 'CONCEPT', 'CONDUCT', 'CONFIDENT', 'CONFLICT/INTERSECTION', 'CONFRONT', 'CONFUSE', 'CONTACT', 'CONTINUE', 'CONTRO', 'COOK', 'COOK/KITCHEN', 'COOKIE', 'COOL', 'COOLER', 'COOPERATE/UNITE', 'COORDINATOR', 'COP', 'COPY', 'CORN', 'CORNER', 'CORRECT', 'COUGH', 'COUNT-ON-FINGERS', 'COUNTRY', 'COURSE', 'COURT', 'COUSIN', 'COW', 'CPR', 'CRACK', 'CRASH', 'CRASHED', 'CRASH_2', 'CRAZY', 'CREAM', 'CREAM+CHEESE', 'CREAM+fs-CHEESE', 'CREATE/PRETEND', 'CREEPY', 'CRIB', 'CRISIS', 'CROPS', 'CROWDED', 'CRY', 'CSUN', 'CUED-SPEECH', 'CULTURE', 'CUP', 'CURIOUS', 'CUT', 'CUTE', 'CUTTER', 'D', 'DAISY', 'DANA', 'DANCE', 'DANGER', 'DARK', 'DATE', 'DATE/DESSERT', 'DAUGHTER', 'DAY', 'DCL', 'DEAD', 'DEAF', 'DEAF-APPLAUSE', 'DEAF_2', 'DEAN', 'DEC', 'DECIDE', 'DECLINE_2', 'DECORATE', 'DEEP', 'DEER', 'DEGREE', 'DELETE', 'DELETE_3', 'DELIBERATE', 'DELICIOUS', 'DEMI-MOORE', 'DEMOCRAT', 'DEPART', 'DEPRESS', 'DESTROY', 'DEVELOP', 'DEVIL', 'DIAGNOSIS', 'DIALOG', 'DIE', 'DIFFERENT', 'DIMWITTED', 'DINNER', 'DIRECT', 'DIRECT/EXPLAIN', 'DISAPPOINT', 'DISCIPLINE', 'DISCONNECT', 'DISCUSS', 'DISCUSS-INTENSELY', 'DISEASE', 'DISGUST', 'DISH', 'DISORDER', 'DISORDERED', 'DISTRIBUTE', 'DO', 'DO-DO', 'DO-IT', 'DOCTOR', 'DOG', 'DOG_2', 'DOING', 'DOLLAR', 'DOLLAR_2', 'DONNA', 'DONUT', 'DOOR', 'DORM', 'DOUBT', 'DOWN', 'DOWN-THE-LIST', 'DOWNHILL', 'DRAMA', 'DREAM', 'DRESS/CLOTHES', 'DRINK', 'DRIVE', 'DROP', 'DROWN', 'DRUG', 'DRUGS', 'DRUNK', 'DRY', 'DRY-CEREAL', 'DURING/WHILE', 'DVD', 'DYE', 'E', 'EACH', 'EACH+ONE', 'EAR', 'EARLY', 'EARLY_2', 'EASIER', 'EASY', 'EASY+MOST', 'EAT', 'EAT+MORNING', 'EAT+NIGHT', 'EAT+NOON', 'EAT-UP', 'ECONOMIC', 'ED', 'EIGHT', 'EIGHTY', 'ELECTION', 'ELEMENTARY', 'ELEVEN', 'ELITE', 'EMAIL', 'EMBARRASS', 'EMERGENCY', 'EMIT', 'EMOTION', 'EMOTION_2', 'EMPTY', 'EMPTY-FROM-NOW-ON', 'EMPTY/EARLY', 'EMPTY/EARLY+MOST', 'ENCOURAGE', 'END', 'ENGINE', 'ENGLISH', 'ENJOY', 'ENNIS', 'ENOUGH', 'ENTER', 'ENVIRONMENT', 'EQUAL', 'ERICK', 'ESCAPE', 'ETC.', 'EUROPE', 'EVALUATE', 'EVEN', 'EVER', 'EVERY+ONE', 'EVERY+THING', 'EVERY+WHERE', 'EVERY+fs-DAY', 'EVERY-MONTH/RENT', 'EVERY-MORNING', 'EVERY-YEAR', 'EVERYDAY', 'EX', 'EXACT', 'EXAM', 'EXAMPLE', 'EXCESS', 'EXCHANGE', 'EXCITE', 'EXCITED', 'EXCUSE', 'EXCUSE-GO', 'EXHAUST', 'EXPECT', 'EXPENSIVE', 'EXPENSIVE_2', 'EXPERIENCE', 'EXPERIMENT', 'EXPERT', 'EXPLAIN', 'EXPRESS', 'EXTEND', 'EXTENSIVE-KNOWLEDGE', 'EXTRA', 'EYES', 'FACE', 'FACE+SAME', 'FAIL', 'FALL', 'FALL-ASLEEP', 'FALL-IN-LOVE', 'FALL-INTO-IT', 'FALL-INTO-PLACE', 'FAMILY', 'FAMOUS', 'FAN', 'FANCY', 'FANS', 'FAR', 'FARM', 'FART', 'FASCINATED', 'FAST', 'FAST_2', 'FAST_3', 'FATHER', 'FAULT', 'FAVORITE/PREFER', 'FEDERATION', 'FEDEX', 'FEED', 'FEEL', 'FEET', 'FENCE', 'FEW', 'FEW/CLOSE', 'FF', 'FIELD', 'FIFTEEN', 'FIFTH-IN-LIST', 'FIGHT', 'FILL-OUT', 'FILM', 'FINALLY/SUCCEED', 'FINALLY_2', 'FIND', 'FIND/FIND-OUT', 'FIND/FIND-OUT+#OUT', 'FINE', 'FINE_2', 'FINGERS-CROSSED', 'FINISH', 'FIRE', 'FIRED', 'FIREWORKS', 'FIREWORKS_2', 'FIRST', 'FIRST-IN-LIST', 'FIRST-SECOND-THIRD-IN-LIST', 'FISH', 'FISHING', 'FIVE', 'FIVE+SIX_2', 'FIVE-MINUTE', 'FIX', 'FLA', 'FLAT', 'FLAT-TIRE_3', 'FLAT/FLOOR', 'FLAT/LEVEL', 'FLIP-FLOP', 'FLIP-FLOP_2', 'FLOOD', 'FLOOR', 'FLOW', 'FLOWER', 'FLUSH', 'FLY-BY-PLANE', 'FOCUS', 'FOCUS/NARROW', 'FOLLOW', 'FOOD', 'FOOT', 'FOOTBALL', 'FOR', 'FOR-FOR', 'FOREVER', 'FORGET', 'FORGET-IT', 'FORK', 'FORMERLY', 'FOUR', 'FOUR-DAY', 'FOUR-THIRTY', 'FOURTH', 'FOURTH-IN-LIST', 'FRANCE', 'FRANK', 'FRATERNITY', 'FREAK', 'FREE', 'FREQUENTLY', 'FRESH', 'FRESHMAN', 'FRIDAY', 'FRIEND', 'FRIENDLY', 'FRISBEE', 'FROM', 'FROM-NOW-ON', 'FRONT', 'FRUGAL', 'FRUIT', 'FS-ZONE', 'FUCK', 'FULL', 'FUMBLE', 'FUN', 'FUNNY', 'FUTURE', 'G', 'GALLAUDET', 'GAMBLE', 'GAME', 'GAMUT', 'GARAGE/SUBMARINE', 'GAS', 'GATE', 'GATHER', 'GAZPACHO', 'GENDER', 'GENERAL', 'GEORGE-BUSH', 'GEORGE-CLOONEY', 'GERMANY', 'GESTURE', 'GET', 'GET-IN', 'GET-IN-BED', 'GET-TICKET', 'GET-UP', 'GHOST', 'GIFT', 'GILBERT', 'GIRL', 'GIRL+CORRECT', 'GIRL+FRIEND', 'GIVE', 'GIVE-UP+fs-UP', 'GLASSES_2', 'GLITTER', 'GLOBE', 'GO', 'GO-AHEAD', 'GO-AWAY', 'GO-BY-BOAT', 'GO-OUT', 'GOAL', 'GOBBLE-UP', 'GOD', 'GOLD', 'GOLD/CALIFORNIA', 'GOLF', 'GONE', 'GOOD', 'GOOD+ENOUGH', 'GOOD/THANK-YOU', 'GOOD/THANK-YOU+NIGHT', 'GOOGLE', 'GO_2', 'GRAB', 'GRAB-CHANCE', 'GRAD', 'GRADUAL-IMPROVE', 'GRADUATE', 'GRANDFATHER', 'GRANDMOTHER', 'GRASS', 'GREAT', 'GREATECHWAECH', 'GREEN', 'GRIEF', 'GROOM', 'GROUND', 'GROUP', 'GROUP/TOGETHER', 'GROW', 'GROW-UP', 'GRUNT', 'GRUNTAL', 'GRUNTS', 'GUARANTEE', 'GUESS', 'GUEST', 'GUITAR', 'GYM', 'HABIT_2', 'HABIT_3', 'HAD-IT', 'HAIR', 'HAIR+DYE', 'HAIR_2+DYE', 'HALF', 'HALL', 'HAMBURGER', 'HANDLE', 'HANDSOME/HAWAII_2', 'HANG-UP-PHONE', 'HANGUPS', 'HAPPEN', 'HAPPY', 'HARD', 'HARRY', 'HATE', 'HAVE', 'HAVE-TO', 'HEAD', 'HEAD-COLD', 'HEAD-TRIP', 'HEADACHE', 'HEADBAND', 'HEADBAND_2', 'HEAR', 'HEAR+NOISE', 'HEAR+RUMBLE', 'HEAR/LISTEN', 'HEARING', 'HEARING+PERSON', 'HEART', 'HEAVY', 'HEGEMONY', 'HEIGHT', 'HEIGHT_2', 'HELMET', 'HELP', 'HER', 'HERE', 'HIDE', 'HIGH', 'HIGHWAY', 'HILL', 'HILLS', 'HISTORY', 'HIT', 'HITCH-HIKE', 'HIT_2', 'HOLD', 'HOME', 'HOME+WORK', 'HOMEWORK', 'HONEST', 'HONOR', 'HOPE', 'HORRIFIED', 'HORROR', 'HORSE', 'HOSPITAL', 'HOT', 'HOTEL', 'HOUR', 'HOURS', 'HOUSE', 'HOW', 'HOW-MANY', 'HOWARD', 'HP', 'HS', 'HUM', 'HUMAN', 'HUMID', 'HUMOR', 'HUNGRY', 'HUNGRY/WISH', 'HURRICANE', 'HURRY', 'HURT', 'HUSBAND', 'I', 'IBLE', 'IBM', 'ICE-CREAM', 'ICL', 'IDEA', 'IDENTIFIED', 'IDENTIFY', 'IDIOT', 'IF', 'IGNITION-ON', 'IGNORE', 'ILL', 'ILLEGAL', 'IMAGINE', 'IMPACT', 'IMPLICATION', 'IMPLY', 'IMPORTANT', 'IMPOSSIBLE', 'IN', 'INCLUDE/ALL-INCLUDED', 'INCLUDE/INVOLVE', 'INCREASE', 'IND', 'INDIA', 'INDIVIDUAL', 'INFORM', 'INFORM+SUBLIMINAL-INFLUENCE', 'INFORM-ALL', 'INFORMATION', 'INHALE', 'INJECT', 'INNOCENT', 'INSPIRE', 'INSTITUTE', 'INSURANCE/INFECTION', 'INTELLIGIBLE', 'INTERACT/COMMUNICATE', 'INTERCOURSE+fs-UP', 'INTERESTING', 'INTERNATIONAL', 'INTERNET', 'INTERPRET', 'INTERPRET+AGENT', 'INTERRUPT', 'INTERVENTION', 'INTERVIEW', 'INTIMIDATING', 'INTRODUCE', 'INVITE', 'INVITE/HIRE', 'INVOLVE', 'INVOLVEMENT', 'IOWA', 'IOWANS', 'IPOD', 'IRELAND', 'IRRITATING', 'IS', 'IT', 'ITALY', 'ITS', 'IX', 'J', 'JACK', 'JAIL', 'JAMAICA', 'JANA', 'JANE', 'JANET', 'JASON', 'JEN', 'JESSICA', 'JIM', 'JOB', 'JOE', 'JOHN', 'JOHN-STEWART', 'JOIN', 'JOINT', 'JOURNEY', 'JUICE', 'JULY', 'JUMP', 'JUNIOR_3', 'K', 'KEEP', 'KERNEL', 'KEY', 'KICK-OFF', 'KICK-OUT', 'KID', 'KILL', 'KIND', 'KING', 'KISS-FIST', 'KITCHEN', 'KNEE', 'KNOW', 'KNOW+NEG', 'KNOW-NOTHING', 'KNOW-THAT', 'KOWTOW', 'L', 'L-X', 'LA', 'LAKE', 'LANDSCAPE', 'LANDSCAPING', 'LANGUAGE', 'LAPD', 'LAPTOP', 'LAST', 'LAST-WEEK', 'LAST-YEAR', 'LATE', 'LATER', 'LATER_2', 'LAUGH', 'LAUGH-HARD', 'LAZY', 'LCL', 'LEAD', 'LEADER', 'LEAF', 'LEARN', 'LEARN+AGENT', 'LEAVE', 'LEAVE-THERE', 'LECTURE', 'LEG', 'LEGAL/LAW', 'LEND', 'LESS', 'LESS-THAN', 'LETTER/MAIL', 'LETTER/MAIL_2', 'LIBRARY', 'LICENSE', 'LIE', 'LIE-DOWN', 'LIFE', 'LIGHT', 'LIGHT-FLASH', 'LIGHT-MATCH', 'LIGHTNING', 'LIKE', 'LIMIT', 'LINCOLN', 'LINE', 'LINGUISTICS', 'LINK', 'LIP', 'LIST', 'LIST-NAMES', 'LIST-ORDER', 'LIST-ORDER-IX', 'LIST-ORDER-MIDDLE', 'LIST-ORDER-PINKY', 'LIST-ORDER-RING', 'LIST-ORDER-THUMB', 'LIST-ORDER_2', 'LIST-ORDER_3', 'LISTEN', 'LIT', 'LITTLE-BIT', 'LIVE', 'LIZ', 'LOCAL', 'LOCK', 'LOCKER+BOX/ROOM', 'LOCKERS', 'LODI', 'LOG', 'LOGS', 'LOMBARDI', 'LOMBIDI', 'LOMDI', 'LONG', 'LONG+MOST', 'LONG-AGO', 'LONG-LIST', 'LONG-LIST_2', 'LONG-PAST', 'LONG-SLEEVE', 'LONG-TERM', 'LOOK', 'LOOK+SAME', 'LOOK-AROUND', 'LOOK-AT', 'LOOK-BACK', 'LOOK-BY-EYE', 'LOOK-DOWN', 'LOOK-FOR', 'LOOK-FORWARD', 'LOOK-LIKE', 'LOOK-OVER', 'LOOK-UP', 'LOOP', 'LOSE', 'LOSE-COMPETITION', 'LOSE-COMPETITION+PCL', 'LOTION', 'LOTTERY', 'LOUD', 'LOUSY', 'LOVE', 'LOW/LOWER', 'LOWERCASE/TINY', 'LOX', 'LUCK', 'LUCKY', 'LVINCE', 'MAC', 'MACHINE', 'MAD', 'MAGAZINE', 'MAIL', 'MAINE', 'MAJOR', 'MAKE', 'MAKE+REALLY', 'MAKE-IT', 'MAKER', 'MALE-COUSIN', 'MALL', 'MAN', 'MANAGE/CONTROL', 'MANAGE/CONTROL+AGENT', 'MANILA', 'MANIPULATE', 'MANY', 'MAN_2', 'MARATHON', 'MARKET', 'MARRY', 'MARY', 'MATCH', 'MATH', 'MAYBE', 'MAYOR-BLOOMBERG', 'MCDONALD', 'MEAN', 'MEANING-LIST', 'MEAT', 'MEDICINE', 'MEDIUM', 'MEET', 'MEETING', 'MELON/PUMPKIN', 'MELT/SOLVE', 'MEMBER', 'MEMORIZE', 'MENTION', 'MENTOR', 'MENU', 'MESSED-UP', 'METAL', 'MEXICO/SPAIN', 'MIAMI', 'MICROPHONE', 'MIDDLE', 'MIKE', 'MILES', 'MILK', 'MILLION', 'MIND', 'MINN', 'MINUTE', 'MINUTES', 'MISMATCH', 'MISS', 'MISS/ASSUME', 'MISTAKE', 'MISUNDERSTAND', 'MITT-ROMNEY', 'MODEL', 'MONDAY', 'MONEY', 'MONEY_2', 'MONSTER', 'MOOD', 'MORE', 'MORE-THAN', 'MORMON', 'MORMON_2', 'MORNING', 'MOST', 'MOST-SKILLED', 'MOTHER', 'MOTHER+FATHER', 'MOTIVATE', 'MOTORCYCLE', 'MOUSE/FICTION', 'MOVE', 'MOVE-AROUND', 'MOVE-AWAY', 'MOVIE', 'MOVIE_2', 'MOVING-ON-TO-NEXT-TOPIC', 'MOW', 'MP', 'MPH', 'MR', 'MUCH', 'MUFFLE', 'MUHAMMAD', 'MUSIC', 'MUST', 'MUTE', 'N', 'NAB', 'NADAL', 'NAKED', 'NAME', 'NARROW-STREET', 'NASAL', 'NATURAL', 'NAUSEA', 'NEAR', 'NEAT', 'NEB', 'NEED', 'NEGATIVE', 'NEGLECT', 'NERVOUS', 'NETFLIX', 'NEV', 'NEVADA', 'NEVER', 'NEW', 'NEW-YORK', 'NEW-YORK_2', 'NEWS', 'NEWTON', 'NEXT', 'NEXT-GROUP', 'NEXT-TO', 'NEXT-TOPIC', 'NEXT-TO_2', 'NEXT-WEEK', 'NEXT/UPCOMING', 'NICE', 'NICE/CLEAN', 'NIGHT', 'NINE', 'NINETEEN', 'NINETEEN_2+NINETY-NINE', 'NINETY', 'NINETY-SIX', 'NO', 'NO+MORE', 'NO-GOOD', 'NOISE', 'NOISE+RUMBLE', 'NOISY', 'NONE', 'NONE/NOTHING', 'NONE/NOTHING+ONE', 'NORMAL', 'NORTH', 'NORTHRIDGE', 'NOSE', 'NOT', 'NOT+MUST', 'NOT-CARE', 'NOT-KNOW', 'NOT-LIKE', 'NOT-MIND', 'NOT-WANT', 'NOT-YET', 'NOTHING', 'NOTHING-AT-ALL', 'NOTICE', 'NOW', 'NOW+AFTERNOON', 'NOW+AFTERNOON_2', 'NOW+MORNING', 'NOW+NIGHT', 'NOW-WEEK', 'NTID', 'NUMB', 'NUMBER', 'NURSE', 'NUT', 'NUT+BUTTER', 'NYC', 'OAKLAND', 'OBAMA', 'OBJECTS', 'OBVIOUS', 'OCCASIONALLY', 'OCEAN', 'OF', 'OF-COURSE', 'OFF', 'OFF-POINT', 'OFFENSE', 'OFFICE', 'OFFICERS', 'OFTEN', 'OH', 'OH-I-SEE', 'OHIO', 'OIL', 'OJ', 'OK', 'OLD', 'OLD+AGE-SIX+HALF', 'OLD+FIFTEEN', 'OLD+NINE', 'OLDER', 'OMAHA', 'ON', 'ON-TIME', 'ONCE', 'ONCE-IN-A-WHILE', 'ONE', 'ONE+HUNDRED', 'ONE-DOLLAR', 'ONE-HOUR', 'ONE-MONTH', 'ONE-MORE', 'ONE-THOUSAND', 'ONE-WEEK', 'ONE-WEEK_2', 'ONE_2', 'ONLY', 'OPEN', 'OPEN-BOOK', 'OPINION', 'OPPORTUNITY', 'OPPOSITE', 'OPPRESS', 'OPTION', 'OR', 'ORANGE', 'ORDER', 'ORGANIZATION', 'ORIENTATION', 'ORIENTED', 'OTHER', 'OUT', 'OUTGOING', 'OUTSIDE', 'OVER-IT', 'OVER-NIGHT', 'OVER/AFTER', 'OVER/AFTER+SCHOOL', 'OVERSLEEP', 'OWN', 'P', 'PACE', 'PACE/PROGRESS', 'PACK', 'PADDLE/CANOE', 'PAGEANT', 'PAGER', 'PAGES', 'PAINT', 'PAINT_2', 'PALE', 'PANIC', 'PANS', 'PANT', 'PAPER', 'PAPER-CHECK/CARD', 'PARADE', 'PARALLEL', 'PARK', 'PARKINSONS', 'PARPF', 'PART', 'PARTY', 'PARTY-HARD', 'PASS', 'PASS-DOWN', 'PASS-OUT_2', 'PASS-OUT_3', 'PAST', 'PAST+ALL-NIGHT', 'PAST+NIGHT', 'PATIENT', 'PAUL', 'PAY', 'PAY-ATTENTION', 'PAY/BUY', 'PAY/SPEND', 'PC', 'PCL', 'PEE', 'PEN', 'PEOPLE', 'PERCENT', 'PERFECT', 'PERMIT', 'PERSON', 'PERSONA', 'PERSONALITY', 'PET-PEEVE', 'PET/SPOILED', 'PETE', 'PETRONESTRATIY', 'PHIL', 'PHILADELPHIA', 'PHONE', 'PHYSICS', 'PICK-UP', 'PICK/CHOOSE', 'PICK/SELECT', 'PICTURE', 'PIE', 'PIG', 'PINEAPPLE', 'PING-PONG/TENNIS', 'PISS', 'PISS-OFF', 'PITCH', 'PITCH-IN', 'PITCHED', 'PITY', 'PIZZA_4', 'PKSON', 'PLACE', 'PLAID', 'PLAN', 'PLANT', 'PLASTIC', 'PLAY', 'PLAY+AGENT', 'PLAY-AGAINST', 'PLAYER', 'PLAYS', 'PLEASE/ENJOY', 'PLUS', 'PO', 'POINT', 'POINT-TO', 'POLE', 'POLICY', 'POLITE', 'POLITICS', 'PONDER', 'POP', 'POP-UP', 'POPCORN', 'POSITIVE', 'POSS', 'POSTER', 'POSTPONE', 'POTATO', 'POTS', 'POTTER', 'POUND', 'POUND/WEIGH', 'POUR-SWEAT', 'POWER', 'POWERFUL', 'PRACTICE', 'PREFER', 'PREGNANT', 'PRESENCE', 'PRESIDENT', 'PRESSURE', 'PRETEND', 'PRETTY', 'PREVENT', 'PRICE', 'PRINCE', 'PRINCIPAL', 'PRINT', 'PRO', 'PROBLEM', 'PROBLEM_2', 'PROCEED', 'PROCEED_2', 'PROCTOR', 'PRODUCT', 'PROFESSION', 'PROGRESS', 'PROJECT', 'PROPORTION', 'PROVE', 'PUBLIC', 'PUERTO-RICO', 'PULL', 'PUMP', 'PUNCH', 'PUNISH', 'PUPPIES', 'PURPLE', 'PUSH', 'PUT', 'PUT-AWAY', 'QB', 'QM', 'QUALITY', 'QUARTERBACK', 'QUESTION', 'QUIET', 'QUIT', 'QUIZ', 'QUOTE', 'QUOTE/TOPIC', 'QUOTE/TOPIC_2', 'R', 'RA', 'RACE', 'RAFT', 'RAIDERS', 'RAIN', 'RAKE', 'RANGE', 'RANGE/SPAN', 'RE+OPEN', 'READ', 'READY', 'READ_2', 'REALIZE', 'REALLY', 'REALLY+WORK', 'REALLY+WORK/BUSINESS', 'REASON', 'RECENT-PAST', 'RECORD', 'RED', 'REFER', 'REFLECT', 'REFUSE', 'REGULAR', 'RELATIONSHIP', 'RELATIONSHIP-OFF', 'RELAX', 'REMEMBER', 'REMIND', 'REMOTE', 'RENT', 'REPAIR_2', 'REPLY', 'REPORT', 'REPORTER', 'REPUBLICAN', 'REQUEST', 'REQUIRE', 'RESEARCH', 'RESIDENCE/ADDRESS', 'RESONANCE', 'RESOURCES', 'RESPONSIBILITY', 'RESTAURANT', 'RESULT', 'RESUME', 'RETIRE', 'RIBS', 'RICK-PERRY', 'RIDE', 'RIGHT', 'RIGHT-HERE', 'RING', 'RISK', 'RIT', 'RIVER', 'ROAD', 'ROCHESTER', 'ROCK', 'RODNEY', 'ROLE', 'ROLL-ON-FLOOR-LAUGHING', 'ROLLERBLADE', 'ROOF', 'ROOM', 'ROOMMATE', 'ROPE', 'ROW/PADDLE', 'ROWDY', 'ROWING', 'RSD', 'RUBBER', 'RUIN', 'RULE', 'RUN', 'RUN-MACHINE', 'RUN-OUT', 'RUNNING-BACK', 'S', 'SACK', 'SAD', 'SAFE', 'SALE', 'SALLY', 'SAME', 'SAME-LIST', 'SAME-OLD', 'SAME-TIME', 'SAME_2', 'SANCTUARY', 'SANDWICH_4', 'SARAH-JESSICA-PARKER', 'SARAH-PALIN', 'SARCASM', 'SAT', 'SATISFIED', 'SAVE-MONEY', 'SAY', 'SB', 'SCARE', 'SCHEDULE', 'SCHOOL', 'SCISSOR', 'SCL', 'SCOUT', 'SCRAPE', 'SCRATCH', 'SCREAM', 'SCREEN/FILTER', 'SCRIMMAGE', 'SD', 'SEAFOOD', 'SEARCH-FOR', 'SEASON', 'SEASONS', 'SEATTLE', 'SECOND', 'SECOND-IN-LIST', 'SECOND-IN-LIST_4', 'SEE', 'SEE-SEE', 'SEEM', 'SELECT', 'SELES', 'SELF', 'SELL', 'SEMESTER', 'SEND', 'SEND/MAIL', 'SENIOR', 'SENSE', 'SENSES', 'SENSORY', 'SENTENCE', 'SEPARATE', 'SEPT', 'SERIOUS', 'SERVICE', 'SET-ASIDE', 'SET-UP', 'SEVEN', 'SEVEN-THIRTY', 'SEVENTH', 'SEVENTY-FIVE', 'SEVEN_2', 'SEW', 'SEWER', 'SEXUAL', 'SF', 'SHAKE', 'SHAMPOO', 'SHARP', 'SHARPOVA', 'SHEEP', 'SHEET-CAKE', 'SHH', 'SHINE_2', 'SHIRT', 'SHOCK', 'SHOE', 'SHOOT', 'SHORT', 'SHORT-HEIGHT', 'SHORTS', 'SHOULD', 'SHOW', 'SHOWER', 'SHUCKS', 'SHUT', 'SIBERIAN', 'SICK', 'SICK-OF', 'SICK/SILLY', 'SIDE', 'SIGH', 'SIGN', 'SILBER', 'SILK', 'SILLY', 'SIMPLE', 'SIREN', 'SISTER', 'SIT', 'SITES', 'SITUATION', 'SIX', 'SIX+HALF', 'SIX-DAY', 'SIXTY', 'SIX_2+ZERO_3', 'SIZE', 'SKI', 'SKILL', 'SLEEP', 'SLICE', 'SLOW', 'SMALL', 'SMALL-AMOUNT', 'SMASHED', 'SMELL', 'SMILE', 'SMIRK', 'SMOKE', 'SMOOTH', 'SNEAK', 'SNOB', 'SNOW', 'SO', 'SO-SO', 'SOCIAL', 'SOCIAL-WORK', 'SOCIAL/INTERACT', 'SOCIAL/INTERACT_2', 'SOCIALIZE', 'SOLICIT', 'SOME', 'SOME+ONE', 'SOME+THING', 'SOME+WHERE', 'SOMETHING/ONE', 'SOMETIMES', 'SON', 'SOON', 'SORRY', 'SOUND', 'SOUP', 'SOUTH', 'SPA', 'SPAGHETTI_2', 'SPARE', 'SPECIAL/EXCEPT', 'SPECIAL/EXCEPT+fs-LY', 'SPECIALIZATION', 'SPECIALTY', 'SPECIALTY+AGENT', 'SPEECH', 'SPEECH/LECTURE', 'SPEECH/ORAL', 'SPEED', 'SPELL', 'SPEND', 'SPIN', 'SPINACH', 'SPIT', 'SPORT', 'SPORTS', 'SPRAIN', 'SPREAD', 'SPRING', 'SS', 'STADIUM', 'STAFF', 'STAMP', 'STAND', 'STAND-UP', 'STAR', 'STAR+BOMB', 'START', 'STATE', 'STATION', 'STATUS', 'STAY', 'STAY-AWAKE', 'STAY-AWAKE-ALL-NIGHT', 'STAY_2', 'STEAK', 'STEAL', 'STICK', 'STICK-ON-SURFACE', 'STICK-ON-THROAT', 'STICKY', 'STILL', 'STITCH', 'STOMACH', 'STOOL', 'STOP', 'STOPLIGHT', 'STORE', 'STORM', 'STORY', 'STRAIGHT', 'STRANGE', 'STRANGER', 'STRATEGY', 'STRICT', 'STRING', 'STRIPE', 'STRIPES', 'STRONG', 'STRUGGLE', 'STUCK', 'STUDENT', 'STUDY', 'STUDY_2', 'STYROFOAM', 'SUBLIMINAL-INFLUENCE', 'SUBTELNY', 'SUBTRACT', 'SUCCEED', 'SUE', 'SUFFER', 'SUGGEST', 'SUMMER', 'SUMMON', 'SUN', 'SUN+#BURN', 'SUNBATHE', 'SUNDAY_2', 'SUNNY', 'SUNRISE', 'SUNSCREEN', 'SUNSET', 'SUNSHINE', 'SUNSHINE_2', 'SUN_2', 'SUPERIOR', 'SUPPORT', 'SUPPOSE', 'SUPREME', 'SURF-INTERNET', 'SURFACE', 'SURGEON', 'SURPASS', 'SURPRISE', 'SUSHI', 'SUSPECT', 'SWALLOW', 'SWASHBUCKLING', 'SWEETHEART', 'SWIM', 'SWIRLING', 'SWITCH', 'SYNTAX', 'SYSTEM', 'TABLE', 'TABOO', 'TACTILE', 'TAG', 'TAKE', 'TAKE-BREAK', 'TAKE-NOTES', 'TAKE-OFF', 'TAKE-OUT', 'TAKE-OVER', 'TAKE-UP', 'TALK', 'TALK-OVER', 'TAN', 'TANK-TOP', 'TAPE-RECORDING', 'TASKS', 'TASTE', 'TD', 'TE', 'TEA', 'TEACH', 'TEACH+AGENT', 'TEACHER', 'TEAM', 'TEASE', 'TEASE_2', 'TECHNOLOGY', 'TELL', 'TEMPO', 'TEMPT', 'TEN', 'TEND', 'TENN', 'TENSE', 'TERMINAL', 'TEST', 'TESTIMONIES', 'TEST_2', 'TEXAS', 'TEXTING', 'TEXTING_3', 'THAN', 'THANK-YOU', 'THAT', 'THATS-ALL', 'THEN', 'THEREFORE', 'THICK', 'THING', 'THINK', 'THINK+APPEAR', 'THINK+BURST', 'THINK+HOPE', 'THINK+LONG-LIST', 'THINK+MARRY', 'THINK+SHOCK', 'THINK-OVER', 'THIRD', 'THIRD-IN-LIST', 'THIRD-PLACE_2', 'THIRTY_2', 'THIS', 'THOSE-TWO', 'THOUSAND', 'THREE', 'THREE-DAY', 'THREE-HOUR', 'THRILL/WHATS-UP', 'THROUGH', 'THROW', 'THUMBS-UP/GOOD', 'THURSDAY', 'TIE', 'TIE_2+FISHING', 'TIGER', 'TIME', 'TIME+DCL', 'TIME-FOUR', 'TIME-PERIOD', 'TIME-TWO', 'TIPS', 'TIRE', 'TIRED', 'TJ', 'TO', 'TO/UNTIL', 'TODAY', 'TOGETHER', 'TOGETHER/GO-STEADY', 'TOGETHER/GO-STEADY_2', 'TOILET', 'TOMORROW', 'TONY', 'TOO', 'TOOTH/GLASS', 'TORTURE_2', 'TOTAL', 'TOTAL_2', 'TOUCHDOWN', 'TOUGH', 'TOY', 'TRACE-PATH', 'TRACK-IX', 'TRACK-MIDDLE', 'TRACK-PINKY', 'TRACK-RING', 'TRACK-THUMB', 'TRADE', 'TRADITION', 'TRAFFIC_2', 'TRAIN', 'TRANSFER', 'TRAY', 'TREAT', 'TREE', 'TRIP', 'TROPHY_2', 'TROUBLE', 'TROUNCE', 'TRUCK', 'TRUE-BUSINESS', 'TRUNK', 'TRUST', 'TRY', 'TUBING', 'TURN', 'TURN-OFF', 'TV', 'TWELVE+HALF', 'TWENTY', 'TWENTY-EIGHT', 'TWENTY-FIVE', 'TWICE', 'TWO', 'TWO-HUNDRED', 'TWO_2', 'TWO_2+ZERO+THIRTEEN', 'TYPE', 'TYRANNY', 'UGLY', 'UMBRELLA', 'UN', 'UN+FORTUNATELY', 'UN+PREDICT', 'UNCLE', 'UNDER', 'UNDERSTAND', 'UNIVERSITY', 'UNLESS', 'UNTIL', 'UP', 'UP-TO-NOW', 'UPSET', 'URINE', 'US', 'USE', 'USE-SIGN-LANGUAGE', 'USE_2', 'UTAH', 'V', 'VACATION', 'VALUE', 'VARIOUS', 'VARY', 'VARY_2', 'VEGETABLE', 'VEGETABLE+AGENT', 'VENTRILOQUISM', 'VERY', 'VERY-FAST', 'VIBRATE', 'VIDEO-GAME', 'VIDEO-PHONE', 'VIDEOTAPE', 'VIDEOTAPE_2', 'VINCE', 'VISION', 'VISIT', 'VISIT_2', 'VISUALIZE', 'VOCAL', 'VOCALIZATION', 'VOICE', 'VOICE-RANGE', 'VOICE-RANGE_2', 'VOICE_2', 'VOLUNTEER/SHIRT', 'VOMIT', 'VOMIT/HATE', 'VOTE', 'VP', 'VT', 'WAIT', 'WAITRESS', 'WAIVE', 'WAKE-UP', 'WALK', 'WALLET', 'WANT', 'WANT+NEG', 'WANT_2', 'WANT_3', 'WARM', 'WARN', 'WAS', 'WASH', 'WASTE', 'WATCH', 'WATCH-TV', 'WATER', 'WATER+DCL', 'WATER+TUBING', 'WAVE-HELLO', 'WAVES', 'WAY', 'WEAR', 'WEATHER', 'WEDDING', 'WEEKEND', 'WEEKEND_2', 'WELCOME', 'WERE', 'WET', 'WHAT', 'WHAT+ACTION', 'WHATS-UP', 'WHEEL', 'WHEN', 'WHERE', 'WHEW/RELIEVED', 'WHICH', 'WHIP', 'WHIRL-TIME-PAST', 'WHITE', 'WHO', 'WHOLE', 'WHY', 'WHY+NOT', 'WIDE', 'WIDE-RECEIVER', 'WIFE', 'WIN', 'WINDOW', 'WINNER', 'WINNING', 'WISH', 'WITH', 'WITTY', 'WOLF', 'WOMAN', 'WONDER', 'WONDERFUL', 'WOOD', 'WORD', 'WORK', 'WORK-OUT', 'WORLD', 'WORRY', 'WORSE', 'WORTH', 'WOW', 'WOW/AWFUL', 'WRESTLE', 'WRITE', 'WRITE_2', 'WRONG', 'WYOMING', 'XRAY', 'YEAR', 'YEAR-LONG', 'YES', 'YESTERDAY', 'YES_2', 'YOUNG', 'ZOMBIE', 'ZONE', 'ZOOM', '[END]', '[PAD]', '[START]', '[UNK]', '^', 'a', 'b', 'c', 'd', 'e', 'f', 'fs-', 'h', 'i', 'k', 'l', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'x']\n",
      "num_eng_tokens 3096\n",
      "num_asl_tokens 2131\n"
     ]
    }
   ],
   "source": [
    "# generate\n",
    "    # 1) list of eng-asl sentence pairs\n",
    "    # 2) set of unique english vocab\n",
    "    # 3) set of unique asl vocab\n",
    "data_path = \"/Users/adrianajimenez/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/sent_pairs_joined.txt\"\n",
    "    \n",
    "text_pairs = []\n",
    "eng_texts = []\n",
    "asl_texts = []\n",
    "SPECIAL_TOKENS = [\"[PAD]\", \"[START]\", \"[END]\", \"[UNK]\"]\n",
    "eng_tokens = set(SPECIAL_TOKENS)\n",
    "asl_tokens = set(SPECIAL_TOKENS)\n",
    "max_length = 0\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "\n",
    "for line in lines:\n",
    "    pair = []\n",
    "    eng_text, asl_text = line.split(\"\\t\")\n",
    "    eng_texts.append(eng_text)\n",
    "    asl_texts.append(asl_text)\n",
    "    pair.append(eng_text.lower())\n",
    "    pair.append(asl_text)\n",
    "    text_pairs.append(pair)\n",
    "    \n",
    "for text in eng_texts:\n",
    "    tokens = custom_eng_tokenize(text)\n",
    "    length = len(tokens)\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "    for token in tokens:\n",
    "        if token not in eng_tokens:\n",
    "                eng_tokens.add(token)\n",
    "            \n",
    "for text in asl_texts:\n",
    "    tokens = custom_asl_tokenize(text)\n",
    "    length = len(tokens)\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "    for token in tokens:\n",
    "        if token not in asl_tokens:\n",
    "                asl_tokens.add(token)\n",
    "                            \n",
    "max_encoder_seq_length = max([len(txt) for txt in eng_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in asl_texts])\n",
    "\n",
    "eng_tokens = sorted(list(eng_tokens))\n",
    "asl_tokens = sorted(list(asl_tokens))\n",
    "\n",
    "print(\"eng_tokens:\", eng_tokens)\n",
    "print(\"asl_tokens\", asl_tokens)\n",
    "num_encoder_tokens = len(eng_tokens)\n",
    "num_decoder_tokens = len(asl_tokens)\n",
    "print(\"num_eng_tokens\", num_encoder_tokens)\n",
    "print(\"num_asl_tokens\", num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbce5ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_asl_glosses = set()\n",
    "split_pattern = r\"[\\/\\+\\-]\"\n",
    "        \n",
    "for token in asl_tokens:\n",
    "    parts = re.split(split_pattern, token)\n",
    "    if all(part.isalpha() and part.isupper() for part in parts):\n",
    "        main_asl_glosses.add(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3f61c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters / hyperparameters\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "EMBED_DIM = 128\n",
    "INTERMEDIATE_DIM = 512\n",
    "NUM_HEADS = 4\n",
    "MAX_SEQUENCE_LENGTH = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b587712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the deaf experience has been shown to be really different. the deaf experience the dominance of sound around them.', 'DEAF EXPERIENCE SHOW DIFFERENT DEAF EXPERIENCE SHOW REALLY fs-HEGEMONY fs-OF NOISE SUPERIOR']\n",
      "[\"mary bought her mother's car and gave it to john.\", 'fs-MARY BUY POSS MOTHER CAR GIFT fs-JOHN']\n",
      "['mother will buy a house.', 'MOTHER FUTURE BUY HOUSE']\n",
      "['i have to admit that sf copies boston a little, but anyway...', 'CITY/COMMUNITY REALLY IX ADMIT IX COPY fs-BOSTON LITTLE-BIT BUT']\n",
      "[\"i don't think he was planning to go, but if i see him, i'll have him call you.\", 'IX NOT THINK IX PLAN GO IF SEE IX FUTURE INFORM CALL-BY-PHONE IX']\n"
     ]
    }
   ],
   "source": [
    "# glimpse pairs\n",
    "\n",
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c010cbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3389 total pairs\n",
      "2373 training pairs\n",
      "508 validation pairs\n",
      "508 test pairs\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b0a1679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.DictTokenizer object at 0x16abbf3d0>\n",
      "<__main__.DictTokenizer object at 0x16abbf3a0>\n"
     ]
    }
   ],
   "source": [
    "eng_vocab = dict([(char, i) for i, char in enumerate(eng_tokens)])\n",
    "asl_vocab = dict([(char, i) for i, char in enumerate(asl_tokens)])\n",
    "\n",
    "eng_tokenizer = DictTokenizer(eng_vocab, tokenizer_fn=custom_eng_tokenize)\n",
    "asl_tokenizer = DictTokenizer(asl_vocab, tokenizer_fn=custom_asl_tokenize)\n",
    "\n",
    "print(eng_tokenizer)\n",
    "print(asl_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb29eb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:  bob hates reading books.\n",
      "Tokens:  [312, 1226, 2157, 318, 9]\n",
      "Recovered text after detokenizing:  bob hates reading books .\n",
      "\n",
      "ASL sentence:  fs-BOB IX VOMIT/HATE READ BOOK\n",
      "Tokens:  [2117, 217, 950, 2017, 1503, 222]\n",
      "Recovered text after detokenizing:  fs- BOB IX VOMIT/HATE READ BOOK\n"
     ]
    }
   ],
   "source": [
    "eng_input_ex = text_pairs[0][0]\n",
    "eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\n",
    "print(\"English sentence: \", eng_input_ex)\n",
    "print(\"Tokens: \", eng_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    eng_tokenizer.detokenize(eng_tokens_ex),\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "asl_input_ex = text_pairs[0][1]\n",
    "asl_tokens_ex = asl_tokenizer.tokenize(asl_input_ex)\n",
    "print(\"ASL sentence: \", asl_input_ex)\n",
    "print(\"Tokens: \", asl_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    asl_tokenizer.detokenize(asl_tokens_ex),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3c4a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(eng, asl):\n",
    "    eng_start_end_packer = keras_hub.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n",
    "        dtype=\"int32\"\n",
    "    )\n",
    "    eng = eng_start_end_packer(eng)\n",
    "\n",
    "    asl_start_end_packer = keras_hub.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
    "        start_value=asl_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value=asl_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value=asl_tokenizer.token_to_id(\"[PAD]\"),\n",
    "        dtype=\"int32\"\n",
    "    )\n",
    "    asl = asl_start_end_packer(asl)\n",
    "\n",
    "    decoder_inputs = asl[:, :-1]\n",
    "    decoder_outputs = asl[:, 1:]\n",
    "\n",
    "    return {\n",
    "        \"encoder_inputs\": eng,\n",
    "        \"decoder_inputs\": decoder_inputs\n",
    "    }, decoder_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30e5e609",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 12:36:32.929244: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-07-17 12:36:32.929446: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-07-17 12:36:32.929454: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752770192.929908 12217163 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1752770192.930333 12217163 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<CacheDataset element_spec=({'encoder_inputs': TensorSpec(shape=(None, 71), dtype=tf.int32, name=None), 'decoder_inputs': TensorSpec(shape=(None, 71), dtype=tf.int32, name=None)}, TensorSpec(shape=(None, 71), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "def make_dataset(pairs):\n",
    "    \n",
    "    eng_ids = [eng_tokenizer.tokenize(sent) for sent, _ in pairs]    \n",
    "    asl_ids = [asl_tokenizer.tokenize(sent) for _, sent in pairs]\n",
    "\n",
    "    # 🛠️ Force token type to int32\n",
    "    eng_tensor = tf.ragged.constant(eng_ids, dtype=tf.int32)\n",
    "    asl_tensor = tf.ragged.constant(asl_ids, dtype=tf.int32)\n",
    "    \n",
    "    dataset = tf_data.Dataset.from_tensor_slices((eng_tensor, asl_tensor))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2a04a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (32, 71)\n",
      "inputs[\"decoder_inputs\"].shape: (32, 71)\n",
      "targets.shape: (32, 71)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 12:36:33.755218: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2025-07-17 12:36:33.756170: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebed136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=num_encoder_tokens,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_hub.layers.TransformerEncoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(inputs=x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=num_decoder_tokens,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(decoder_inputs)\n",
    "\n",
    "x = keras_hub.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")(x)\n",
    "decoder = keras.Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoded_seq_inputs,\n",
    "    ],\n",
    "    decoder_outputs,\n",
    ")\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"transformer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "092c923e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_and_position… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">405,376</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionE…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ token_and_positi… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">821,331</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">2131</span>)             │            │ transformer_enco… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_and_position… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m405,376\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mTokenAndPositionE…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_encoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m198,272\u001b[0m │ token_and_positi… │\n",
       "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │    \u001b[38;5;34m821,331\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m2131\u001b[0m)             │            │ transformer_enco… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,424,979</span> (5.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,424,979\u001b[0m (5.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,424,979</span> (5.44 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,424,979\u001b[0m (5.44 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 12:36:35.470491: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 139ms/step - accuracy: 0.8175 - loss: 2.4179 - val_accuracy: 0.8841 - val_loss: 0.7673\n",
      "Epoch 2/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 117ms/step - accuracy: 0.8914 - loss: 0.7135 - val_accuracy: 0.8860 - val_loss: 0.7242\n",
      "Epoch 3/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 133ms/step - accuracy: 0.8945 - loss: 0.6669 - val_accuracy: 0.8887 - val_loss: 0.7017\n",
      "Epoch 4/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 137ms/step - accuracy: 0.8972 - loss: 0.6373 - val_accuracy: 0.8901 - val_loss: 0.6874\n",
      "Epoch 5/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 122ms/step - accuracy: 0.8991 - loss: 0.6143 - val_accuracy: 0.8905 - val_loss: 0.6795\n",
      "Epoch 6/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 129ms/step - accuracy: 0.8999 - loss: 0.5977 - val_accuracy: 0.8911 - val_loss: 0.6728\n",
      "Epoch 7/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 128ms/step - accuracy: 0.9012 - loss: 0.5818 - val_accuracy: 0.8905 - val_loss: 0.6716\n",
      "Epoch 8/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 134ms/step - accuracy: 0.9013 - loss: 0.5681 - val_accuracy: 0.8913 - val_loss: 0.6672\n",
      "Epoch 9/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 161ms/step - accuracy: 0.9023 - loss: 0.5552 - val_accuracy: 0.8916 - val_loss: 0.6624\n",
      "Epoch 10/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 135ms/step - accuracy: 0.9036 - loss: 0.5445 - val_accuracy: 0.8917 - val_loss: 0.6600\n",
      "Epoch 11/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 134ms/step - accuracy: 0.9042 - loss: 0.5330 - val_accuracy: 0.8930 - val_loss: 0.6572\n",
      "Epoch 12/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 149ms/step - accuracy: 0.9047 - loss: 0.5241 - val_accuracy: 0.8929 - val_loss: 0.6563\n",
      "Epoch 13/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 216ms/step - accuracy: 0.9054 - loss: 0.5143 - val_accuracy: 0.8932 - val_loss: 0.6559\n",
      "Epoch 14/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 143ms/step - accuracy: 0.9065 - loss: 0.5047 - val_accuracy: 0.8939 - val_loss: 0.6503\n",
      "Epoch 15/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 160ms/step - accuracy: 0.9075 - loss: 0.4938 - val_accuracy: 0.8955 - val_loss: 0.6475\n",
      "Epoch 16/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 168ms/step - accuracy: 0.9095 - loss: 0.4828 - val_accuracy: 0.8959 - val_loss: 0.6377\n",
      "Epoch 17/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 177ms/step - accuracy: 0.9116 - loss: 0.4694 - val_accuracy: 0.8967 - val_loss: 0.6352\n",
      "Epoch 18/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 133ms/step - accuracy: 0.9130 - loss: 0.4571 - val_accuracy: 0.8972 - val_loss: 0.6409\n",
      "Epoch 19/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - accuracy: 0.9145 - loss: 0.4469 - val_accuracy: 0.8982 - val_loss: 0.6276\n",
      "Epoch 20/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 132ms/step - accuracy: 0.9161 - loss: 0.4357 - val_accuracy: 0.8993 - val_loss: 0.6318\n",
      "Epoch 21/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 126ms/step - accuracy: 0.9184 - loss: 0.4253 - val_accuracy: 0.9007 - val_loss: 0.6206\n",
      "Epoch 22/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - accuracy: 0.9191 - loss: 0.4162 - val_accuracy: 0.9002 - val_loss: 0.6323\n",
      "Epoch 23/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 129ms/step - accuracy: 0.9208 - loss: 0.4041 - val_accuracy: 0.9016 - val_loss: 0.6195\n",
      "Epoch 24/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 126ms/step - accuracy: 0.9234 - loss: 0.3922 - val_accuracy: 0.9017 - val_loss: 0.6224\n",
      "Epoch 25/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 145ms/step - accuracy: 0.9249 - loss: 0.3829 - val_accuracy: 0.9021 - val_loss: 0.6362\n",
      "Epoch 26/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 127ms/step - accuracy: 0.9257 - loss: 0.3763 - val_accuracy: 0.9026 - val_loss: 0.6187\n",
      "Epoch 27/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 131ms/step - accuracy: 0.9266 - loss: 0.3698 - val_accuracy: 0.9026 - val_loss: 0.6333\n",
      "Epoch 28/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 131ms/step - accuracy: 0.9274 - loss: 0.3641 - val_accuracy: 0.9023 - val_loss: 0.6368\n",
      "Epoch 29/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 129ms/step - accuracy: 0.9288 - loss: 0.3587 - val_accuracy: 0.8999 - val_loss: 0.6473\n",
      "Epoch 30/30\n",
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 127ms/step - accuracy: 0.9286 - loss: 0.3578 - val_accuracy: 0.9028 - val_loss: 0.6269\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16b887e50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad617331",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`ContrastiveSampler` requires passing a `hidden_states`, butreceived `None`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m output_pairs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     43\u001b[0m input_sentence \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(test_eng_texts)\n\u001b[0;32m---> 44\u001b[0m translated \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_sentence\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m translated \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     46\u001b[0m     translated\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[START]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[END]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m output_pairs\u001b[38;5;241m.\u001b[39mappend(input_sentence)\n",
      "Cell \u001b[0;32mIn[41], line 27\u001b[0m, in \u001b[0;36mdecode_sequences\u001b[0;34m(input_sentences)\u001b[0m\n\u001b[1;32m     24\u001b[0m     pad \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mfull((batch_size, length \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m), asl_tokenizer\u001b[38;5;241m.\u001b[39mtoken_to_id(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     25\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconcatenate((start, pad), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m     generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mkeras_hub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamplers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mContrastiveSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.75\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_token_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43masl_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_to_id\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[END]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     generated_tokens \u001b[38;5;241m=\u001b[39m generated_tokens\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     35\u001b[0m     generated_sentences \u001b[38;5;241m=\u001b[39m asl_tokenizer\u001b[38;5;241m.\u001b[39mdetokenize(generated_tokens)\n",
      "File \u001b[0;32m~/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.10/site-packages/keras_hub/src/samplers/contrastive_sampler.py:65\u001b[0m, in \u001b[0;36mContrastiveSampler.__call__\u001b[0;34m(self, next, prompt, cache, index, mask, stop_token_ids, hidden_states, model)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mnext\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m ):\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     66\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`ContrastiveSampler` requires passing a `hidden_states`, but\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceived `None`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m         )\n\u001b[1;32m     69\u001b[0m     batch_size, max_length \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mshape(prompt)[\u001b[38;5;241m0\u001b[39m], ops\u001b[38;5;241m.\u001b[39mshape(prompt)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     70\u001b[0m     index \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mcast(index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: `ContrastiveSampler` requires passing a `hidden_states`, butreceived `None`."
     ]
    }
   ],
   "source": [
    "def decode_sequences(input_sentences):\n",
    "    with tf.device('/CPU:0'):\n",
    "        batch_size = 1\n",
    "\n",
    "        # Tokenize the encoder input.\n",
    "        encoder_input_tokens = ops.convert_to_tensor(eng_tokenizer(input_sentences))\n",
    "        if len(encoder_input_tokens[0]) < MAX_SEQUENCE_LENGTH:\n",
    "            pads = ops.full((1, MAX_SEQUENCE_LENGTH - len(encoder_input_tokens[0])), 0)\n",
    "            encoder_input_tokens = ops.concatenate(\n",
    "                [encoder_input_tokens, pads], 1\n",
    "            )\n",
    "\n",
    "        # Define a function that outputs the next token's probability given the\n",
    "        # input sequence.\n",
    "        def next(prompt, cache, index):\n",
    "            logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "            # Ignore hidden states for now; only needed for contrastive search.\n",
    "            hidden_states = None\n",
    "            return logits, hidden_states, cache\n",
    "\n",
    "        # Build a prompt of length 40 with a start token and padding tokens.\n",
    "        length = MAX_SEQUENCE_LENGTH\n",
    "        start = ops.full((batch_size, 1), asl_tokenizer.token_to_id(\"[START]\"))\n",
    "        pad = ops.full((batch_size, length - 1), asl_tokenizer.token_to_id(\"[PAD]\"))\n",
    "        prompt = ops.concatenate((start, pad), axis=-1)\n",
    "\n",
    "        generated_tokens = keras_hub.samplers.ContrastiveSampler(k=25, alpha=0.75)(\n",
    "        next,\n",
    "        prompt,\n",
    "        stop_token_ids=[asl_tokenizer.token_to_id(\"[END]\")],\n",
    "        index=1,\n",
    "    )\n",
    "        \n",
    "        generated_tokens = generated_tokens.numpy().tolist()[0]\n",
    "        generated_sentences = asl_tokenizer.detokenize(generated_tokens)\n",
    "        return generated_sentences\n",
    "\n",
    "outputs = []\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(50):\n",
    "    output_pairs = []\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequences([input_sentence])\n",
    "    translated = (\n",
    "        translated.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    output_pairs.append(input_sentence)\n",
    "    output_pairs.append(translated)\n",
    "    outputs.append(output_pairs)\n",
    "    \n",
    "df = pd.DataFrame(outputs, columns=[\"input sentence\", \"translation\"])\n",
    "df.to_csv(\"/Users/adrianajimenez/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/seq2seq_code/word_level/k25.txt\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ff7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_1 = keras_hub.metrics.RougeN(order=1)\n",
    "rouge_2 = keras_hub.metrics.RougeN(order=2)\n",
    "\n",
    "for test_pair in test_pairs:\n",
    "    input_sentence = test_pair[0]\n",
    "    reference_sentence = test_pair[1]\n",
    "\n",
    "    translated_sentence = decode_sequences([input_sentence])\n",
    "    translated_sentence = translated_sentence[0]\n",
    "    translated_sentence = (\n",
    "        translated_sentence.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    rouge_1(reference_sentence, translated_sentence)\n",
    "    rouge_2(reference_sentence, translated_sentence)\n",
    "\n",
    "print(\"ROUGE-1 Score: \", rouge_1.result())\n",
    "print(\"ROUGE-2 Score: \", rouge_2.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
