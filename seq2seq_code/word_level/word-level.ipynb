{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for model\n",
    "\n",
    "import keras_hub\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df1779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for pre-parsing\n",
    "\n",
    "from pyparsing import Word, alphas, nums\n",
    "import pyparsing as pp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handshapes\n",
    "\n",
    "handshapelist = [\n",
    "  '1',\n",
    "  '3',\n",
    "  '4',\n",
    "  '5',\n",
    "  '6',\n",
    "  '7',\n",
    "  '8',\n",
    "  '9',\n",
    "  '10',\n",
    "  '25',\n",
    "  'A',\n",
    "  'B',\n",
    "  'C',\n",
    "  'D',\n",
    "  'E',\n",
    "  'F',\n",
    "  'G',\n",
    "  'H-U',\n",
    "  'I',\n",
    "  'K',\n",
    "  'L',\n",
    "  'M',\n",
    "  'N',\n",
    "  'O',\n",
    "  'R',\n",
    "  'S',\n",
    "  'T',\n",
    "  'U',\n",
    "  'V',\n",
    "  'W',\n",
    "  'X',\n",
    "  'Y',\n",
    "  'C-L',\n",
    "  'U-L',\n",
    "  'B-L',\n",
    "  'P-K',\n",
    "  'Q-G',\n",
    "  'L-X',\n",
    "  'I-L-Y',\n",
    "  '5-C',\n",
    "  '5-C-L',\n",
    "  '5-C-tt',\n",
    "  'alt-M',\n",
    "  'alt-N',\n",
    "  'alt-P',\n",
    "  'B-xd',\n",
    "  'baby-O',\n",
    "  'bent-1',\n",
    "  'bent-B',\n",
    "  'bent-B-L',\n",
    "  'bent-horns',\n",
    "  'bent-L',\n",
    "  'bent-M',\n",
    "  'bent-N',\n",
    "  'bent-U',\n",
    "  'cocked-8',\n",
    "  'cocked-F',\n",
    "  'cocked-S',\n",
    "  'crvd-5',\n",
    "  'crvd-B',\n",
    "  'crvd-flat-B',\n",
    "  'crvd-L',\n",
    "  'crvd-sprd-B',\n",
    "  'crvd-U',\n",
    "  'crvd-V',\n",
    "  'fanned-flat-O',\n",
    "  'flat-B',\n",
    "  'flat-C',\n",
    "  'flat-F',\n",
    "  'flat-G',\n",
    "  'flat-O-2',\n",
    "  'flat-O',\n",
    "  'full-M',\n",
    "  'horns',\n",
    "  'loose-E',\n",
    "  'O-2-horns',\n",
    "  'open-8',\n",
    "  'open-F',\n",
    "  'sml-C-3',\n",
    "  'tight-C-2',\n",
    "  'tight-C',\n",
    "  'X-over-thumb'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex rules\n",
    "\n",
    "number_regexp = r\"\"\"\n",
    "    [0-9]+                      # one or more digits\n",
    "    (?:\\.[0-9]+)?               # optional decimal\n",
    "    s?                          # optional trailing 's'\n",
    "\"\"\"\n",
    "\n",
    "alpha_regexp = r\"\"\"\n",
    "    (?!                         # negative lookahead for:\n",
    "        (?:THUMB-)?             # optional THUMB-\n",
    "        (?:IX-|POSS-|SELF-)     # followed by IX-, POSS-, SELF-\n",
    "    )\n",
    "    [A-Z0-9]                    # starts with uppercase or digit\n",
    "    (?:                         # optionally followed by:\n",
    "        [A-Z0-9'-]*             #   more letters, digits, hyphen or apostrophe\n",
    "        [A-Z0-9]                #   ends on a letter or digit\n",
    "    )?\n",
    "    (?:                         # optionally followed by:\n",
    "        \\.                      #   a period\n",
    "      |                         # or\n",
    "        :[0-9]                  #   colon-digit (e.g., :2)\n",
    "    )?\n",
    "\"\"\"\n",
    "\n",
    "lookahead_regexp = r\"\"\"\n",
    "    (?:\n",
    "        (?![a-z])               # not followed by lowercase letter\n",
    "      | (?=wg)                  # unless it's specifically \"wg\"\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "word_all_regexp = r\"\"\"\n",
    "    (?: {number_regexp} | {alpha_regexp} )\n",
    "    {lookalookahead_regexphead}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbd4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conventions kept for parsing\n",
    "\n",
    "cl_prefix = pp.one_of([\"CL\", \"DCL\", \"LCL\", \"SCL\", \"BCL\", \"BPCL\", \"PCL\", \"ICL\"])\n",
    "ns_prefix = pp.Literal(\"ns\")\n",
    "fs_prefix = pp.Literal(\"fs\")\n",
    "lex_exceptions = pp.one_of([\"part\", \"WHAT\"])\n",
    "aspect_text = pp.Literal(\"aspect\")\n",
    "index_core_ix = pp.Literal(\"IX\")\n",
    "other_index_core = pp.one_of([\"POSS\", \"SELF\"])\n",
    "handshape = pp.one_of(handshapelist)\n",
    "person = pp.one_of([\"1p\", \"2p\", \"3p\"])\n",
    "dash = pp.Literal(\"-\")\n",
    "arc = pp.Literal(\"arc\") \n",
    "loc = pp.Literal(\"loc\")\n",
    "pl = pp.Literal(\"pl\")\n",
    "compound = pp.Literal(\"+\")\n",
    "hashtag = pp.Literal(\"#\")\n",
    "choice = pp.Literal(\"/\")\n",
    "sym = pp.Literal(\">\")\n",
    "par1 = pp.Literal(\"(\")\n",
    "par2 = pp.Literal(\")\")\n",
    "contraction = pp.Literal(\"^\")\n",
    "colon = pp.Literal(\":\")\n",
    "omit_quote = pp.Literal(\"xx\")\n",
    "period = pp.Literal(\".\")\n",
    "alphas = pp.Word(alphas, max=1)\n",
    "nums = pp.Word(nums, max=1)\n",
    "word = pp.Regex(word_all_regexp, flags=re.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b062760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammar rules\n",
    "\n",
    "full_grammar = pp.OneOrMore(\n",
    "    cl_prefix |               # classifiers like CL, DCL, etc.\n",
    "    ns_prefix |               # non-specific ns\n",
    "    fs_prefix |               # fingerspelling fs\n",
    "    index_core_ix |           # IX\n",
    "    other_index_core |        # POSS, SELF\n",
    "    person |                  # 1p, 2p, 3p\n",
    "    lex_exceptions |          # part, WHAT\n",
    "    aspect_text |             # aspect\n",
    "    arc |                     # arc\n",
    "    loc |                     # loc\n",
    "    pl |                      # pl\n",
    "    handshape |               # handshapes like B, 1, 5, etc.\n",
    "    compound |                # +\n",
    "    hashtag |                 # #\n",
    "    choice |                  # /\n",
    "    sym |                     # >\n",
    "    contraction |             # ^\n",
    "    colon |                   # :\n",
    "    dash |                    # -\n",
    "    par1 | par2 |             # ( and )\n",
    "    omit_quote |              # xx\n",
    "    period |                  # .\n",
    "    word |\n",
    "    nums |                    # numbers last resort\n",
    "    alphas                     # fallback LAST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45096aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing grammar parsing\n",
    "\n",
    "trial = full_grammar.parse_string(\"SCL:1xx\", parse_all=True).asList()\n",
    "trial2 = full_grammar.parse_string(\"IX-1p-pl-2 WORK LANDSCAPE fs-LANDSCAPING IX-1p 5xx\", parse_all=True).as_list()\n",
    "\n",
    "print(trial)\n",
    "print(trial2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize based on predefined grammar rules\n",
    "\n",
    "def custom_tokenize(text):\n",
    "    try:\n",
    "        return full_grammar.parse_string(text, parse_all=True).as_list()\n",
    "    except pp.ParseException as pe:\n",
    "        print(f\"Failed to parse: {pe}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc0cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing custom_tokenize\n",
    "\n",
    "trial = custom_tokenize(\"SCL:1xx\")\n",
    "trial2 = custom_tokenize(\"IX-1p-pl-2 WORK LANDSCAPE fs-LANDSCAPING IX-1p 5xx\")\n",
    "\n",
    "print(trial)\n",
    "print(trial2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79a6b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters / hyperparameters\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30  # This should be at least 10 for convergence\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "ENG_VOCAB_SIZE = 5056\n",
    "ASL_VOCAB_SIZE = 2283\n",
    "num_samples = 1400\n",
    "\n",
    "EMBED_DIM = 128\n",
    "INTERMEDIATE_DIM =1024\n",
    "NUM_HEADS = 8\n",
    "data_path = #change to sent_pairs.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df98b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate files of\n",
    "    # 1) eng-asl sentence pairs\n",
    "    # 2) list of unique english vocab\n",
    "    # 3) list of unique asl vocab (main glosses only)\n",
    "text_pairs = [[]]\n",
    "index = 0\n",
    "\n",
    "input_tokens = set()\n",
    "target_tokens = set()\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "   \n",
    "tokens_used = [] \n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split(\"\\t\")\n",
    "    text_pairs[index].append(input_text)\n",
    "    text_pairs[index].append(target_text)\n",
    "    index += 1\n",
    "    for token in input_text:\n",
    "        if token not in input_tokens:\n",
    "            input_tokens.add(token)\n",
    "            \n",
    "            \n",
    "for pair in text_pairs:\n",
    "    for asl in pair[1]:\n",
    "        sent_tokens = custom_tokenize(asl)\n",
    "        if sent_tokens not in target_tokens:\n",
    "            target_tokens.add(sent_tokens)\n",
    "\n",
    "\n",
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))\n",
    "\n",
    "print(\"input_tokens:\", input_tokens)\n",
    "print(\"output_tokens\", target_tokens)\n",
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "print(\"num_eng_tokens\", num_encoder_tokens)\n",
    "print(\"num_asl_tokens\", num_decoder_tokens)\n",
    "print(target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b587712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glimpse pairs\n",
    "\n",
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c010cbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train_word_piece\n",
    "\n",
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_hub.tokenizers.compute_word_piece_vocabulary(\n",
    "        word_piece_ds.batch(1000).prefetch(2),\n",
    "        vocabulary_size=vocab_size,\n",
    "        reserved_tokens=reserved_tokens,\n",
    "    )\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5335f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "asl_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "asl_vocab = train_word_piece(asl_samples, ASL_VOCAB_SIZE, reserved_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e3764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"English Tokens: \", eng_vocab[100:110])\n",
    "print(\"ASL Tokens: \", asl_vocab[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9895e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=eng_vocab, lowercase=False\n",
    ")\n",
    "asl_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=asl_vocab, lowercase=False, split=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb29eb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_input_ex = text_pairs[0][0]\n",
    "eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\n",
    "print(\"English sentence: \", eng_input_ex)\n",
    "print(\"Tokens: \", eng_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    eng_tokenizer.detokenize(eng_tokens_ex),\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "asl_input_ex = text_pairs[0][1]\n",
    "asl_tokens_ex = asl_tokenizer.tokenize(asl_input_ex)\n",
    "print(\"ASL Gloss sentence: \", asl_input_ex)\n",
    "print(\"Tokens: \", asl_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    asl_tokenizer.detokenize(asl_tokens_ex),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c4a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(eng, asl):\n",
    "    batch_size = ops.shape(asl)[0]\n",
    "\n",
    "    eng = eng_tokenizer(eng)\n",
    "    asl = asl_tokenizer(asl)\n",
    "\n",
    "    # Pad `eng` to `MAX_SEQUENCE_LENGTH`.\n",
    "    eng_start_end_packer = keras_hub.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    eng = eng_start_end_packer(eng)\n",
    "\n",
    "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `asl` and pad it as well.\n",
    "    asl_start_end_packer = keras_hub.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
    "        start_value=asl_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value=asl_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value=asl_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    asl = asl_start_end_packer(asl)\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": eng,\n",
    "            \"decoder_inputs\": asl[:, :-1],\n",
    "        },\n",
    "        asl[:, 1:],\n",
    "    )\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, asl_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    asl_texts = list(asl_texts)\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, asl_texts))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a04a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebed136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=ENG_VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_hub.layers.TransformerEncoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(inputs=x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=ASL_VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(decoder_inputs)\n",
    "\n",
    "x = keras_hub.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(ASL_VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoded_seq_inputs,\n",
    "    ],\n",
    "    decoder_outputs,\n",
    ")\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"transformer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092c923e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad617331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequences(input_sentences):\n",
    "    batch_size = 1\n",
    "\n",
    "    # Tokenize the encoder input.\n",
    "    encoder_input_tokens = ops.convert_to_tensor(eng_tokenizer(input_sentences))\n",
    "    if len(encoder_input_tokens[0]) < MAX_SEQUENCE_LENGTH:\n",
    "        pads = ops.full((1, MAX_SEQUENCE_LENGTH - len(encoder_input_tokens[0])), 0)\n",
    "        encoder_input_tokens = ops.concatenate(\n",
    "            [encoder_input_tokens.to_tensor(), pads], 1\n",
    "        )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def next(prompt, cache, index):\n",
    "        logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "        # Ignore hidden states for now; only needed for contrastive search.\n",
    "        hidden_states = None\n",
    "        return logits, hidden_states, cache\n",
    "\n",
    "    # Build a prompt of length 40 with a start token and padding tokens.\n",
    "    length = 40\n",
    "    start = ops.full((batch_size, 1), asl_tokenizer.token_to_id(\"[START]\"))\n",
    "    pad = ops.full((batch_size, length - 1), asl_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    prompt = ops.concatenate((start, pad), axis=-1)\n",
    "\n",
    "    generated_tokens = keras_hub.samplers.GreedySampler()(\n",
    "        next,\n",
    "        prompt,\n",
    "        stop_token_ids=[asl_tokenizer.token_to_id(\"[END]\")],\n",
    "        index=1,  # Start sampling after start token.\n",
    "    )\n",
    "    generated_sentences = asl_tokenizer.detokenize(generated_tokens)\n",
    "    return generated_sentences\n",
    "\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(2):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequences([input_sentence])\n",
    "    translated = translated.numpy()[0].decode(\"utf-8\")\n",
    "    translated = (\n",
    "        translated.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    print(f\"** Example {i} **\")\n",
    "    print(input_sentence)\n",
    "    print(translated)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881ff7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_1 = keras_hub.metrics.RougeN(order=1)\n",
    "rouge_2 = keras_hub.metrics.RougeN(order=2)\n",
    "\n",
    "for test_pair in test_pairs[:30]:\n",
    "    input_sentence = test_pair[0]\n",
    "    reference_sentence = test_pair[1]\n",
    "\n",
    "    translated_sentence = decode_sequences([input_sentence])\n",
    "    translated_sentence = translated_sentence.numpy()[0].decode(\"utf-8\")\n",
    "    translated_sentence = (\n",
    "        translated_sentence.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    rouge_1(reference_sentence, translated_sentence)\n",
    "    rouge_2(reference_sentence, translated_sentence)\n",
    "\n",
    "print(\"ROUGE-1 Score: \", rouge_1.result())\n",
    "print(\"ROUGE-2 Score: \", rouge_2.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
