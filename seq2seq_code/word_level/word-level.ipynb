{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43c1584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adrianajimenez/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import keras_hub\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow as tf\n",
    "from tensorflow_text.tools.wordpiece_vocab import (\n",
    "    bert_vocab_from_dataset,\n",
    ")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df1779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for pre-parsing\n",
    "from pyparsing import Word, alphas as pp_alpha, nums as pp_nums\n",
    "import pyparsing as pp\n",
    "pp.ParserElement.enablePackrat()\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd3efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# handshapes\n",
    "\n",
    "handshapelist = [\n",
    "  '1',\n",
    "  '3',\n",
    "  '4',\n",
    "  '5',\n",
    "  '6',\n",
    "  '7',\n",
    "  '8',\n",
    "  '9',\n",
    "  '10',\n",
    "  '25',\n",
    "  'A',\n",
    "  'B',\n",
    "  'C',\n",
    "  'D',\n",
    "  'E',\n",
    "  'F',\n",
    "  'G',\n",
    "  'H-U',\n",
    "  'I',\n",
    "  'K',\n",
    "  'L',\n",
    "  'M',\n",
    "  'N',\n",
    "  'O',\n",
    "  'R',\n",
    "  'S',\n",
    "  'T',\n",
    "  'U',\n",
    "  'V',\n",
    "  'W',\n",
    "  'X',\n",
    "  'Y',\n",
    "  'C-L',\n",
    "  'U-L',\n",
    "  'B-L',\n",
    "  'P-K',\n",
    "  'Q-G',\n",
    "  'L-X',\n",
    "  'I-L-Y',\n",
    "  '5-C',\n",
    "  '5-C-L',\n",
    "  '5-C-tt',\n",
    "  'alt-M',\n",
    "  'alt-N',\n",
    "  'alt-P',\n",
    "  'B-xd',\n",
    "  'baby-O',\n",
    "  'bent-1',\n",
    "  'bent-B',\n",
    "  'bent-B-L',\n",
    "  'bent-horns',\n",
    "  'bent-L',\n",
    "  'bent-M',\n",
    "  'bent-N',\n",
    "  'bent-U',\n",
    "  'cocked-8',\n",
    "  'cocked-F',\n",
    "  'cocked-S',\n",
    "  'crvd-5',\n",
    "  'crvd-B',\n",
    "  'crvd-flat-B',\n",
    "  'crvd-L',\n",
    "  'crvd-sprd-B',\n",
    "  'crvd-U',\n",
    "  'crvd-V',\n",
    "  'fanned-flat-O',\n",
    "  'flat-B',\n",
    "  'flat-C',\n",
    "  'flat-F',\n",
    "  'flat-G',\n",
    "  'flat-O-2',\n",
    "  'flat-O',\n",
    "  'full-M',\n",
    "  'horns',\n",
    "  'loose-E',\n",
    "  'O-2-horns',\n",
    "  'open-8',\n",
    "  'open-F',\n",
    "  'sml-C-3',\n",
    "  'tight-C-2',\n",
    "  'tight-C',\n",
    "  'X-over-thumb'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0a3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex rules\n",
    "\n",
    "alpha_regexp  = r\"(?!((?:THUMB-)?(?:IX-|POSS-|SELF-)))[A-Z](?:[A-Z/'-]*[A-Z])?(?:\\.)?\"\n",
    "lookahead_regexp = r\"(?:(?![a-z])|(?=wg))\"\n",
    "\n",
    "word_all_regexp = r\"\"\"(?x)\n",
    "    (?: %s )\n",
    "    %s\n",
    "\"\"\" % (alpha_regexp, lookahead_regexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbd4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conventions kept for parsing\n",
    "\n",
    "cl_prefix = pp.one_of([\"CL\", \"DCL\", \"LCL\", \"SCL\", \"BCL\", \"BPCL\", \"PCL\", \"ICL\"])\n",
    "ns_prefix = pp.Literal(\"ns\")\n",
    "fs_prefix = pp.Literal(\"fs\")\n",
    "lex_exceptions = pp.one_of([\"part\", \"WHAT\"])\n",
    "aspect_text = pp.Literal(\"aspect\")\n",
    "index_core_ix = pp.Literal(\"IX\")\n",
    "other_index_core = pp.one_of([\"POSS\", \"SELF\"])\n",
    "handshape = pp.one_of(handshapelist)\n",
    "person = pp.one_of([\"1p\", \"2p\", \"3p\"])\n",
    "arc = pp.Literal(\"arc\") \n",
    "loc = pp.Literal(\"loc\")\n",
    "pl = pp.Literal(\"pl\")\n",
    "compound = pp.Literal(\"+\")\n",
    "hashtag = pp.Literal(\"#\")\n",
    "sym = pp.Literal(\">\")\n",
    "par1 = pp.Literal(\"(\")\n",
    "par2 = pp.Literal(\")\")\n",
    "dash = pp.Literal(\"-\")\n",
    "contraction = pp.Literal(\"^\")\n",
    "colon = pp.Literal(\":\")\n",
    "omit_quote = pp.Literal(\"xx\")\n",
    "period = pp.Literal(\".\")\n",
    "alpha = pp.Word(pp_alpha, max=1)\n",
    "num = pp.Word(pp_nums, max=1)\n",
    "word = pp.Regex(word_all_regexp, flags=re.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b062760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammar rules\n",
    "\n",
    "full_grammar = pp.OneOrMore(\n",
    "    cl_prefix |               # classifiers like CL, DCL, etc.\n",
    "    word |\n",
    "    ns_prefix |               # non-specific ns\n",
    "    fs_prefix |               # fingerspelling fs\n",
    "    index_core_ix |           # IX\n",
    "    other_index_core |        # POSS, SELF\n",
    "    person |                  # 1p, 2p, 3p\n",
    "    lex_exceptions |          # part, WHAT\n",
    "    aspect_text |             # aspect\n",
    "    arc |                     # arc\n",
    "    loc |                     # loc\n",
    "    pl |                      # pl\n",
    "    handshape |               # handshapes like B, 1, 5, etc.\n",
    "    compound |                # +\n",
    "    hashtag |                 # #\n",
    "    sym |                     # >\n",
    "    contraction |             # ^\n",
    "    colon |                   # :\n",
    "    par1 | par2 |             # ( and )\n",
    "    omit_quote |              # xx\n",
    "    period |                  # .\n",
    "    dash |\n",
    "    num |\n",
    "    alpha                     # fallback LAST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45096aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCL', ':', '1', 'xx']\n",
      "['IX', '-', '1p', 'BCL', 'xx', 'FIND/FIND-OUT', 'fs', '-', 'HER']\n"
     ]
    }
   ],
   "source": [
    "# testing grammar parsing\n",
    "\n",
    "trial = full_grammar.parse_string(\"SCL:1xx\", parse_all=True).asList()\n",
    "trial2 = full_grammar.parse_string(\"IX-1p BCLxx FIND/FIND-OUT fs-HER\", parse_all=True).asList()\n",
    "\n",
    "print(trial)\n",
    "print(trial2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258c52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize based on predefined grammar rules\n",
    "\n",
    "def custom_asl_tokenize(text):\n",
    "    try:\n",
    "        return full_grammar.parse_string(text, parse_all=True).asList()\n",
    "    except pp.ParseException as pe:\n",
    "        print(f\"Failed to parse: {pe}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03d3e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_eng_tokenize(text):\n",
    "    # Perserve punctuation and digits\n",
    "    text = re.sub(r'([^\\w\\s]|\\d)', r' \\1 ', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Split on whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cc0cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCL', ':', '1', 'xx']\n",
      "['SCL', ':', '1', 'xx', 'SHOWER', 'WASH', 'FEEL', 'THUMBS-UP/GOOD']\n"
     ]
    }
   ],
   "source": [
    "# testing custom_asl_tokenize\n",
    "\n",
    "trial = custom_asl_tokenize(\"SCL:1xx\")\n",
    "trial2 = custom_asl_tokenize('SCL:1xx SHOWER WASH FEEL THUMBS-UP/GOOD')\n",
    "\n",
    "print(trial)\n",
    "print(trial2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae869df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['but', 'all', 'the', 'same', ',', 'he', 'told', 'me', 'i', 'better', 'go', 'downstairs', 'and', 'get', 'an', 'x', '-', 'ray', '.']\n",
      "['i', 'waited', '2', '3', '4', 'years', 'and', '2', ',', '1', '4', '2', 'days']\n"
     ]
    }
   ],
   "source": [
    "# testing custom_eng_tokenize\n",
    "\n",
    "trial = custom_eng_tokenize(\"But all the same, he told me I better go downstairs and get an x-ray.\")\n",
    "trial2 = custom_eng_tokenize('I waited 234 years and 2,142 days')\n",
    "\n",
    "print(trial)\n",
    "print(trial2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e79a6b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters / hyperparameters\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30 \n",
    "EMBED_DIM = 128\n",
    "INTERMEDIATE_DIM = 512\n",
    "NUM_HEADS = 4\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "ENG_VOCAB_SIZE = 1678 + 4\n",
    "ASL_VOCAB_SIZE = 1117 + 4\n",
    "num_samples = 1400\n",
    "\n",
    "data_path = \"/Users/adrianajimenez/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/data/sent_pairs.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3df98b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to parse: Expected end of text, found '/'  (at char 19), (line:1, col:20)\n",
      "Failed to parse: Expected end of text, found '/'  (at char 23), (line:1, col:24)\n",
      "eng_tokens: ['!', '\"', '$', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'a', 'aaahhh', 'abdominal', 'able', 'about', 'accepted', 'accident', 'across', 'act', 'action', 'actually', 'admit', 'advantage', 'adventure', 'advice', 'after', 'again', 'against', 'ago', 'agree', 'ah', 'ahead', 'ahh', 'ail', 'air', 'ali', 'all', 'alone', 'along', 'already', 'alright', 'also', 'am', 'amazing', 'an', 'and', 'angeles', 'angled', 'angry', 'animal', 'ankle', 'ann', 'another', 'answer', 'any', 'anymore', 'anyone', 'anything', 'anyway', 'anywhere', 'applaud', 'apple', 'approach', 'are', 'area', 'arm', 'arms', 'around', 'arrest', 'arrested', 'arrive', 'arrived', 'arrives', 'arriving', 'as', 'aside', 'ask', 'asked', 'asks', 'assistant', 'at', 'ate', 'attack', 'attacker', 'attention', 'average', 'awake', 'away', 'awful', 'b', 'babies', 'baby', 'back', 'bad', 'bag', 'bags', 'ball', 'band', 'bandage', 'banged', 'banging', 'banks', 'bar', 'barely', 'basketball', 'bathroom', 'be', 'beat', 'beating', 'beautiful', 'became', 'because', 'become', 'bed', 'been', 'beer', 'before', 'began', 'begin', 'behind', 'being', 'believe', 'belong', 'belt', 'beneath', 'bent', 'best', 'better', 'between', 'beverly', 'bib', 'big', 'bigger', 'bill', 'birth', 'bit', 'bite', 'bladder', 'blames', 'bled', 'bleed', 'bleeding', 'blew', 'block', 'blood', 'blown', 'blue', 'boat', 'body', 'book', 'books', 'boots', 'border', 'bored', 'boring', 'born', 'borrowed', 'boss', 'boston', 'bostonians', 'both', 'bottle', 'bought', 'bowl', 'box', 'boxer', 'boxes', 'boxing', 'boy', 'boys', 'braced', 'brakes', 'bread', 'break', 'breakfast', 'breath', 'breathe', 'brick', 'bricks', 'bring', 'brings', 'broccoli', 'broke', 'broken', 'brother', 'brought', 'brownish', 'bruise', 'bucks', 'build', 'building', 'buildings', 'built', 'bumps', 'bun', 'bunch', 'bundled', 'burger', 'burn', 'burned', 'bus', 'bush', 'business', 'but', 'butcher', 'butts', 'buy', 'buying', 'by', 'cabin', 'cabinet', 'calfornia', 'california', 'call', 'called', 'calm', 'came', 'camp', 'campground', 'can', 'candy', 'cannot', 'cape', 'car', 'care', 'carrying', 'cars', 'case', 'casino', 'catch', 'caught', 'cause', 'caused', 'celebrate', 'celebrated', 'celebrating', 'celebrations', 'chair', 'chairs', 'challenge', 'change', 'changed', 'changing', 'channel', 'chased', 'chasing', 'chat', 'chatted', 'check', 'checks', 'cheer', 'chemicals', 'chess', 'chicago', 'chicken', 'chimney', 'china', 'chocolate', 'choice', 'choices', 'chokes', 'choose', 'chopped', 'churning', 'cigarette', 'cigarettes', 'cities', 'city', 'class', 'clean', 'clear', 'clearly', 'climb', 'climbing', 'cliques', 'clock', 'close', 'closed', 'closer', 'clubs', 'clue', 'cnn', 'coach', 'coat', 'coats', 'cod', 'coffee', 'cofffee', 'coffin', 'cohesiveness', 'cold', 'colder', 'collar', 'collected', 'college', 'color', 'colorado', 'come', 'comfortable', 'coming', 'communicate', 'communicated', 'community', 'company', 'compare', 'compares', 'compelled', 'competition', 'complete', 'completely', 'confidence', 'confident', 'confusion', 'continue', 'continued', 'continues', 'contributed', 'control', 'cooked', 'cool', 'cooler', 'coordinator', 'cop', 'copies', 'cops', 'copy', 'corn', 'corner', 'costs', 'coughing', 'could', 'couldn', 'counselor', 'country', 'couple', 'course', 'cover', 'covers', 'cows', 'cpr', 'cracked', 'crashed', 'crashes', 'crazy', 'create', 'creative', 'creepy', 'crib', 'cried', 'crops', 'cross', 'crossing', 'cruiser', 'crying', 'csun', 'cuff', 'cultural', 'culture', 'curvy', 'cut', 'cute', 'cutting', 'd', 'dairy', 'daisy', 'dana', 'dance', 'danced', 'dark', 'date', 'dawn', 'day', 'days', 'dead', 'deaf', 'deafies', 'deal', 'december', 'decided', 'decorate', 'deep', 'deeply', 'deer', 'definitely', 'degrees', 'delicious', 'destroy', 'did', 'didn', 'didnt', 'died', 'difference', 'differences', 'different', 'dinner', 'direction', 'disappointed', 'disciplinary', 'discussed', 'disease', 'dislike', 'dissipate', 'distance', 'diversity', 'do', 'doctor', 'doctors', 'does', 'doesn', 'doing', 'dollars', 'don', 'done', 'donut', 'door', 'doorbell', 'doorknob', 'doors', 'dorm', 'dorms', 'down', 'downstairs', 'drag', 'drain', 'drank', 'dream', 'dreams', 'drinking', 'drinks', 'drive', 'drivers', 'drives', 'driving', 'drop', 'dropped', 'drops', 'drove', 'drugs', 'dry', 'dug', 'dumbfounded', 'dumped', 'during', 'each', 'ear', 'ears', 'easier', 'easy', 'eat', 'eaten', 'eating', 'economic', 'edge', 'effort', 'eight', 'either', 'eleven', 'elite', 'else', 'email', 'emergency', 'emotion', 'empty', 'encouraging', 'end', 'ended', 'ends', 'engine', 'english', 'engrossed', 'enjoy', 'enjoyed', 'enjoying', 'enjoys', 'enough', 'entered', 'environments', 'erick', 'escape', 'especially', 'etc', 'europe', 'even', 'eventually', 'ever', 'everwhere', 'every', 'everyday', 'everyone', 'everything', 'example', 'exchanged', 'exchanging', 'excited', 'excused', 'exhausted', 'exit', 'expected', 'expecting', 'expects', 'expensive', 'experiences', 'explain', 'explained', 'express', 'eye', 'face', 'facing', 'fall', 'falls', 'family', 'fan', 'fancy', 'fans', 'far', 'farm', 'farther', 'fascinated', 'fast', 'father', 'favorite', 'feed', 'feel', 'feeling', 'feels', 'feet', 'fell', 'felt', 'fenced', 'few', 'field', 'fifteen', 'finally', 'find', 'finds', 'fine', 'fined', 'finger', 'fingers', 'finish', 'finished', 'finishes', 'finishing', 'fire', 'fireplace', 'first', 'fist', 'fists', 'five', 'fixed', 'flashed', 'flat', 'flavorful', 'flew', 'flood', 'flooding', 'floor', 'flowers', 'fluent', 'focus', 'follow', 'following', 'food', 'foot', 'football', 'for', 'forest', 'forever', 'forget', 'forgot', 'former', 'forth', 'forward', 'found', 'four', 'francisco', 'frank', 'fraternity', 'freak', 'french', 'frends', 'fresh', 'fresher', 'friday', 'friend', 'friendly', 'friends', 'fries', 'from', 'front', 'frozen', 'fruit', 'fruits', 'frustrated', 'fumble', 'fun', 'funny', 'further', 'future', 'gain', 'gamble', 'gambled', 'gambling', 'game', 'gas', 'gather', 'gave', 'geez', 'general', 'generations', 'gesture', 'gestured', 'get', 'gets', 'getting', 'girl', 'girls', 'give', 'given', 'gives', 'giving', 'go', 'god', 'goes', 'going', 'gone', 'good', 'goose', 'got', 'gotten', 'grabbed', 'grandfather', 'grass', 'great', 'green', 'grew', 'gross', 'ground', 'group', 'grow', 'growing', 'guess', 'guy', 'guys', 'had', 'hadn', 'hah', 'haha', 'half', 'hall', 'hamburger', 'hand', 'handcuffed', 'handcuffs', 'handle', 'hands', 'hanging', 'happened', 'happening', 'happens', 'happily', 'happy', 'hard', 'has', 'hasn', 'hastily', 'hate', 'have', 'he', 'head', 'headache', 'headed', 'headlights', 'hear', 'heard', 'hearing', 'heart', 'held', 'helmet', 'her', 'here', 'herself', 'hey', 'hi', 'hid', 'high', 'highway', 'hill', 'hills', 'him', 'himself', 'his', 'hit', 'hitchhiking', 'hits', 'hitting', 'hold', 'holding', 'home', 'homes', 'hope', 'hoped', 'hoping', 'hospital', 'hospitals', 'hot', 'hotel', 'hotter', 'hours', 'house', 'how', 'however', 'huge', 'humid', 'humor', 'hung', 'hungry', 'hurried', 'hurry', 'hurts', 'husky', 'i', 'idea', 'idiot', 'if', 'ignored', 'illinois', 'imagination', 'imagine', 'imagined', 'imagines', 'impacted', 'impatient', 'important', 'impossible', 'in', 'incredulously', 'indiana', 'indicated', 'infection', 'inform', 'information', 'informed', 'ingore', 'inhaled', 'injections', 'injured', 'inside', 'inspiring', 'instructions', 'insurance', 'intense', 'interested', 'intersection', 'interstate', 'into', 'invited', 'involved', 'involves', 'iowa', 'ironic', 'is', 'isn', 'it', 'its', 'jail', 'jana', 'jersey', 'john', 'join', 'joined', 'joint', 'jumping', 'just', 'k', 'keep', 'keeps', 'kept', 'kicks', 'kids', 'killed', 'kind', 'king', 'knee', 'knew', 'know', 'knows', 'l', 'la', 'laboriously', 'laid', 'landed', 'landscaping', 'lane', 'lapd', 'last', 'lasts', 'late', 'lately', 'later', 'laughed', 'laughing', 'lays', 'lazy', 'leader', 'learn', 'learned', 'least', 'leave', 'leaves', 'leaving', 'led', 'left', 'leg', 'legal', 'let', 'lid', 'lies', 'life', 'lightning', 'like', 'liked', 'likes', 'liking', 'limit', 'limited', 'limp', 'lincoln', 'line', 'lips', 'list', 'lit', 'literally', 'littered', 'little', 'live', 'lived', 'll', 'loaf', 'location', 'locked', 'log', 'lombardi', 'long', 'look', 'looked', 'looking', 'looks', 'los', 'lost', 'lot', 'louder', 'loudly', 'lound', 'love', 'loves', 'luck', 'lucky', 'lunch', 'lurches', 'm', 'machine', 'made', 'magazine', 'maine', 'make', 'makers', 'man', 'many', 'march', 'market', 'markets', 'mary', 'match', 'matter', 'maybe', 'mcdonald', 'mcdonalds', 'me', 'mean', 'means', 'meat', 'mechanic', 'medium', 'men', 'menu', 'messing', 'metal', 'meter', 'mexican', 'mexico', 'middle', 'mike', 'miles', 'millions', 'mind', 'minded', 'mine', 'mingling', 'minutes', 'mischevious', 'misunderstood', 'mix', 'mixes', 'mom', 'money', 'more', 'morning', 'most', 'mostly', 'mother', 'motivating', 'motorcycle', 'motorcycles', 'motorcycling', 'mouth', 'move', 'moved', 'movie', 'movies', 'mph', 'mr', 'much', 'muhammad', 'muscles', 'must', 'my', 'myself', 'nah', 'nailed', 'nails', 'naked', 'name', 'named', 'near', 'nearby', 'nebraska', 'need', 'needed', 'needle', 'needs', 'negative', 'neither', 'nervous', 'nevada', 'never', 'new', 'news', 'next', 'nice', 'night', 'nightstick', 'no', 'nobody', 'north', 'northridge', 'not', 'nothing', 'notice', 'noticed', 'now', 'numb', 'number', 'nurse', 'o', 'oakland', 'obviously', 'occasionally', 'of', 'off', 'offense', 'offensive', 'office', 'officers', 'often', 'oh', 'ohio', 'oil', 'ok', 'okay', 'old', 'older', 'omaha', 'on', 'once', 'one', 'ones', 'only', 'onto', 'oozing', 'open', 'opened', 'opens', 'opinion', 'opportunity', 'options', 'or', 'orders', 'other', 'others', 'otherwise', 'our', 'out', 'outgoing', 'outside', 'outta', 'over', 'overnight', 'own', 'pace', 'paddle', 'pageant', 'pager', 'pain', 'panic', 'pans', 'pants', 'paper', 'parents', 'parkinson', 'party', 'pass', 'passed', 'past', 'path', 'patient', 'paul', 'pay', 'pee', 'peeked', 'pen', 'people', 'percent', 'perfect', 'perpendicular', 'person', 'personally', 'persuit', 'phone', 'pick', 'picked', 'picks', 'picture', 'pieces', 'pig', 'pigs', 'pile', 'pill', 'place', 'places', 'plain', 'planning', 'plans', 'plants', 'plastic', 'play', 'player', 'players', 'plays', 'plus', 'pocket', 'pointed', 'police', 'policies', 'policy', 'pond', 'popped', 'popping', 'positive', 'post', 'potatoes', 'pots', 'pounding', 'pounds', 'poured', 'pours', 'practically', 'prefer', 'preferred', 'prefers', 'pregnant', 'pressure', 'pretty', 'price', 'pries', 'probably', 'problem', 'problems', 'products', 'profusely', 'pry', 'public', 'pull', 'pulled', 'pulls', 'pump', 'punched', 'punches', 'punching', 'punished', 'punishment', 'pushed', 'put', 'puts', 'quarterback', 'question', 'quick', 'quickly', 'quote', 'ra', 'raft', 'rafting', 'raiders', 'rain', 'raining', 'rains', 'rainy', 'raised', 'ran', 'rang', 'rapids', 'ras', 'rather', 'ray', 're', 'reached', 'reaches', 'read', 'reading', 'reads', 'ready', 'real', 'realize', 'realized', 'really', 'reason', 'reassures', 'receiver', 'recommend', 'record', 'refused', 'regular', 'regularly', 'relaxed', 'relief', 'relieved', 'remember', 'reminds', 'removes', 'repeatedly', 'replaces', 'replacing', 'reply', 'report', 'reporter', 'require', 'requires', 'residence', 'residences', 'restaurant', 'restaurants', 'result', 'results', 'ribs', 'riding', 'right', 'ring', 'rises', 'river', 'road', 'rock', 'rodney', 'rollerbladed', 'rollerblades', 'roof', 'room', 'roommates', 'rope', 'rose', 'route', 'row', 'rowing', 'rubber', 'rummaged', 'run', 'running', 'runs', 'rushes', 's', 'sack', 'sad', 'safe', 'said', 'same', 'san', 'sank', 'sarcastically', 'sat', 'saw', 'say', 'says', 'scared', 'school', 'scissors', 'score', 'scream', 'screamed', 'seafood', 'seasons', 'seat', 'seats', 'second', 'security', 'see', 'seeing', 'seem', 'seemed', 'seems', 'seen', 'sell', 'semester', 'send', 'sent', 'sentence', 'separated', 'september', 'serious', 'service', 'set', 'sets', 'setting', 'seven', 'sewer', 'sf', 'shake', 'shaken', 'shakes', 'shaking', 'shampoo', 'sharp', 'she', 'sheep', 'shines', 'shirt', 'shirts', 'shock', 'shocked', 'shoes', 'shone', 'shook', 'shoots', 'shop', 'shopping', 'shorts', 'shot', 'should', 'shoulder', 'shouldn', 'show', 'showed', 'shower', 'siberian', 'side', 'sign', 'signer', 'signing', 'silk', 'simple', 'since', 'siren', 'sister', 'sit', 'sitting', 'situation', 'six', 'sizes', 'skidding', 'slammed', 'slaughtering', 'sleep', 'sleeping', 'sleeps', 'sleeves', 'sliced', 'slide', 'slings', 'slow', 'slowly', 'small', 'smaller', 'smell', 'smoking', 'smoothly', 'snatch', 'sneak', 'snows', 'snuck', 'so', 'social', 'socially', 'sold', 'some', 'someone', 'something', 'sometimes', 'somewhere', 'soon', 'sore', 'sorry', 'sort', 'spa', 'spare', 'speak', 'special', 'spedometer', 'speech', 'speed', 'spit', 'spoke', 'sport', 'sports', 'spots', 'sprained', 'spring', 'spun', 'staff', 'stand', 'standing', 'stared', 'staring', 'start', 'started', 'starts', 'state', 'states', 'station', 'stay', 'stayed', 'stays', 'steak', 'steam', 'stiill', 'still', 'stitched', 'stitches', 'stitching', 'stolen', 'stomach', 'stood', 'stop', 'stopped', 'stops', 'store', 'storm', 'story', 'straight', 'strange', 'strangers', 'strict', 'strikes', 'string', 'stripes', 'strong', 'struck', 'struggling', 'stuck', 'student', 'students', 'sudden', 'suddenly', 'sue', 'summer', 'sun', 'sundown', 'sunny', 'sunrise', 'sunset', 'super', 'superior', 'supposed', 'sure', 'surgeon', 'surprised', 'survive', 'swerved', 'swinging', 'switched', 'switching', 'system', 'systems', 't', 'table', 'tables', 'tag', 'tail', 'take', 'takes', 'taking', 'talk', 'talked', 'talking', 'tank', 'tape', 'tapped', 'tappee', 'tapper', 'taps', 'taste', 'tea', 'teach', 'teacher', 'team', 'teams', 'tease', 'teasing', 'tell', 'telling', 'tells', 'tempted', 'ten', 'tend', 'tended', 'tends', 'tents', 'test', 'than', 'thanks', 'that', 'thats', 'the', 'their', 'them', 'themselves', 'then', 'there', 'they', 'thick', 'thing', 'things', 'think', 'thinking', 'thinks', 'third', 'this', 'those', 'though', 'thought', 'thousand', 'thousands', 'three', 'threw', 'through', 'throw', 'throws', 'thrusts', 'ticked', 'tie', 'tied', 'tigers', 'tight', 'til', 'time', 'times', 'tiny', 'tire', 'tired', 'tires', 'to', 'today', 'together', 'told', 'tomorrow', 'tonight', 'tons', 'tony', 'too', 'took', 'tool', 'top', 'total', 'touch', 'touchdown', 'tougher', 'toward', 'towards', 'toys', 'tray', 'trees', 'tried', 'trip', 'trouble', 'trunk', 'trust', 'try', 'trying', 'turn', 'turned', 'turns', 'tv', 'twenty', 'twice', 'two', 'typical', 'u', 'ugh', 'uh', 'umbrellas', 'under', 'underage', 'understand', 'uneasy', 'unfortunately', 'university', 'unless', 'unpredictable', 'unsatified', 'until', 'untill', 'up', 'upset', 'upstairs', 'us', 'use', 'used', 'using', 'utah', 'various', 'vary', 've', 'vegetables', 'very', 'vibrating', 'victim', 'videotape', 'vince', 'violently', 'visit', 'visited', 'visiting', 'wait', 'waited', 'waiters', 'waiting', 'waitress', 'waive', 'walk', 'walked', 'walking', 'walks', 'wallet', 'want', 'wanted', 'wants', 'wards', 'warmly', 'warn', 'warned', 'was', 'wash', 'washed', 'wasn', 'wasnt', 'wasting', 'watch', 'watched', 'watches', 'watching', 'water', 'wave', 'waved', 'waving', 'way', 'ways', 'we', 'wear', 'wearing', 'weather', 'week', 'weekend', 'weigh', 'welcome', 'welcoming', 'well', 'went', 'were', 'wet', 'what', 'whatever', 'wheel', 'when', 'where', 'whether', 'which', 'while', 'whipping', 'white', 'who', 'whoa', 'whole', 'whom', 'whose', 'why', 'wide', 'wife', 'will', 'willing', 'win', 'winding', 'windows', 'winner', 'winning', 'wish', 'with', 'without', 'wnt', 'woke', 'woken', 'wolf', 'woman', 'won', 'wonder', 'wondered', 'wonderful', 'wondering', 'wood', 'word', 'words', 'work', 'worked', 'worker', 'working', 'worried', 'worry', 'worse', 'worst', 'worth', 'would', 'wouldn', 'wow', 'wrapped', 'write', 'writing', 'written', 'wrong', 'wrote', 'wymoing', 'wymoning', 'wyoming', 'x', 'yawned', 'year', 'years', 'yell', 'yelled', 'yes', 'yesterday', 'yet', 'york', 'you', 'young', 'your', 'yourself', 'zipped', 'zone']\n",
      "asl_tokens ['#', '(', ')', '+', '-', '.', '0', '1', '10', '1p', '2', '25', '2p', '3', '3p', '4', '5', '5-C', '5-C-L', '6', '7', '8', '9', ':', '>', 'A', 'A-WAYS', 'ABOUT', 'ABUSE', 'ACCEPT', 'ACCIDENT', 'ACROSS', 'ADMIT', 'ADVANTAGE', 'ADVENTURE', 'ADVISE', 'ADVISER', 'AFTER', 'AGAIN', 'AGAINST', 'AGE', 'AGREE', 'AIR', 'ALI', 'ALL', 'ALL-DAY', 'ALL-GONE', 'ALL-NIGHT', 'ALL-THE-WAY', 'ALMOST', 'ALONE', 'ALRIGHT', 'AND', 'ANGRY', 'ANIMAL', 'ANKLE', 'ANN', 'ANY', 'ANYWAY/NOT-MATTER', 'APPEAR', 'APPLAUSE', 'APPLE', 'APPROACH', 'AREA', 'ARREST', 'ARRIVE', 'ASK', 'ASS', 'ASSISTANT', 'AT', 'AVERAGE', 'AWFUL', 'B', 'BABY', 'BACK', 'BAD', 'BAG', 'BAGS', 'BALL', 'BANDAGE', 'BANKS', 'BASKETBALL', 'BATHROOM', 'BCL', 'BE', 'BEAUTIFUL', 'BECAUSE', 'BECOME', 'BED', 'BEER', 'BEFORE', 'BELIEVE', 'BELONG', 'BELT', 'BEST', 'BETTER', 'BETWEEN', 'BEVERLY', 'BIB', 'BIG', 'BIGGER', 'BILL', 'BLACK', 'BLADDER', 'BLAME', 'BLOCK', 'BLOOD', 'BLOW', 'BLOWN-AWAY', 'BLUE', 'BOAT', 'BONE', 'BOOK', 'BOOT', 'BORDER', 'BORE', 'BORN', 'BORROW', 'BOSS', 'BOSTON', 'BOTH', 'BOX', 'BOXER', 'BOXING', 'BOY', 'BPCL', 'BRAKE', 'BREAD', 'BREAK', 'BREAK-DOWN', 'BREATHE', 'BRICK', 'BRING', 'BROCCOLI', 'BROTHER', 'BROWN', 'BUILD', 'BUILDING', 'BUS', 'BUSH', 'BUSINESS', 'BUT', 'BUTCHER-SHOP', 'BUTTS', 'BUY', 'BY', 'C', 'CABIN', 'CALIFORNIA', 'CALL', 'CALL-BY-PHONE', 'CAMP', 'CAN', 'CANDY', 'CANNOT', 'CAPE-COD', 'CAPECOD', 'CAR', 'CARE', 'CASINO', 'CAUSE', 'CELEBRATE', 'CHAIR', 'CHALLENGE', 'CHANGE', 'CHANNEL', 'CHARACTER', 'CHASE', 'CHAT', 'CHECK', 'CHEER', 'CHEMICAL', 'CHESS', 'CHICAGO', 'CHICKEN', 'CHINA', 'CHOCOLATE', 'CHOICE', 'CHOP', 'CIGARETTE', 'CITY/COMMUNITY', 'CL', 'CNN', 'COACH', 'COAT', 'COFFEE', 'COFFIN', 'COLD', 'COLLECT', 'COLLEGE', 'COLOR', 'COLORADO', 'COME', 'COME-ON', 'COMFORTABLE', 'COMMUNICATE', 'COMPANY', 'COMPARE', 'CONCEPT', 'CONFIDENT', 'CONFUSE', 'CONTACT', 'CONTINUE', 'COOK', 'COOL', 'COOLER', 'COORDINATOR', 'COP', 'COPY', 'CORN', 'CORNER', 'COUGH', 'COUNT-ON-FINGERS', 'COUNTRY', 'COW', 'CPR', 'CRACK', 'CRASH', 'CRAZY', 'CREATE/PRETEND', 'CREEPY', 'CRIB', 'CROPS', 'CRY', 'CSUN', 'CULTURE', 'CUT', 'CUTE', 'CUTTER', 'DAISY', 'DANA', 'DANCE', 'DARK', 'DATE', 'DAY', 'DCL', 'DEAD', 'DEAF', 'DEC', 'DECIDE', 'DECORATE', 'DEER', 'DEGREE', 'DELICIOUS', 'DEPART', 'DEPRESS', 'DESTROY', 'DEVIL', 'DIE', 'DIFFERENT', 'DISAPPOINT', 'DISCIPLINE', 'DISCONNECT', 'DISEASE', 'DO', 'DOCTOR', 'DOLLAR', 'DONUT', 'DOOR', 'DORM', 'DOWN', 'DOWN-THE-LIST', 'DREAM', 'DRINK', 'DRIVE', 'DROP', 'DRUG', 'DRUGS', 'DRY', 'DURING/WHILE', 'E', 'EACH', 'EAN', 'EAR', 'EASIER', 'EAT', 'EAT-UP', 'ECONOMIC', 'EIGHTY', 'ELEVEN', 'ELITE', 'EMAIL', 'EMERGENCY', 'EMOTION', 'EMPTY', 'END', 'ENGINE', 'ENGLISH', 'ENJOY', 'ENOUGH', 'ENTER', 'ENVIRONMENT', 'EQUAL', 'ERICK', 'ESCAPE', 'ETC.', 'EUROPE', 'EVEN', 'EVER', 'EVERY', 'EVERYDAY', 'EX', 'EXAM', 'EXAMPLE', 'EXCITE', 'EXCITED', 'EXCUSE', 'EXCUSE-GO', 'EXHAUST', 'EXPECT', 'EXPENSIVE', 'EXPERIENCE', 'EXPLAIN', 'EXPRESS', 'EXTEND', 'F', 'FACE', 'FALL', 'FALL-ASLEEP', 'FALL-INTO-IT', 'FALL-INTO-PLACE', 'FAMILY', 'FAN', 'FANCY', 'FANS', 'FAR', 'FARM', 'FASCINATED', 'FAST', 'FATHER', 'FAVORITE/PREFER', 'FEED', 'FEEL', 'FEET', 'FENCE', 'FEW', 'FF', 'FIELD', 'FIFTEEN', 'FILM', 'FINALLY/SUCCEED', 'FIND', 'FIND/FIND-OUT', 'FINE', 'FINISH', 'FIRE', 'FIRST', 'FIVE', 'FIVE-MINUTE', 'FIX', 'FLAT', 'FLOOD', 'FLOOR', 'FLOWER', 'FOCUS', 'FOLLOW', 'FOOD', 'FOOT', 'FOOTBALL', 'FOR', 'FOREVER', 'FORGET', 'FORGET-IT', 'FORMERLY', 'FORTUNATELY', 'FOUR', 'FOUR-DAY', 'FOUR-THIRTY', 'FRANK', 'FRATERNITY', 'FREAK', 'FRESH', 'FRIDAY', 'FRIEND', 'FRIENDLY', 'FROM', 'FROM-NOW-ON', 'FRONT', 'FRUGAL', 'FRUIT', 'FS-ZONE', 'FULL', 'FUMBLE', 'FUN', 'FUNNY', 'FUTURE', 'G', 'GAMBLE', 'GAME', 'GAS', 'GENERAL', 'GESTURE', 'GET', 'GET-IN-BED', 'GET-UP', 'GIFT', 'GIRL', 'GIVE', 'GO', 'GO-AHEAD', 'GO-AWAY', 'GO-OUT', 'GOD', 'GONE', 'GOOD', 'GRAB-CHANCE', 'GRANDFATHER', 'GRASS', 'GREAT', 'GREEN', 'GROUND', 'GROUP/TOGETHER', 'GROW', 'GROW-UP', 'GUESS', 'HALF', 'HALL', 'HAMBURGER', 'HANDLE', 'HAPPEN', 'HAPPY', 'HARD', 'HAVE', 'HAVE-TO', 'HEAD', 'HEAD-COLD', 'HEAD-TRIP', 'HEADACHE', 'HEAR', 'HEARING', 'HEART', 'HELMET', 'HER', 'HERE', 'HIDE', 'HIGH', 'HIGHWAY', 'HILL', 'HILLS', 'HIT', 'HITCH-HIKE', 'HOLD', 'HOME', 'HOPE', 'HOSPITAL', 'HOT', 'HOTEL', 'HOUR', 'HOURS', 'HOUSE', 'HOW', 'HOW-MANY', 'HS', 'HUMB-IX', 'HUMID', 'HUMOR', 'HUNGRY', 'HURRY', 'HURT', 'I', 'ICL', 'IDEA', 'IDIOT', 'IF', 'IGNORE', 'ILL', 'IMAGINE', 'IMB', 'IMPACT', 'IMPORTANT', 'IN', 'INCLUDE/ALL-INCLUDED', 'INCREASE', 'IND', 'INFORM', 'INFORMATION', 'INJECT', 'INSPIRE', 'INSURANCE/INFECTION', 'INTERACT/COMMUNICATE', 'INVITE', 'INVOLVE', 'IOWA', 'IS', 'IT', 'ITS', 'IX', 'J', 'JAIL', 'JANA', 'JOHN', 'JOIN', 'JOINT', 'JUMP', 'K', 'KID', 'KILL', 'KIND', 'KING', 'KISS-FIST', 'KNEE', 'KNOW', 'KNOW-THAT', 'L', 'LA', 'LANDSCAPE', 'LANDSCAPING', 'LAPD', 'LAST-WEEK', 'LAST-YEAR', 'LATE', 'LATER', 'LAUGH', 'LAZY', 'LCL', 'LEAD', 'LEADER', 'LEAF', 'LEARN', 'LEAVE-THERE', 'LEG', 'LEGAL/LAW', 'LIFE', 'LIGHTNING', 'LIKE', 'LIMIT', 'LINCOLN', 'LINE', 'LIP', 'LIST', 'LITTLE-BIT', 'LIVE', 'LOCK', 'LODI', 'LOG', 'LOMBARDI', 'LOMBIDI', 'LOMDI', 'LONG-LIST', 'LONG-SLEEVE', 'LONG-TERM', 'LOOK', 'LOOK-AROUND', 'LOOK-AT', 'LOOK-BACK', 'LOOK-FOR', 'LOOK-LIKE', 'LOOK-UP', 'LOSE', 'LOUD', 'LOVE', 'LOW/LOWER', 'LUCK', 'LUCKY', 'LVINCE', 'LY', 'MACHINE', 'MAGAZINE', 'MAIL', 'MAINE', 'MAKE', 'MAKER', 'MAN', 'MANY', 'MARKET', 'MARY', 'MAYBE', 'MCDONALD', 'MEAN', 'MEAT', 'MEDIUM', 'MELT/SOLVE', 'MENU', 'METAL', 'MEXICO/SPAIN', 'MIDDLE', 'MIKE', 'MILES', 'MILLION', 'MIND', 'MINUTE', 'MINUTES', 'MISS', 'MISTAKE', 'MISUNDERSTAND', 'MONEY', 'MORE', 'MORNING', 'MOST', 'MOTHER', 'MOTIVATE', 'MOTORCYCLE', 'MOVE', 'MOVIE', 'MP', 'MPH', 'MR', 'MUCH', 'MUHAMMAD', 'MUST', 'NAKED', 'NAME', 'NAUSEA', 'NEAR', 'NEB', 'NEED', 'NEGATIVE', 'NERVOUS', 'NEV', 'NEVADA', 'NEVER', 'NEW', 'NEW-YORK', 'NEWS', 'NEXT', 'NEXT-TO', 'NEXT-WEEK', 'NICE', 'NIGHT', 'NINETY', 'NINETY-SIX', 'NO', 'NONE', 'NONE/NOTHING', 'NOON', 'NORTH', 'NORTHRIDGE', 'NOT', 'NOT-CARE', 'NOT-KNOW', 'NOT-LIKE', 'NOT-MIND', 'NOT-WANT', 'NOT-YET', 'NOTHING', 'NOTICE', 'NOW', 'NUMB', 'NUMBER', 'NURSE', 'O', 'OAKLAND', 'OBVIOUS', 'OF', 'OF-COURSE', 'OFFENSE', 'OFFICE', 'OFFICERS', 'OH-I-SEE', 'OHIO', 'OIL', 'OK', 'OLD', 'OLDER', 'OMAHA', 'ON', 'ONCE', 'ONCE-IN-A-WHILE', 'ONE', 'ONE-THOUSAND', 'ONLY', 'OPEN', 'OPEN-BOOK', 'OPINION', 'OPPORTUNITY', 'OR', 'OSE', 'OSE-CALL', 'OTHER', 'OUT', 'OUTGOING', 'OUTSIDE', 'OVER-NIGHT', 'OVER/AFTER', 'OWN', 'P', 'PACE', 'PACE/PROGRESS', 'PAGEANT', 'PAGER', 'PANS', 'PANT', 'PAPER', 'PARADE', 'PARKINSONS', 'PART', 'PARTY', 'PASS-DOWN', 'PAST', 'PATIENT', 'PAUL', 'PAY', 'PAY-ATTENTION', 'PCL', 'PEE', 'PEN', 'PEOPLE', 'PERCENT', 'PERFECT', 'PERSON', 'PHONE', 'PICK-UP', 'PICK/SELECT', 'PICTURE', 'PIG', 'PITCH-IN', 'PITY', 'PKSON', 'PLACE', 'PLAN', 'PLANT', 'PLASTIC', 'PLAY', 'PLAY-AGAINST', 'PLAYER', 'PLAYS', 'PLUS', 'POLE', 'POLICY', 'POSITIVE', 'POSS', 'POSSIBLE', 'POTATO', 'POTS', 'POUND', 'POUR-SWEAT', 'PREDICT', 'PREFER', 'PREGNANT', 'PRESSURE', 'PRETEND', 'PRETTY', 'PRICE', 'PROBLEM', 'PROCEED', 'PRODUCT', 'PUBLIC', 'PULL', 'PUMP', 'PUNCH', 'PUNISH', 'PUSH', 'PUT-AWAY', 'QB', 'QM', 'QUARTERBACK', 'QUESTION', 'QUOTE', 'R', 'RA', 'RAFT', 'RAIDERS', 'RAIN', 'READ', 'READY', 'REALIZE', 'REALLY', 'REASON', 'RECENT-PAST', 'RECORD', 'REFUSE', 'REGULAR', 'RELAX', 'REMEMBER', 'REMIND', 'REPLY', 'REPORT', 'REPORTER', 'REQUIRE', 'RESIDENCE/ADDRESS', 'RESTAURANT', 'RESULT', 'RIBS', 'RIDE', 'RIGHT', 'RIGHT-HERE', 'RING', 'RIVER', 'ROAD', 'ROCK', 'RODNEY', 'ROLLERBLADE', 'ROOF', 'ROOM', 'ROOMMATE', 'ROPE', 'ROW/PADDLE', 'RUBBER', 'RUN', 'RUNNING-BACK', 'S', 'SACK', 'SAD', 'SAFE', 'SAME', 'SARCASM', 'SATISFIED', 'SAY', 'SB', 'SCARE', 'SCISSOR', 'SCL', 'SCREAM', 'SEAFOOD', 'SEARCH-FOR', 'SEASON', 'SEASONS', 'SECOND', 'SEE', 'SEE-SEE', 'SEEM', 'SELECT', 'SELF', 'SELL', 'SEMESTER', 'SEND', 'SENTENCE', 'SEPARATE', 'SEPT', 'SERIOUS', 'SERVICE', 'SET-ASIDE', 'SET-UP', 'SEVEN', 'SEVEN-THIRTY', 'SEW', 'SEWER', 'SF', 'SHAKE', 'SHAMPOO', 'SHARP', 'SHEEP', 'SHIRT', 'SHOCK', 'SHOE', 'SHOOT', 'SHORT', 'SHOULD', 'SHOW', 'SHOWER', 'SIBERIAN', 'SICK', 'SIDE', 'SIGN', 'SILK', 'SIMPLE', 'SIREN', 'SISTER', 'SIT', 'SITUATION', 'SIX', 'SIX-DAY', 'SIZE', 'SKILL', 'SLEEP', 'SLICE', 'SLOW', 'SMALL', 'SMELL', 'SMOKE', 'SNEAK', 'SNOW', 'SO', 'SOCIAL/INTERACT', 'SOCIALIZE', 'SOME', 'SOMETHING/ONE', 'SOMETIMES', 'SOON', 'SORRY', 'SPA', 'SPARE', 'SPECIAL/EXCEPT', 'SPECIALIZATION', 'SPECIALTY', 'SPEECH/LECTURE', 'SPEED', 'SPELL', 'SPIN', 'SPIT', 'SPORT', 'SPRAIN', 'SPRING', 'SS', 'STAFF', 'STAND', 'STAND-UP', 'START', 'STATE', 'STATION', 'STAY', 'STAY-AWAKE', 'STAY-AWAKE-ALL-NIGHT', 'STEAK', 'STEAL', 'STICK', 'STILL', 'STITCH', 'STOMACH', 'STOOL', 'STOP', 'STORE', 'STORM', 'STORY', 'STRANGE', 'STRANGER', 'STRICT', 'STRING', 'STRIPE', 'STRONG', 'STUCK', 'STUDENT', 'SUE', 'SUGGEST', 'SUMMER', 'SUN', 'SUNNY', 'SUNRISE', 'SUNSET', 'SUPERIOR', 'SUPPOSE', 'SURGEON', 'SWITCH', 'SYSTEM', 'T', 'TABLE', 'TAG', 'TAKE', 'TAKE-BREAK', 'TAKE-OFF', 'TAKE-OVER', 'TALK', 'TANK-TOP', 'TASTE', 'TD', 'TE', 'TEA', 'TEACH', 'TEACHER', 'TEAM', 'TEASE', 'TELL', 'TEMPT', 'TEN', 'TEND', 'TEST', 'THAN', 'THAT', 'THEN', 'THICK', 'THING', 'THINK', 'THIRD', 'THOUSAND', 'THREE', 'THREE-DAY', 'THREE-HOUR', 'THROUGH', 'THROW', 'THUMBS-UP/GOOD', 'TIE', 'TIGER', 'TIME', 'TIRE', 'TO', 'TODAY', 'TOGETHER', 'TOMORROW', 'TONY', 'TOO', 'TOTAL', 'TOUCHDOWN', 'TOUGH', 'TOY', 'TRAY', 'TREE', 'TRIP', 'TROUBLE', 'TRUE-BUSINESS', 'TRUNK', 'TRUST', 'TRY', 'TURN', 'TURN-OFF', 'TWENTY', 'TWICE', 'TWO', 'UB', 'UE', 'UMBRELLA', 'UN', 'UNDER', 'UNDERSTAND', 'UNIVERSITY', 'UNLESS', 'UNTIL', 'UP', 'UP-TO-NOW', 'UPSET', 'US', 'USE', 'USE-SIGN-LANGUAGE', 'UTAH', 'V', 'VARIOUS', 'VARY', 'VEGETABLE', 'VERY', 'VIBRATE', 'VIDEOTAPE', 'VINCE', 'VISIT', 'VOMIT', 'WAIT', 'WAITRESS', 'WAIVE', 'WAKE-UP', 'WALK', 'WALLET', 'WANT', 'WARN', 'WAS', 'WASH', 'WASTE', 'WATCH', 'WATCH-TV', 'WATER', 'WAY', 'WEAR', 'WEATHER', 'WEEKEND', 'WELCOME', 'WET', 'WHA', 'WHAT', 'WHEEL', 'WHEN', 'WHERE', 'WHEW/RELIEVED', 'WHICH', 'WHITE', 'WHO', 'WHOLE', 'WHY', 'WIDE-RECEIVER', 'WIFE', 'WIN', 'WINNER', 'WINNING', 'WISH', 'WITH', 'WOLF', 'WOMAN', 'WONDER', 'WONDERFUL', 'WOOD', 'WORD', 'WORK', 'WORK-OUT', 'WORRY', 'WORSE', 'WORTH', 'WOW', 'WOW/AWFUL', 'WRITE', 'WRONG', 'WYOMING', 'X', 'XRAY', 'Y', 'YEAR', 'YEAR-LONG', 'YES', 'YESTERDAY', 'YOUNG', 'ZONE', '^', 'a', 'arc', 'b', 'bent-1', 'bent-B', 'bent-B-L', 'c', 'crvd-5', 'crvd-B', 'crvd-L', 'crvd-V', 'crvd-sprd-B', 'd', 'e', 'f', 'fanned-flat-O', 'flat-B', 'flat-O', 'fs', 'g', 'h', 'i', 'j', 'k', 'l', 'loc', 'm', 'n', 'ns', 'o', 'p', 'part', 'pl', 'r', 's', 't', 'u', 'v', 'w', 'xx']\n",
      "num_eng_tokens 1678\n",
      "num_asl_tokens 1117\n"
     ]
    }
   ],
   "source": [
    "# generate\n",
    "    # 1) list of eng-asl sentence pairs\n",
    "    # 2) set of unique english vocab\n",
    "    # 3) set of unique asl vocab\n",
    "\n",
    "text_pairs = []\n",
    "eng_tokens = set()\n",
    "asl_tokens = set()\n",
    "\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    " \n",
    "for line in lines:\n",
    "    pair = []\n",
    "    eng_text, asl_text = line.split(\"\\t\")\n",
    "    pair.append(eng_text.lower())\n",
    "    pair.append(asl_text)\n",
    "    text_pairs.append(pair)\n",
    "\n",
    "for pair in text_pairs:\n",
    "    sent_tokens = custom_eng_tokenize(pair[0])\n",
    "    for token in sent_tokens:\n",
    "        if token not in eng_tokens:\n",
    "            eng_tokens.add(token)\n",
    "            \n",
    "for pair in text_pairs:\n",
    "    sent_tokens = custom_asl_tokenize(pair[1])\n",
    "    for token in sent_tokens:\n",
    "        if token not in asl_tokens:\n",
    "            asl_tokens.add(token)\n",
    "\n",
    "eng_tokens = sorted(list(eng_tokens))\n",
    "asl_tokens = sorted(list(asl_tokens))\n",
    "\n",
    "print(\"eng_tokens:\", eng_tokens)\n",
    "print(\"asl_tokens\", asl_tokens)\n",
    "num_encoder_tokens = len(eng_tokens)\n",
    "num_decoder_tokens = len(asl_tokens)\n",
    "print(\"num_eng_tokens\", num_encoder_tokens)\n",
    "print(\"num_asl_tokens\", num_decoder_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b587712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['john was reading that book, what did he say it is about?', 'IX-3p fs-JOHN READ BOOK IX-3p SAY IX-3p xx ABOUT IX-3p']\n",
      "['did the teacher already buy a house yesterday?', 'YESTERDAY TEACHER BUY HOUSE FINISH']\n",
      "['i looked at cows while i was driving and i felt sad for them because they couldn\\'t even move. at end of the \"jail\", i saw a butcher shop where they were slaughtering the cows. i also saw some dead cows that were hanging!', 'IX-1p REALLY SAD COW CANNOT MOVE REALLY SEE (5)WOW WOW/AWFUL SCL:3xx REALLY END OF THAT DCL:4xx IX-loc HAVE fs-BUTCHER-SHOP REALLY ICLxx SHOW DEAD COW BPCL:2xx']\n",
      "['as i was driving, i was really bored. so i started thinking about what i could do.', 'DRIVE BORE IX-1p DRIVE #DO']\n",
      "['will john finish reading soon?', 'IX-3p fs-JOHN READ IX-3p FINISH (L)FUTURE SOON IX-3p']\n"
     ]
    }
   ],
   "source": [
    "# glimpse pairs\n",
    "\n",
    "for _ in range(5):\n",
    "    print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c010cbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1406 total pairs\n",
      "986 training pairs\n",
      "210 validation pairs\n",
      "210 test pairs\n"
     ]
    }
   ],
   "source": [
    "# split data\n",
    "\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
    "\n",
    "print(f\"{len(text_pairs)} total pairs\")\n",
    "print(f\"{len(train_pairs)} training pairs\")\n",
    "print(f\"{len(val_pairs)} validation pairs\")\n",
    "print(f\"{len(test_pairs)} test pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fba5322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_hub.tokenizers.compute_word_piece_vocabulary(\n",
    "        word_piece_ds.batch(500).prefetch(2),\n",
    "        vocabulary_size=vocab_size,\n",
    "        reserved_tokens=reserved_tokens,\n",
    "    )\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5335f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 23:50:30.111494: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2\n",
      "2025-07-15 23:50:30.111525: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 8.00 GB\n",
      "2025-07-15 23:50:30.111547: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 2.67 GB\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1752637830.112142 9896833 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1752637830.112212 9896833 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-07-15 23:50:30.916491: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-07-15 23:50:32.371043: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[UNK]', '[START]', '[END]', '!', '\"', '$', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'the', 'to', 'and', '##s', 'john', 'it', 'was', 'he', 'that', 'in', '##d', '##ed', '##y', '##e', '##ing', 'is', 'car', 'my', 'book', 'of', '##t', '##n', 'we', 'said', 'house', 'mother', 'will', 'you', 'teacher', 'for', 'really', 'did', 'his', 'buy', 'on', 'at', '##er', 'they', 'up', '##es', 'but', '##k', 'mary', '##r', 'have', 'know', 'me', 'had', 'reading', 'there', '##m', 'she', 'him', 'like', 'so', '##ay', '##a', '##ly', 'as', 'back', 'who', 'then', '##h', 'father', 'read', 'when', 'not', 'were', 'would', '##l', 'went', 'what', 'about', 'one', '##le', 'chocolate', 'down', 'looked', 'some', '##it', 'out', 'student', 'all', 'books', 'give', 'likes', 'over', '##g', '##st', 'many', 'wolf', 'cop', 'don', 'go', 'has', 'thought', '##an', '##ar', '##at', '##king', 'deaf', 'two', '##0', '##ce', 'be', 'do', 'or', 'arrived', 'bought', 'didn', 'gave', 'la', 'no', '##al', '##se', '##ter', 'can', 'how', 'because', 'just', 'see', 'should', '##ive', '##o', 'boston', 'here', 'if', 'off', 'saw', 'time', 'with', 'yesterday', '##ch', '##on', '##or', '##ow', 'people', 'pigs', '##ew', '##ight', '##ry', 'decided', 'driving', 'fine', 'loves', 'told', 'while', '##ad', '##ake', '##ame', '##ge', '##in', '##ot', '##pe', 'friend', 'man', 'started', 'want', '##ally', '##thing', '##un', 'could', 'got', 'hand', 'her', 'person', 'sure', 'them', 'three', 'why', '##ice', '##ind', '##ven', 'an', 'are', 'doesn', 'finished', 'get', '##all', '##ass', '##ck', '##ep', '##il', '##ire', '##ning', '##ome', '##self', 'after', 'big', 'drove', 'going', 'movies', 'now', 'oh', 'right', 'same', 'through', '##ate', '##ir', '##les', '##ood', '##p', '##ped', 'across', 'again', 'been', 'buying', 'couldn', 'does', 'finish', 'giving', 'more', 'pulled', 'very', '##ell', '##ent', '##ft', '##i', '##ine', '##ong', '##other', '##rs', 'around', 'dana', 'from', 'good', 'hearing', 'into', 'movie', 'must', 'rain', 'still', 'thing', 'us', '##ars', '##ation', '##ays', '##c', '##el', '##en', '##ever', '##gh', '##her', '##ick', '##is', '##ke', '##ked', '##rees', '##ys', 'before', 'boat', 'hit', 'put', 'road', 'seen', 'students', 'walked', 'which', '##ds', '##eer', '##ents', '##et', '##fe', '##ks', '##ned', '##ple', '##stairs', '##ther', '##ue', '##ut', '##ved', '##way', 'ahead', 'away', 'cars', 'each', 'football', 'found', 'maybe', 'someone', 'well', '##!', '##\"', '##$', \"##'\", '##(', '##)', '##*', '##,', '##-', '##.', '##/', '##1', '##2', '##3', '##4', '##5', '##6', '##7', '##8', '##9', '##:', '##;', '##?', '##b', '##f', '##j', '##q', '##u', '##v', '##w', '##x', '##z']\n",
      "['[PAD]', '[UNK]', '[START]', '[END]', '#', '(', ')', '+', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', '>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '^', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'IX', 'fs', '1p', '3p', '5xx', 'JOHN', 'REALLY', 'BOOK', 'POSS', 'NOT', 'loc', '##E', 'SCL', '##T', 'CAR', 'DCL', 'BUY', 'READ', 'IN', 'FUTURE', 'ONE', 'part', '##D', 'ns', 'FINISH', 'MOTHER', 'TEACHER', '##Y', 'BCLxx', 'arc', 'LIKE', 'SAME', 'HOUSE', 'LOOK', '##ER', 'ICLxx', '##L', '##xx', '1xx', 'MARY', 'TO', 'GIFT', 'SEE', 'THAT', 'DRIVE', 'ON', 'SAY', 'xx', 'STUDENT', 'pl', '3xx', 'KNOW', 'BUT', 'UP', 'FATHER', '2p', '##CH', '##M', '##N', '##S', '##SE', 'SO', 'CHOCOLATE', 'FINE', 'HAVE', 'ICL', 'WHO', 'MANY', 'SOMETHING', 'CAN', 'NOW', 'COP', 'FOR', 'MOVIE', '##G', '##TER', 'ARRIVE', 'DEAF', 'GO', 'Sxx', '##ET', 'ABOUT', 'FIND', '##ING', '##ST', 'BPCL', 'HERE', 'MUST', 'OUT', 'bent', '##AL', '##CE', '##EAR', '##K', 'WHY', 'FRIEND', 'THINK', 'Vxx', 'WANT', 'crvd', '##AKE', '##OOD', '##R', 'Bxx', 'THREE', 'YESTERDAY', '##H', '##ITE', '##P', 'ALL', 'GIVE', 'IT', 'LA', 'SHOULD', 'TELL', '##O', '##OW', 'DAY', 'MORE', 'RAIN', 'TIME', 'WOW', '##0', '##AY', '##B', '##LY', 'BCL', 'Cxx', 'HOW', 'LOVE', 'STILL', 'WHEN', '##A', '##END', '##GE', '##IN', '##IRE', '##RY', '##t', '25', 'DO', 'MAN', '##BLE', '##CK', '##NY', '##UE', 'AND', 'FEEL', 'MAKE', 'MOTORCYCLE', 'NONE', 'OVER', 'THING', '##AD', '##ATE', '##ICE', '##MIT', '##OR', '##OY', 'BOSTON', 'DECIDE', 'Lxx', 'MAYBE', 'OF', 'TWO', '##AN', '##EN', '##INE', '##OUGH', '##WAY', '##wg', 'OTHER', 'PIG', 'WITH', '##AME', '##ANT', '##ECT', '##ION', '##OME', '##PLE', '##UN', 'DANA', 'FORMERLY', 'GOOD', 'NIGHT', 'OR', 'QMwg', '##ASS', '##AUSE', '##C', '##EA', '##EED', '##ES', '##I', '##IL', '##ITY', '##OP', '##ORE', '##RING', '##TAL', '##USE', '##VEN', 'BIT', 'BUSINESS', 'CANNOT', 'FALL', 'FOOTBALL', 'FROM', 'LITTLE', 'PEOPLE', 'TRUE', 'WAIT', '##ACK', '##AR', '##EAK', '##ED', '##FUL', '##HERE', '##IDE', '##RICK', '##SH', '##SS', '##TE', '##US', '##X', 'BOAT', 'EAT', 'END', 'EVERY', 'INFORM', 'LCL', 'SOME', 'VISIT', '##AST', '##AT', '##DER', '##ESS', '##F', '##IVE', '##LEEP', '##NING', '##ON', '##OPE', '##ORN', '##OUR', '##REE', '##TY', 'AFTER', 'AGAIN', 'CHANGE', 'DIFFERENT', 'GIRL', 'HEARING', 'HIT', 'LIVE', 'MOTHERwg', 'NEVER', 'NEW', 'PREFER', 'PROCEED', 'SELF', 'STAY', 'SUPPOSE', 'THEN', 'THUMB', 'TOGETHER', 'WOLF', '###', '##(', '##)', '##+', '##-', '##.', '##/', '##1', '##2', '##3', '##4', '##5', '##6', '##7', '##8', '##9', '##:', '##>', '##J', '##Q', '##U', '##V', '##W', '##Z', '##^', '##a', '##b', '##c', '##d', '##e', '##f', '##g', '##h', '##i', '##j', '##k', '##l', '##m', '##n', '##o', '##p', '##r', '##s', '##u', '##v', '##w', '##x']\n"
     ]
    }
   ],
   "source": [
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "trained_eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "asl_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "trained_asl_vocab = train_word_piece(asl_samples, ASL_VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "print(trained_eng_vocab)\n",
    "print(trained_asl_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0e3764c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Tokens:  ['me', 'had', 'reading', 'there', '##m', 'she', 'him', 'like', 'so', '##ay']\n",
      "ASL Tokens:  ['##Y', 'BCLxx', 'arc', 'LIKE', 'SAME', 'HOUSE', 'LOOK', '##ER', 'ICLxx', '##L']\n"
     ]
    }
   ],
   "source": [
    "print(\"English Tokens: \", trained_eng_vocab[100:110])\n",
    "print(\"ASL Tokens: \", trained_asl_vocab[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9895e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=trained_eng_vocab, lowercase=False\n",
    ")\n",
    "asl_tokenizer = keras_hub.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=trained_asl_vocab, lowercase=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb29eb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentence:  if john needs more information, he will read the book.\n",
      "Tokens:  tf.Tensor(\n",
      "[180  58  41  67  65  57 265  63 359 188 104 289  11  61  80 118  54  72\n",
      "  13], shape=(19,), dtype=int32)\n",
      "Recovered text after detokenizing:  if john needs more information , he will read the book .\n",
      "\n",
      "ASL sentence:  fs-JOHN NEED MORE INFORMATION IX-3p FUTURE READ BOOK\n",
      "Tokens:  tf.Tensor([ 74   8  78  36 264 191 302 307 250  73   8  76  92  90  80], shape=(15,), dtype=int32)\n",
      "Recovered text after detokenizing:  fs - JOHN NEED MORE INFORMATION IX - 3p FUTURE READ BOOK\n"
     ]
    }
   ],
   "source": [
    "eng_input_ex = text_pairs[0][0]\n",
    "eng_tokens_ex = eng_tokenizer.tokenize(eng_input_ex)\n",
    "print(\"English sentence: \", eng_input_ex)\n",
    "print(\"Tokens: \", eng_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    eng_tokenizer.detokenize(eng_tokens_ex),\n",
    ")\n",
    "\n",
    "print()\n",
    "\n",
    "asl_input_ex = text_pairs[0][1]\n",
    "asl_tokens_ex = asl_tokenizer.tokenize(asl_input_ex)\n",
    "print(\"ASL sentence: \", asl_input_ex)\n",
    "print(\"Tokens: \", asl_tokens_ex)\n",
    "print(\n",
    "    \"Recovered text after detokenizing: \",\n",
    "    asl_tokenizer.detokenize(asl_tokens_ex),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3c4a4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(eng, asl):\n",
    "    eng = eng_tokenizer(eng)\n",
    "    asl = asl_tokenizer(asl)\n",
    "\n",
    "    # Pad `eng` to `MAX_SEQUENCE_LENGTH`.\n",
    "    eng_start_end_packer = keras_hub.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "        pad_value=eng_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    eng = eng_start_end_packer(eng)\n",
    "\n",
    "    # Add special tokens (`\"[START]\"` and `\"[END]\"`) to `asl` and pad it as well.\n",
    "    asl_start_end_packer = keras_hub.layers.StartEndPacker(\n",
    "        sequence_length=MAX_SEQUENCE_LENGTH + 1,\n",
    "        start_value=asl_tokenizer.token_to_id(\"[START]\"),\n",
    "        end_value=asl_tokenizer.token_to_id(\"[END]\"),\n",
    "        pad_value=asl_tokenizer.token_to_id(\"[PAD]\"),\n",
    "    )\n",
    "    asl = asl_start_end_packer(asl)\n",
    "\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": eng,\n",
    "            \"decoder_inputs\": asl[:, :-1],\n",
    "        },\n",
    "        asl[:, 1:],\n",
    "    )\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, asl_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    asl_texts = list(asl_texts)\n",
    "    dataset = tf_data.Dataset.from_tensor_slices((eng_texts, asl_texts))\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    dataset = dataset.map(preprocess_batch, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "    return dataset.shuffle(1400).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e2a04a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (16, 100)\n",
      "inputs[\"decoder_inputs\"].shape: (16, 100)\n",
      "targets.shape: (16, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 23:50:34.438976: W tensorflow/core/kernels/data/cache_dataset_ops.cc:916] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
    "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ebed136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "encoder_inputs = keras.Input(shape=(None,), name=\"encoder_inputs\")\n",
    "\n",
    "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=ENG_VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(encoder_inputs)\n",
    "\n",
    "encoder_outputs = keras_hub.layers.TransformerEncoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(inputs=x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = keras.Input(shape=(None,), name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, EMBED_DIM), name=\"decoder_state_inputs\")\n",
    "\n",
    "x = keras_hub.layers.TokenAndPositionEmbedding(\n",
    "    vocabulary_size=ASL_VOCAB_SIZE,\n",
    "    sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    embedding_dim=EMBED_DIM,\n",
    ")(decoder_inputs)\n",
    "\n",
    "x = keras_hub.layers.TransformerDecoder(\n",
    "    intermediate_dim=INTERMEDIATE_DIM, num_heads=NUM_HEADS\n",
    ")(decoder_sequence=x, encoder_sequence=encoded_seq_inputs)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = keras.layers.Dense(ASL_VOCAB_SIZE, activation=\"softmax\")(x)\n",
    "decoder = keras.Model(\n",
    "    [\n",
    "        decoder_inputs,\n",
    "        encoded_seq_inputs,\n",
    "    ],\n",
    "    decoder_outputs,\n",
    ")\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs],\n",
    "    decoder_outputs,\n",
    "    name=\"transformer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "092c923e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"transformer\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"transformer\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_and_position… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">228,096</span> │ encoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionE…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_encoder │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>) │    <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ token_and_positi… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncode…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │    <span style=\"color: #00af00; text-decoration-color: #00af00\">565,473</span> │ decoder_inputs[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1121</span>)             │            │ transformer_enco… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ encoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ token_and_position… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m228,096\u001b[0m │ encoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mTokenAndPositionE…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ decoder_inputs      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ transformer_encoder │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m) │    \u001b[38;5;34m198,272\u001b[0m │ token_and_positi… │\n",
       "│ (\u001b[38;5;33mTransformerEncode…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ functional_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,      │    \u001b[38;5;34m565,473\u001b[0m │ decoder_inputs[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)        │ \u001b[38;5;34m1121\u001b[0m)             │            │ transformer_enco… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">991,841</span> (3.78 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m991,841\u001b[0m (3.78 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">991,841</span> (3.78 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m991,841\u001b[0m (3.78 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-15 23:50:36.233451: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 143ms/step - accuracy: 0.7660 - loss: 2.1798 - val_accuracy: 0.8557 - val_loss: 0.8114\n",
      "Epoch 2/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 107ms/step - accuracy: 0.8546 - loss: 0.8161 - val_accuracy: 0.8643 - val_loss: 0.7080\n",
      "Epoch 3/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 97ms/step - accuracy: 0.8613 - loss: 0.7168 - val_accuracy: 0.8683 - val_loss: 0.6661\n",
      "Epoch 4/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 99ms/step - accuracy: 0.8655 - loss: 0.6696 - val_accuracy: 0.8690 - val_loss: 0.6406\n",
      "Epoch 5/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 102ms/step - accuracy: 0.8681 - loss: 0.6389 - val_accuracy: 0.8695 - val_loss: 0.6245\n",
      "Epoch 6/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step - accuracy: 0.8681 - loss: 0.6192 - val_accuracy: 0.8695 - val_loss: 0.6156\n",
      "Epoch 7/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 101ms/step - accuracy: 0.8696 - loss: 0.6023 - val_accuracy: 0.8705 - val_loss: 0.6073\n",
      "Epoch 8/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 98ms/step - accuracy: 0.8695 - loss: 0.5925 - val_accuracy: 0.8710 - val_loss: 0.6009\n",
      "Epoch 9/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 99ms/step - accuracy: 0.8702 - loss: 0.5819 - val_accuracy: 0.8701 - val_loss: 0.5987\n",
      "Epoch 10/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 104ms/step - accuracy: 0.8713 - loss: 0.5718 - val_accuracy: 0.8700 - val_loss: 0.5943\n",
      "Epoch 11/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 97ms/step - accuracy: 0.8721 - loss: 0.5638 - val_accuracy: 0.8714 - val_loss: 0.5925\n",
      "Epoch 12/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 99ms/step - accuracy: 0.8735 - loss: 0.5550 - val_accuracy: 0.8715 - val_loss: 0.5886\n",
      "Epoch 13/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step - accuracy: 0.8742 - loss: 0.5506 - val_accuracy: 0.8724 - val_loss: 0.5860\n",
      "Epoch 14/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 105ms/step - accuracy: 0.8744 - loss: 0.5458 - val_accuracy: 0.8716 - val_loss: 0.5858\n",
      "Epoch 15/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step - accuracy: 0.8745 - loss: 0.5424 - val_accuracy: 0.8722 - val_loss: 0.5858\n",
      "Epoch 16/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step - accuracy: 0.8753 - loss: 0.5369 - val_accuracy: 0.8724 - val_loss: 0.5824\n",
      "Epoch 17/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step - accuracy: 0.8765 - loss: 0.5301 - val_accuracy: 0.8721 - val_loss: 0.5839\n",
      "Epoch 18/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 102ms/step - accuracy: 0.8773 - loss: 0.5254 - val_accuracy: 0.8727 - val_loss: 0.5822\n",
      "Epoch 19/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 105ms/step - accuracy: 0.8778 - loss: 0.5207 - val_accuracy: 0.8733 - val_loss: 0.5789\n",
      "Epoch 20/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 101ms/step - accuracy: 0.8793 - loss: 0.5160 - val_accuracy: 0.8737 - val_loss: 0.5823\n",
      "Epoch 21/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step - accuracy: 0.8788 - loss: 0.5092 - val_accuracy: 0.8749 - val_loss: 0.5801\n",
      "Epoch 22/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 99ms/step - accuracy: 0.8802 - loss: 0.5053 - val_accuracy: 0.8743 - val_loss: 0.5819\n",
      "Epoch 23/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 111ms/step - accuracy: 0.8818 - loss: 0.4974 - val_accuracy: 0.8760 - val_loss: 0.5728\n",
      "Epoch 24/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step - accuracy: 0.8839 - loss: 0.4907 - val_accuracy: 0.8759 - val_loss: 0.5719\n",
      "Epoch 25/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 103ms/step - accuracy: 0.8844 - loss: 0.4869 - val_accuracy: 0.8760 - val_loss: 0.5714\n",
      "Epoch 26/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 99ms/step - accuracy: 0.8856 - loss: 0.4809 - val_accuracy: 0.8764 - val_loss: 0.5688\n",
      "Epoch 27/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 109ms/step - accuracy: 0.8864 - loss: 0.4760 - val_accuracy: 0.8772 - val_loss: 0.5698\n",
      "Epoch 28/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step - accuracy: 0.8875 - loss: 0.4663 - val_accuracy: 0.8777 - val_loss: 0.5660\n",
      "Epoch 29/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 100ms/step - accuracy: 0.8900 - loss: 0.4627 - val_accuracy: 0.8783 - val_loss: 0.5652\n",
      "Epoch 30/30\n",
      "\u001b[1m62/62\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 126ms/step - accuracy: 0.8902 - loss: 0.4553 - val_accuracy: 0.8792 - val_loss: 0.5626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16a14ba90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_ds, epochs=EPOCHS, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad617331",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequences(input_sentences):\n",
    "    with tf.device('/CPU:0'):\n",
    "        batch_size = 1\n",
    "\n",
    "        # Tokenize the encoder input.\n",
    "        encoder_input_tokens = ops.convert_to_tensor(eng_tokenizer(input_sentences))\n",
    "        if len(encoder_input_tokens[0]) < MAX_SEQUENCE_LENGTH:\n",
    "            pads = ops.full((1, MAX_SEQUENCE_LENGTH - len(encoder_input_tokens[0])), 0)\n",
    "            encoder_input_tokens = ops.concatenate(\n",
    "                [encoder_input_tokens, pads], 1\n",
    "            )\n",
    "\n",
    "        # Define a function that outputs the next token's probability given the\n",
    "        # input sequence.\n",
    "        def next(prompt, cache, index):\n",
    "            logits = transformer([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "            # Ignore hidden states for now; only needed for contrastive search.\n",
    "            hidden_states = None\n",
    "            return logits, hidden_states, cache\n",
    "\n",
    "        # Build a prompt of length 40 with a start token and padding tokens.\n",
    "        length = 40\n",
    "        start = ops.full((batch_size, 1), asl_tokenizer.token_to_id(\"[START]\"))\n",
    "        pad = ops.full((batch_size, length - 1), asl_tokenizer.token_to_id(\"[PAD]\"))\n",
    "        prompt = ops.concatenate((start, pad), axis=-1)\n",
    "\n",
    "\n",
    "        generated_tokens = keras_hub.samplers.GreedySampler()(\n",
    "                next,\n",
    "                prompt,\n",
    "                stop_token_ids=[asl_tokenizer.token_to_id(\"[END]\")],\n",
    "                index=1,  # Start sampling after start token.\n",
    "            )\n",
    "        generated_sentences = asl_tokenizer.detokenize(generated_tokens)\n",
    "    return generated_sentences\n",
    "\n",
    "outputs = []\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for i in range(200):\n",
    "    output_pairs = []\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    translated = decode_sequences([input_sentence])\n",
    "    translated = translated[0]\n",
    "    translated = (\n",
    "        translated.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "    output_pairs.append(input_sentence)\n",
    "    output_pairs.append(translated)\n",
    "    outputs.append(output_pairs)\n",
    "    \n",
    "df = pd.DataFrame(outputs, columns=[\"input sentence\", \"translation\"])\n",
    "df.to_csv(\"/Users/adrianajimenez/Desktop/Downloads/REUAICT/Real-Code/2025-ASL-data/seq2seq_code/word_level/outputs50.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "881ff7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.42068034410476685>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.32498180866241455>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.34988173842430115>}\n",
      "ROUGE-2 Score:  {'precision': <tf.Tensor: shape=(), dtype=float32, numpy=0.21741840243339539>, 'recall': <tf.Tensor: shape=(), dtype=float32, numpy=0.16280566155910492>, 'f1_score': <tf.Tensor: shape=(), dtype=float32, numpy=0.17630094289779663>}\n"
     ]
    }
   ],
   "source": [
    "rouge_1 = keras_hub.metrics.RougeN(order=1)\n",
    "rouge_2 = keras_hub.metrics.RougeN(order=2)\n",
    "\n",
    "for test_pair in test_pairs:\n",
    "    input_sentence = test_pair[0]\n",
    "    reference_sentence = test_pair[1]\n",
    "\n",
    "    translated_sentence = decode_sequences([input_sentence])\n",
    "    translated_sentence = translated_sentence[0]\n",
    "    translated_sentence = (\n",
    "        translated_sentence.replace(\"[PAD]\", \"\")\n",
    "        .replace(\"[START]\", \"\")\n",
    "        .replace(\"[END]\", \"\")\n",
    "        .strip()\n",
    "    )\n",
    "\n",
    "    rouge_1(reference_sentence, translated_sentence)\n",
    "    rouge_2(reference_sentence, translated_sentence)\n",
    "\n",
    "print(\"ROUGE-1 Score: \", rouge_1.result())\n",
    "print(\"ROUGE-2 Score: \", rouge_2.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
