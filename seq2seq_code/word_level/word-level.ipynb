{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43c1584",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\adriana\\official-code\\2025-ASL-data\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import keras_hub\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import keras\n",
    "from keras import ops\n",
    "\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df1779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyparsing import Word, alphas, nums\n",
    "import pyparsing as pp\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd3efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "handshapelist = [\n",
    "  '1',\n",
    "  '3',\n",
    "  '4',\n",
    "  '5',\n",
    "  '6',\n",
    "  '7',\n",
    "  '8',\n",
    "  '9',\n",
    "  '10',\n",
    "  '25',\n",
    "  'A',\n",
    "  'B',\n",
    "  'C',\n",
    "  'D',\n",
    "  'E',\n",
    "  'F',\n",
    "  'G',\n",
    "  'H-U',\n",
    "  'I',\n",
    "  'K',\n",
    "  'L',\n",
    "  'M',\n",
    "  'N',\n",
    "  'O',\n",
    "  'R',\n",
    "  'S',\n",
    "  'T',\n",
    "  'U',\n",
    "  'V',\n",
    "  'W',\n",
    "  'X',\n",
    "  'Y',\n",
    "  'C-L',\n",
    "  'U-L',\n",
    "  'B-L',\n",
    "  'P-K',\n",
    "  'Q-G',\n",
    "  'L-X',\n",
    "  'I-L-Y',\n",
    "  '5-C',\n",
    "  '5-C-L',\n",
    "  '5-C-tt',\n",
    "  'alt-M',\n",
    "  'alt-N',\n",
    "  'alt-P',\n",
    "  'B-xd',\n",
    "  'baby-O',\n",
    "  'bent-1',\n",
    "  'bent-B',\n",
    "  'bent-B-L',\n",
    "  'bent-horns',\n",
    "  'bent-L',\n",
    "  'bent-M',\n",
    "  'bent-N',\n",
    "  'bent-U',\n",
    "  'cocked-8',\n",
    "  'cocked-F',\n",
    "  'cocked-S',\n",
    "  'crvd-5',\n",
    "  'crvd-B',\n",
    "  'crvd-flat-B',\n",
    "  'crvd-L',\n",
    "  'crvd-sprd-B',\n",
    "  'crvd-U',\n",
    "  'crvd-V',\n",
    "  'fanned-flat-O',\n",
    "  'flat-B',\n",
    "  'flat-C',\n",
    "  'flat-F',\n",
    "  'flat-G',\n",
    "  'flat-O-2',\n",
    "  'flat-O',\n",
    "  'full-M',\n",
    "  'horns',\n",
    "  'loose-E',\n",
    "  'O-2-horns',\n",
    "  'open-8',\n",
    "  'open-F',\n",
    "  'sml-C-3',\n",
    "  'tight-C-2',\n",
    "  'tight-C',\n",
    "  'X-over-thumb'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc0a3bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_regexp = r\"\"\"\n",
    "    [0-9]+                      # one or more digits\n",
    "    (?:\\.[0-9]+)?               # optional decimal\n",
    "    s?                          # optional trailing 's'\n",
    "\"\"\"\n",
    "\n",
    "alpha_regexp = r\"\"\"\n",
    "    (?!                         # negative lookahead for:\n",
    "        (?:THUMB-)?             # optional THUMB-\n",
    "        (?:IX-|POSS-|SELF-)     # followed by IX-, POSS-, SELF-\n",
    "    )\n",
    "    [A-Z0-9]                    # starts with uppercase or digit\n",
    "    (?:                         # optionally followed by:\n",
    "        [A-Z0-9'-]*             #   more letters, digits, hyphen or apostrophe\n",
    "        [A-Z0-9]                #   ends on a letter or digit\n",
    "    )?\n",
    "    (?:                         # optionally followed by:\n",
    "        \\.                      #   a period\n",
    "      |                         # or\n",
    "        :[0-9]                  #   colon-digit (e.g., :2)\n",
    "    )?\n",
    "\"\"\"\n",
    "\n",
    "lookahead_regexp = r\"\"\"\n",
    "    (?:\n",
    "        (?![a-z])               # not followed by lowercase letter\n",
    "      | (?=wg)                  # unless it's specifically \"wg\"\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "word_all_regexp = r\"\"\"\n",
    "    (?: {number_regexp} | {alpha_regexp} )\n",
    "    {lookalookahead_regexphead}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbd4aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_prefix = pp.one_of([\"CL\", \"DCL\", \"LCL\", \"SCL\", \"BCL\", \"BPCL\", \"PCL\", \"ICL\"])\n",
    "ns_prefix = pp.Literal(\"ns\")\n",
    "fs_prefix = pp.Literal(\"fs\")\n",
    "lex_exceptions = pp.one_of([\"part\", \"WHAT\"])\n",
    "aspect_text = pp.Literal(\"aspect\")\n",
    "index_core_ix = pp.Literal(\"IX\")\n",
    "other_index_core = pp.one_of([\"POSS\", \"SELF\"])\n",
    "handshape = pp.one_of(handshapelist)\n",
    "person = pp.one_of([\"1p\", \"2p\", \"3p\"])\n",
    "dash = pp.Literal(\"-\")\n",
    "arc = pp.Literal(\"arc\") \n",
    "loc = pp.Literal(\"loc\")\n",
    "pl = pp.Literal(\"pl\")\n",
    "compound = pp.Literal(\"+\")\n",
    "hashtag = pp.Literal(\"#\")\n",
    "choice = pp.Literal(\"/\")\n",
    "sym = pp.Literal(\">\")\n",
    "par1 = pp.Literal(\"(\")\n",
    "par2 = pp.Literal(\")\")\n",
    "contraction = pp.Literal(\"^\")\n",
    "colon = pp.Literal(\":\")\n",
    "omit_quote = pp.Literal(\"xx\")\n",
    "period = pp.Literal(\".\")\n",
    "alphas = pp.Word(alphas, max=1)\n",
    "nums = pp.Word(nums, max=1)\n",
    "word = pp.Regex(word_all_regexp, flags=re.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b062760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_grammar = pp.OneOrMore(\n",
    "    cl_prefix |               # classifiers like CL, DCL, etc.\n",
    "    ns_prefix |               # non-specific ns\n",
    "    fs_prefix |               # fingerspelling fs\n",
    "    index_core_ix |           # IX\n",
    "    other_index_core |        # POSS, SELF\n",
    "    person |                  # 1p, 2p, 3p\n",
    "    lex_exceptions |          # part, WHAT\n",
    "    aspect_text |             # aspect\n",
    "    arc |                     # arc\n",
    "    loc |                     # loc\n",
    "    pl |                      # pl\n",
    "    handshape |               # handshapes like B, 1, 5, etc.\n",
    "    compound |                # +\n",
    "    hashtag |                 # #\n",
    "    choice |                  # /\n",
    "    sym |                     # >\n",
    "    contraction |             # ^\n",
    "    colon |                   # :\n",
    "    dash |                    # -\n",
    "    par1 | par2 |             # ( and )\n",
    "    omit_quote |              # xx\n",
    "    period |                  # .\n",
    "    word |\n",
    "    nums |                    # numbers last resort\n",
    "    alphas                     # fallback LAST\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45096aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCL', ':', '1', 'xx']\n",
      "['IX', '-', '1p', '-', 'pl', '-', '2', 'W', 'O', 'R', 'K', 'L', 'A', 'N', 'D', 'S', 'C', 'A', 'P', 'E', 'fs', '-', 'L', 'A', 'N', 'D', 'S', 'C', 'A', 'P', 'I', 'N', 'G', 'IX', '-', '1p', '5', 'xx']\n"
     ]
    }
   ],
   "source": [
    "trial = full_grammar.parse_string(\"SCL:1xx\", parse_all=True).asList()\n",
    "trial2 = full_grammar.parse_string(\"IX-1p-pl-2 WORK LANDSCAPE fs-LANDSCAPING IX-1p 5xx\", parse_all=True).as_list()\n",
    "\n",
    "print(trial)\n",
    "print(trial2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258c52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenize(text):\n",
    "    try:\n",
    "        return full_grammar.parse_string(text, parse_all=True).as_list()\n",
    "    except pp.ParseException as pe:\n",
    "        print(f\"Failed to parse: {pe}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cc0cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SCL', ':', '1', 'xx']\n",
      "['IX', '-', '1p', '-', 'pl', '-', '2', 'W', 'O', 'R', 'K', 'L', 'A', 'N', 'D', 'S', 'C', 'A', 'P', 'E', 'fs', '-', 'L', 'A', 'N', 'D', 'S', 'C', 'A', 'P', 'I', 'N', 'G', 'IX', '-', '1p', '5', 'xx']\n"
     ]
    }
   ],
   "source": [
    "trial = custom_tokenize(\"SCL:1xx\")\n",
    "trial2 = custom_tokenize(\"IX-1p-pl-2 WORK LANDSCAPE fs-LANDSCAPING IX-1p 5xx\")\n",
    "\n",
    "print(trial)\n",
    "print(trial2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e79a6b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 30  # This should be at least 10 for convergence\n",
    "MAX_SEQUENCE_LENGTH = 50\n",
    "ENG_VOCAB_SIZE = 5056\n",
    "ASL_VOCAB_SIZE = 2283\n",
    "num_samples = 1400\n",
    "\n",
    "EMBED_DIM = 128\n",
    "INTERMEDIATE_DIM =1024\n",
    "NUM_HEADS = 8\n",
    "data_path = \"C:\\\\adriana\\\\official-code\\\\2025-ASL-data\\\\data\\\\sent_pairs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3df98b8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m     lines \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines[: \u001b[38;5;28mmin\u001b[39m(num_samples, \u001b[38;5;28mlen\u001b[39m(lines) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)]:\n\u001b[1;32m---> 10\u001b[0m     input_text, target_text \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     tokenized_sent \u001b[38;5;241m=\u001b[39m custom_tokenize(target_text)\n\u001b[0;32m     12\u001b[0m     token_list\u001b[38;5;241m.\u001b[39mappend(tokenized_sent)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "token_list = []\n",
    "input_tokens = set()\n",
    "target_tokens = set()\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    \n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text = line.split(\"\\t\")\n",
    "    tokenized_sent = custom_tokenize(target_text)\n",
    "    token_list.append(tokenized_sent)\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for token in input_text:\n",
    "        if token not in input_tokens:\n",
    "            input_tokens.add(token)\n",
    "            \n",
    "            \n",
    "for sent in token_list:\n",
    "    for tok in sent:\n",
    "        if tok not in target_tokens:\n",
    "            target_tokens.add(tok)\n",
    "\n",
    "\n",
    "input_tokens = sorted(list(input_tokens))\n",
    "target_tokens = sorted(list(target_tokens))\n",
    "\n",
    "print(\"input_tokens:\", input_tokens)\n",
    "print(\"output_tokens\", target_tokens)\n",
    "num_encoder_tokens = len(input_tokens)\n",
    "num_decoder_tokens = len(target_tokens)\n",
    "print(\"num_eng_tokens\", num_encoder_tokens)\n",
    "print(\"num_asl_tokens\", num_decoder_tokens)\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c55a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_sent = []\n",
    "for sent in token_list:\n",
    "    parsed_sent.append(\" \".join(sent))\n",
    "\n",
    "print(parsed_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_and_split(x):\n",
    "    \"Split on whitespace.\"\n",
    "    return tf_string.split(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32be0357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word_piece(text_samples, vocab_size, reserved_tokens):\n",
    "    word_piece_ds = tf_data.Dataset.from_tensor_slices(text_samples)\n",
    "    vocab = keras_hub.tokenizers.compute_word_piece_vocabulary(\n",
    "        word_piece_ds.batch(1000).prefetch(2),\n",
    "        vocabulary_size=vocab_size,\n",
    "        reserved_tokens=reserved_tokens,\n",
    "    )\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5335f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
    "eng_vocab = train_word_piece(eng_samples, ENG_VOCAB_SIZE, reserved_tokens)\n",
    "\n",
    "spa_samples = [text_pair[1] for text_pair in train_pairs]\n",
    "spa_vocab = train_word_piece(spa_samples, SPA_VOCAB_SIZE, reserved_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
